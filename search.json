[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Hello and welcome to my website! My name is Oliver Myers, and I am excited to share my journey in data science and UX research with you.\n\nLocation: Dallas, Texas\nRole: Mixed-Methods User Experience Researcher\nSchool: University of Texas at Dallas\nDegree: Master of Science in Applied Cognition and Neuroscience, focusing on Human-Computer Interaction"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "About Me",
    "section": "",
    "text": "About Me\nHi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nPortfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 2"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 3"
  },
  {
    "objectID": "CopyOfassignment2.html",
    "href": "CopyOfassignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Method 1: Analyze Google Trends search term data for “Trump”, “Kamala Harris” and “Election”\n\ngoogle_trends_data &lt;- read.csv(\"~/Desktop/Trump.Harris.Google.TrendsData.Method1.csv\")\n\nMethod 2: Using gtrendsR Package to collect data\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\ninstall.packages(\"gtrendsR\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//Rtmpa9c09Q/downloaded_packages\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n#plot(HarrisTrumpElection_interest$hits, type = \"l\", main = \"Google Trends Data for Trump, Harris, and Election\",\n#     xlab = \"Time\", ylab = \"Search Interest\")\n\n\n## Install package\n#install.packages(\"gtrendsR\")\n\n#library(gtrendsR)\n\n#res &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"))\n#plot(res)\n\nDifferences between the two methods: In the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 5"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 4"
  },
  {
    "objectID": "finalProject.html",
    "href": "finalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nUX Research and Design Portfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "index.html#epps-6302-data-analysis-and-visualization",
    "href": "index.html#epps-6302-data-analysis-and-visualization",
    "title": "Oliver Myers",
    "section": "",
    "text": "Here are my assignments and final project for the EPPS 6302 course:\n&lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"col\"&gt;\n      1 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      2 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      3 of 3\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "EPPS6302Final.html",
    "href": "EPPS6302Final.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "EPPS6302FinalProjects.html",
    "href": "EPPS6302FinalProjects.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "r"
  },
  {
    "objectID": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "href": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "title": "Assignment 2",
    "section": "Method 1: Analyze Google Trends search term data for “Trump”, “Harris” and “Election”",
    "text": "Method 1: Analyze Google Trends search term data for “Trump”, “Harris” and “Election”\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\nKey Dates of Interest:\n\nJuly 14, 2024:\n\nTrump’s Peak: This date marks the first attempted assassination at a Trump rally, leading to a significant spike in search interest.\n\nJuly 21, 2024:\n\nHarris Begins to Trend: Following President Biden’s decision to drop out of the race, interest in Kamala Harris starts to increase.\n\nJuly 22, 2024:\n\nHarris Surpasses Trump: Harris peaks above Trump as she announces her candidacy for president.\n\nAugust 6, 2024:\n\nHarris’s Peak Over Trump: Harris reaches another peak after announcing Tim Walz as her running mate.\n\nAugust 23, 2024:\n\nAcceptance Speech: Harris experiences another spike in search interest during her acceptance speech at the DNC, where she becomes the Democratic front-runner for the 2024 presidential election.\n\nSeptember 11, 2024:\n\nSimultaneous Peaks: Both Trump and Harris see significant spikes as they attend the 9/11 Memorial event in New York City.\n\nSeptember 15, 2024:\n\nTrump’s Peak: A second attempted assassination at Trump’s international golf course results in another surge in interest for Trump.\n\nElection Momentum:\n\nAs the dates approach Election Day, search interest for all three terms—Trump, Harris, and Election—steadily increases."
  },
  {
    "objectID": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "href": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "title": "Assignment 2",
    "section": "Method 2: Using gtrendsR Package to collect data",
    "text": "Method 2: Using gtrendsR Package to collect data\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n\n\nDifferences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "Below are the resources for the EPPS 6302 Final Project"
  },
  {
    "objectID": "final_project.html#project-summary",
    "href": "final_project.html#project-summary",
    "title": "EPPS 6302 Final Project",
    "section": "Project Summary",
    "text": "Project Summary\nOur study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\nData Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram\n\n\n\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#data-collection-flow",
    "href": "final_project.html#data-collection-flow",
    "title": "EPPS 6302 Final Project",
    "section": "Data Collection Flow",
    "text": "Data Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "final_project.html#citations",
    "href": "final_project.html#citations",
    "title": "EPPS 6302 Final Project",
    "section": "Citations",
    "text": "Citations\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#contact-me",
    "href": "final_project.html#contact-me",
    "title": "EPPS 6302 Final Project",
    "section": "Contact Me",
    "text": "Contact Me\nIf you have any questions about this project, please feel free to contact me at:\nEmail: oliver.myers@utdallas.edu"
  },
  {
    "objectID": "final_project.html#links-to-external-resources",
    "href": "final_project.html#links-to-external-resources",
    "title": "EPPS 6302 Final Project",
    "section": "Links to External Resources",
    "text": "Links to External Resources\nBelow are the resources for the EPPS 6302 Final Project:\n\n\nFinal Project Paper \nFinal Presentation \nMovie Collection Code (R) \nAnalysis Code (Stata)"
  },
  {
    "objectID": "final_project.html#section",
    "href": "final_project.html#section",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "References\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "epps.6302.home.html",
    "href": "epps.6302.home.html",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research.\n\n\n\n\n\n\n\n\n\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing political discourse through text mining: Twitter trends, presidential speeches, and Wordfish modeling\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Foreign Reserve Data, U.S. Dollar Table, and Downloading Government Documents\n\n\n\n\n\n\n\n\n\n\n\n\nExploring YouTube Search Trends, Video Metadata, and Comment Analysis using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps.6302.home.html#overview",
    "href": "epps.6302.home.html#overview",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6302.home.html#assignments",
    "href": "epps.6302.home.html#assignments",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\n\n\n\n\nAssignment 2\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPPS 6302 Final Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignment 2:Data Preprocessing\n\n\nAssignment 3:Text Mining\n\n\nAssignment 4:Sentiment Analysis\n\n\nAssignment 5:Knowledge Graphs"
  },
  {
    "objectID": "epps.6302.home.html#course-overview",
    "href": "epps.6302.home.html#course-overview",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research.\n\n\n\n\n\n\n\n\n\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing political discourse through text mining: Twitter trends, presidential speeches, and Wordfish modeling\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Foreign Reserve Data, U.S. Dollar Table, and Downloading Government Documents\n\n\n\n\n\n\n\n\n\n\n\n\nExploring YouTube Search Trends, Video Metadata, and Comment Analysis using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps.6302.assignment1.html",
    "href": "epps.6302.assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\nWhat problems do you encounter when working with the dataset?\nThere is a few missing values that result in a NA from the dataset.\nHow to deal with missing values?\nFollowing the assignmnet and example online, to resolve this issue"
  },
  {
    "objectID": "epps.6302.home.html#grading-requirements",
    "href": "epps.6302.home.html#grading-requirements",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Participation: 10%\n\nAssignments (posted on website): 10%\n\nProject Proposal: 20% (Due February 25, 2025)\n\nProgress Reports: 30% (1st due March 25, 2025; 2nd due April 22, 2025)\n\nFinal Project & Presentation: 30% (Final due May 6, 2025)"
  },
  {
    "objectID": "epps.6302.home.html#course-topics",
    "href": "epps.6302.home.html#course-topics",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "This course covers an interdisciplinary approach to knowledge mining, integrating data science, AI, and machine learning with a focus on LLMs, generative AI, and text mining.\nKey topics include: - Data Science Foundations - Machine Learning for Knowledge Mining - Text Mining & NLP - Large Language Models (LLMs) & AI for Research - Causal & Predictive Modeling - Ethics in AI & Knowledge Mining\nFor more details, refer to the syllabus provided by Dr. Ho."
  },
  {
    "objectID": "epps.6302.home.html#assignments-1",
    "href": "epps.6302.home.html#assignments-1",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1: Data Preprocessing\n\n\nAssignment 2: Text Mining\n\n\nAssignment 3: Sentiment Analysis\n\n\nAssignment 4: Knowledge Graphs\n\n\nAssignment 5: Document Clustering\n\n\nFinal Project"
  },
  {
    "objectID": "epps.6302.home.html#assignments-2",
    "href": "epps.6302.home.html#assignments-2",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:\nData Preprocessing\n\n\nAssignment 2:\nText Mining\n\n\nAssignment 3:\nSentiment Analysis\n\n\nAssignment 4:\nKnowledge Graphs\n\n\nAssignment 5:\nDocument Clustering\n\n\nFinal Project:\nCapstone Analysis"
  },
  {
    "objectID": "epps.6302.home.html#assignments-3",
    "href": "epps.6302.home.html#assignments-3",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs\n\n\nAssignment 5:Document Clustering\n\n\nFinal Project:Capstone Analysis"
  },
  {
    "objectID": "particle-js/Particle.Test.html",
    "href": "particle-js/Particle.Test.html",
    "title": "Some particles",
    "section": "",
    "text": "This doc showcases how to use particle.js to get a nice header in your quarto document.\nlet’s dive in.\n\nWhat is particle.js\nIt’s a javascript library that draws stunning particles in a HTML document.\nYou can check it on github, and play with this little tool to find the configuration that is right for you.\n\n\nHow to use it in Quarto?\nIt is possible thanks to a “template partials”. It’s a quarto option that allows to replace the code of a section of the document.\nThe title-block partial can be used to customize the header, and inject some particles in it!\nFor more explanation, check the gallery of Quarto tips and tricks!"
  },
  {
    "objectID": "Particle.Test.html",
    "href": "Particle.Test.html",
    "title": "Some particles",
    "section": "",
    "text": "This doc showcases how to use particle.js to get a nice header in your quarto document.\nlet’s dive in.\n\nWhat is particle.js\nIt’s a javascript library that draws stunning particles in a HTML document.\nYou can check it on github, and play with this little tool to find the configuration that is right for you.\n\n\nHow to use it in Quarto?\nIt is possible thanks to a “template partials”. It’s a quarto option that allows to replace the code of a section of the document.\nThe title-block partial can be used to customize the header, and inject some particles in it!\nFor more explanation, check the gallery of Quarto tips and tricks!"
  },
  {
    "objectID": "particles.js-master/LICENSE.html",
    "href": "particles.js-master/LICENSE.html",
    "title": "Oliver Myers",
    "section": "",
    "text": "The MIT License (MIT)\nCopyright (c) 2015, Vincent Garreau\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "epps.6302.home.html#course-project",
    "href": "epps.6302.home.html#course-project",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "Course Project",
    "text": "Course Project\nThis project explores the relationship between Google Trends search interest and movie ratings on IMDb and Rotten Tomatoes. By analyzing the drop rate of search interest over the first 21 days post-release, we assess whether early online engagement correlates with audience reception. Using web scraping, API integration, and regression analysis in R, this study applies data collection, cleaning, and predictive modeling to uncover insights into digital engagement and consumer behavior.\n\n\nFinal Project Page:Page\n\n\nFinal Project Presentation:PDF\n\n\nProject Final Paper:PDF"
  },
  {
    "objectID": "epps.6354.home.html",
    "href": "epps.6354.home.html",
    "title": "Information Management (6354)",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "epps.6354.home.html#course-overview",
    "href": "epps.6354.home.html#course-overview",
    "title": "Information Management (6354)",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "epps.6354.home.html#assignments",
    "href": "epps.6354.home.html#assignments",
    "title": "Information Management (6354)",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:—-\n\n\nAssignment 2:—-\n\n\nAssignment 3:—-\n\n\nAssignment 4:—–"
  },
  {
    "objectID": "epps.6354.home.html#course-project",
    "href": "epps.6354.home.html#course-project",
    "title": "Information Management (6354)",
    "section": "Course Project",
    "text": "Course Project\nThis project focuses on designing a relational database and interactive dashboard for the Texas Public Safety Association (TPSA) to evaluate the effectiveness of scoring rubrics in competitive events. By integrating student scores, rubric details, event types, and conference data, the system will enable data-driven insights into rubric fairness and effectiveness over time. Using SQL, PostgreSQL, and a Shiny-based web dashboard, this project will provide TPSA staff with an intuitive tool to refine scoring criteria, ensuring fairer and more accurate assessments across events.\n\n\nProject Proposal Paper:PDF\n\n\nProject Proposal Slides:PDF\n\n\nMore to come after the completion of the final project"
  },
  {
    "objectID": "epps.6323.home.html",
    "href": "epps.6323.home.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6323.home.html#course-overview",
    "href": "epps.6323.home.html#course-overview",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6323.home.html#assignments",
    "href": "epps.6323.home.html#assignments",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs"
  },
  {
    "objectID": "epps.6323.home.html#course-project",
    "href": "epps.6323.home.html#course-project",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "Course Project",
    "text": "Course Project\nHere is my proposal for the final project in this course. I will be exploring the topic of “Forecasting User Sentiment in Mobile Apps: A Knowledge Mining Approach”. This project will leverage sentiment analysis, NLP, and predictive modeling to forecast user sentiment in mobile apps, helping developers improve user experience and app ratings.\n\n\nProject Proposal Paper:PDF\n\n\nProject Proposal Slides:PDF\n\n\nMore to come after the completion of the final project"
  },
  {
    "objectID": "assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "href": "assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "title": "Assignment 2",
    "section": "Google Trends Analysis of Political Search Interest (July–November 2024)",
    "text": "Google Trends Analysis of Political Search Interest (July–November 2024)\nThis assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "assignment2.html#method-1-google-trends-csv-data-analysis",
    "href": "assignment2.html#method-1-google-trends-csv-data-analysis",
    "title": "Assignment 2",
    "section": "Method 1: Google Trends CSV Data & Analysis",
    "text": "Method 1: Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#method-2-google-trends-using-gtrendsr-packageusing",
    "href": "assignment2.html#method-2-google-trends-using-gtrendsr-packageusing",
    "title": "Assignment 2",
    "section": "Method 2: Google Trends Using “gtrendsR” PackageUsing",
    "text": "Method 2: Google Trends Using “gtrendsR” PackageUsing\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion"
  },
  {
    "objectID": "assignment2.html#discussion",
    "href": "assignment2.html#discussion",
    "title": "Assignment 2",
    "section": "Discussion",
    "text": "Discussion\n\nDifferences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment2.html#method-1",
    "href": "assignment2.html#method-1",
    "title": "Assignment 2",
    "section": "Method 1:",
    "text": "Method 1:\n\nGoogle Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#method-2",
    "href": "assignment2.html#method-2",
    "title": "Assignment 2",
    "section": "Method 2:",
    "text": "Method 2:\n\nGoogle Trends Using “gtrendsR” PackageUsing\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion"
  },
  {
    "objectID": "assignment2.html#google-trends-csv-data-analysis",
    "href": "assignment2.html#google-trends-csv-data-analysis",
    "title": "Assignment 2",
    "section": "1. Google Trends CSV Data & Analysis",
    "text": "1. Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n\nLoading the Google Trends CSV Data\nThe dataset is read into R, and unnecessary rows are removed to clean the data.\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\n\n\nPlotting Google Search Trends Over Time\nThis plot visualizes the search interest over time for Trump, Harris, and Election, with key event dates highlighted.\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\nFigure 1: Google Trends Search Interest Over Time: Search interest for ‘Trump,’ ‘Harris,’ and ‘Election’ from July to November 2024, highlighting key political events influencing search spikes.\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#google-trends-using-gtrendsr-packageusing",
    "href": "assignment2.html#google-trends-using-gtrendsr-packageusing",
    "title": "Assignment 2",
    "section": "2. Google Trends Using “gtrendsR” PackageUsing",
    "text": "2. Google Trends Using “gtrendsR” PackageUsing\nn this method, the gtrendsR package in R was used to directly query Google Trends for real-time search interest data on “Trump,” “Harris,” and “Election.” Instead of manually downloading a CSV, this approach allows for automated data retrieval over a specified time range.\n\nFetching Data from Google Trends API\nThis code retrieves real-time Google search interest data directly from Google’s API.\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\nFigure 2: Google Trends Data Retrieved via API: Real-time search interest trends retrieved using the gtrendsR package, offering an automated alternative to manual CSV downloads.\n\n\n\n\nKey Advantages\n\nAutomated Data Collection → Eliminates the need for manual downloads.\nReal-Time Updates → Ensures the latest data can be pulled dynamically.\nReproducibility → Allows future analysis with updated data."
  },
  {
    "objectID": "assignment2.html#discussion-differences-between-the-two-methods",
    "href": "assignment2.html#discussion-differences-between-the-two-methods",
    "title": "Assignment 2",
    "section": "Discussion: Differences between the two methods:",
    "text": "Discussion: Differences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment3.html#analyzing-political-discourse-using-text-data",
    "href": "assignment3.html#analyzing-political-discourse-using-text-data",
    "title": "Assignment 3",
    "section": "Analyzing Political Discourse Using Text Data",
    "text": "Analyzing Political Discourse Using Text Data\nThis assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "assignment3.html#biden-xi-summit-twitter-analysis",
    "href": "assignment3.html#biden-xi-summit-twitter-analysis",
    "title": "Assignment 3",
    "section": "1. Biden-Xi Summit Twitter Analysis",
    "text": "1. Biden-Xi Summit Twitter Analysis\n\nLoading Twitter Data\nThe dataset consists of tweets discussing the Biden-Xi summit (November 2021). We load the dataset using readr and extract the tweet text.\n\n\nPreprocessing the Text Data\nWe tokenize the tweet text, remove punctuation, and create a document-feature matrix (DFM), which converts text into a structured numerical format. then to analyze discussion topics, we extract hashtags from tweets and identify the most frequently used ones. Finaly, to visualize the relationships between hashtags, we create a feature co-occurrence matrix (FCM) and plot a network graph.\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\nFigure 1: Hashtag Network: The network visualization highlights key discussion topics related to the summit. Central hashtags like #biden and #china dominate the conversation, while #humanrights and #uyghurs indicate concerns over human rights issues.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event."
  },
  {
    "objectID": "assignment3.html#u.s.-presidential-inaugural-speeches",
    "href": "assignment3.html#u.s.-presidential-inaugural-speeches",
    "title": "Assignment 3",
    "section": "2. U.S. Presidential Inaugural Speeches",
    "text": "2. U.S. Presidential Inaugural Speeches\nThis section examines U.S. presidential inaugural speeches over time, analyzing their linguistic trends and thematic focus.\n\nAnalyzing Early Inaugural Speeches & Keyword Trends Over Time\nWe create a document-feature matrix (DFM) for speeches from 1789 to 1826, removing common stopwords. We analyze post-1949 speeches and generate x-ray plots for key terms like liberty.\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\nFigure 2: Keyword Frequency of ‘Liberty’: This visualization highlights the usage pattern of ‘liberty’ in presidential speeches, showing how its prominence fluctuates over time.\n\n\n\n\n\n\nComparing Key Terms in Presidential Speeches\nWe generate x-ray plots for three key words: foreign, we, and god.\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\nFigure 3: Word Frequency Comparison: ‘Foreign’ was more common during Cold War-era speeches. ‘We’ is frequently used by presidents emphasizing unity (e.g., Obama, Biden). ‘God’ appears consistently toward the end of speeches, reflecting a tradition of invoking divine guidance.\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery."
  },
  {
    "objectID": "assignment3.html#wordfish-scaling-model",
    "href": "assignment3.html#wordfish-scaling-model",
    "title": "Assignment 3",
    "section": "3. Wordfish Scaling Model",
    "text": "3. Wordfish Scaling Model\nThe Wordfish model is an unsupervised text scaling method that estimates document positions based on word frequencies.\n\nApplying Wordfish to the 2010 Irish Budget Speeches\nWe use Wordfish to analyze Irish parliamentary speeches and estimate ideological positions.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\nFigure 4: Wordfish model with document positions: This plot visualizes word scaling based on frequency and distribution in Irish Budget 2010 speeches. Highlighted terms like government, economy, and deficit shape document positioning, while central clustering suggests shared vocabulary. Words at the extremes indicate stronger differentiation in political discourse.\n\n\n\n\n\n\nVisualizing Word Positions\nThis plot highlights the relative importance of words in different political contexts.\n\n# Load necessary libraries\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\nFigure 5: Estimated Document Positions by Political Party: This plot shows estimated ideological positions of politicians based on word usage. Fianna Fáil (FF) leans right, Sinn Féin (SF) and Labour (LAB) lean left, while Fine Gael (FG) and the Greens vary. Black dots represent individual positions, with confidence intervals highlighting linguistic and ideological differences.\n\n\n\n\n\n\nScaling Political Documents by Party\nWe visualize document positions grouped by political party.\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\nFigure 6: Estimated Document Positions by Political Party: This plot visualizes document positions using Correspondence Analysis (CA), grouping speeches by political party. It highlights linguistic differences across parties, mapping ideological tendencies based on word usage in Irish Budget 2010 speeches.\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772.\n\n\n\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties."
  },
  {
    "objectID": "assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "href": "assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "title": "Assignment 4",
    "section": "Automating Web Scraping for Economic and Government Data",
    "text": "Automating Web Scraping for Economic and Government Data\nThis assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website.\n\n\n1. Scraping Foreign Reserve Data\n\nReading the Wikipedia Page using the rvest package\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $638,261         2025-02-07 7,654  \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "assignment4.html#scraping-u.s.-dollar-table",
    "href": "assignment4.html#scraping-u.s.-dollar-table",
    "title": "Assignment 4",
    "section": "2. Scraping U.S. Dollar Table",
    "text": "2. Scraping U.S. Dollar Table\nThis section extracts a table from the Wikipedia page on U.S. currency, removing unnecessary image columns.\n\n\n\nFigure: Wikipedia Page on the U.S. Dollar – The Wikipedia entry for the United States dollar (USD), detailing its history, value, and international use. The right panel displays various denominations of U.S. banknotes.\n\n\n\nReading the U.S. Dollar Wikipedia Page\nThe script extracts the third table using XPath and removes unnecessary columns. This cleaned dataset helps track changes in U.S. banknotes over time.\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(rvest)\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/United_States_dollar'\n\n# Read the webpage\nusd_page &lt;- read_html(url)\n\n# Extract the third table on the page using XPath (skipping image columns 2 and 3)\nusd_table &lt;- usd_page %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[3]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nusd_data &lt;- usd_table[[1]]\n\n# Remove columns 2 and 3 (image columns)\nusd_data &lt;- usd_data %&gt;%\n  select(-`Front`, -`Reverse`)\n\n# Rename columns for clarity\ncolnames(usd_data) &lt;- c(\"Denomination\", \"Portrait\", \"Reverse_Motif\", \n                        \"First_Series\", \"Latest_Series\", \"Circulation\")\n\n# Clean up the data (if necessary)\nusd_data &lt;- usd_data %&gt;%\n  filter(!is.na(Denomination) & Denomination != \"\")  # Remove empty rows\n\n# View the cleaned and structured data\nhead(usd_data)\n\n# A tibble: 6 × 6\n  Denomination   Portrait   Reverse_Motif First_Series Latest_Series Circulation\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n1 One dollar     George Wa… Great Seal o… Series 1963… Series 2021[… Wide       \n2 Two dollars    Thomas Je… Declaration … Series 1976  Series 2017A  Limited[48]\n3 Five dollars   Abraham L… Lincoln Memo… Series 2006  Series 2021[… Wide       \n4 Ten dollars    Alexander… Treasury Bui… Series 2004A Series 2017A  Wide       \n5 Twenty dollars Andrew Ja… White House   Series 2004  Series 2017A  Wide       \n6 Fifty dollars  Ulysses S… United State… Series 2004  Series 2017A  Wide"
  },
  {
    "objectID": "assignment4.html#downloading-government-documents",
    "href": "assignment4.html#downloading-government-documents",
    "title": "Assignment 4",
    "section": "3. Downloading Government Documents",
    "text": "3. Downloading Government Documents\nThis section automates the bulk download of government bills related to water policy using “https://www.govinfo.gov/app/search/”.\n\n\n\nFigure: GovInfo Search Portal – The homepage of GovInfo, a U.S. government website for accessing official documents. Users can search for records using the search bar or browse by category, date, committee, or author.\n\n\n\nReading the Government Search Results\nThe script downloads 10 Congressional bills using a loop with error handling.\n\nlibrary(purrr)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\nsetwd(\"/Users/olivermyers/MyWebsite/govtdata.assignent04\")\n\n# csv downloaed from https://www.govinfo.gov/app/search/ and searching \"water\", filtering congressonal bills from 2024\n\n# Read the CSV file without skipping rows\ngovfiles &lt;- read.csv(file = \"/Users/olivermyers/MyWebsite/govinfo-search-results.csv\", skip = 2, header = FALSE)\n\ncolnames(govfiles) &lt;- govfiles[1, ]\ngovfiles &lt;- govfiles[-1, ]\nrownames(govfiles) &lt;- NULL\ncolnames(govfiles) &lt;- make.names(colnames(govfiles), unique = TRUE)\nhead(govfiles$packageId)\n\n[1] \"BILLS-118hr5770rh\"  \"BILLS-118hr5770rfs\" \"BILLS-118hr5770eh\" \n[4] \"BILLS-118hr8096ih\"  \"BILLS-118s4188is\"   \"BILLS-118hr7065ih\" \n\n# Preparing for bulk download of government documents\ngovfiles$id &lt;- govfiles$packageId\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# saving files into govdata.assignent04 folder\nsave_dir &lt;- \"/Users/olivermyers/MyWebsite/govtdata.assignent04\"\n\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    # Ensure the file path includes a proper separator\n    destfile &lt;- file.path(save_dir, paste0(\"govfiles_\", id, \".pdf\"))\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to avoid server throttling\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n\n## Download the first 10 from the csv file\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;%  # Change to limit to the first 10 files\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 25.64369 secs\n\n# List and print all files in the directory\nall_files &lt;- list.files(path = save_dir, full.names = FALSE)  \nprint(\"Files in the govtdata.assignent04 directory:\")\n\n[1] \"Files in the govtdata.assignent04 directory:\"\n\nhead(all_files)\n\n[1] \"govfiles_BILLS-118hr10150ih.pdf\" \"govfiles_BILLS-118hr3675rh.pdf\" \n[3] \"govfiles_BILLS-118hr5770eh.pdf\"  \"govfiles_BILLS-118hr5770rfs.pdf\"\n[5] \"govfiles_BILLS-118hr5770rh.pdf\"  \"govfiles_BILLS-118hr7021ih.pdf\""
  },
  {
    "objectID": "assignment4.html#discussion",
    "href": "assignment4.html#discussion",
    "title": "Assignment 4",
    "section": "Discussion:",
    "text": "Discussion:\n\nSimple report on difficulties encountered in the scraping process:\nScraping data using the first method, rvest, was initially a bit challenging for me. The need to inspect elements on the webpage and copy the XPath IDs to make the code work was a new concept. Additionally, some parts of the code were not as straightforward compared to the second method. That said, I found rvest to be significantly more useful in the long run because it allows for automated web scraping of large amounts of data from various webpage elements. Once I became familiar with the process, I appreciated the potential for efficiently formatting and organizing scraped data, even if it was tricky to set up at first.\nThe second method, on the other hand, was easier to use but felt less practical. This approach requires manually finding and downloading the necessary list yourself, which limits its automation capabilities. Initially, I encountered issues with downloading the files into the correct folder, but after consulting ChatGPT, I resolved the problem and successfully downloaded the files to the appropriate directory.\nIn conclusion, both methods have their advantages and can produce highly usable data. However, in my personal opinion, the rvest method stands out for its versatility and ability to scrape and format large-scale data efficiently. Although it requires more time and effort to understand and implement correctly, its potential for automating repetitive scraping tasks makes it the more valuable option overall. This could then be improved with more automation and cleaning steps build into the flow when using rvest."
  },
  {
    "objectID": "assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "href": "assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "title": "Assignment 5",
    "section": "Analyzing YouTube News Coverage on the 2024 U.S. Election",
    "text": "Analyzing YouTube News Coverage on the 2024 U.S. Election\nThis assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "assignment5.html#collecting-youtube-video-data",
    "href": "assignment5.html#collecting-youtube-video-data",
    "title": "Assignment 5",
    "section": "1. Collecting YouTube Video Data",
    "text": "1. Collecting YouTube Video Data\n\nRun YouTubenews01.R (prerequisites: YouTube developer API)\nThe code first authenticates the YouTube API using yt_oauth(). Then, it searches for videos related to “US election 2024” and extracts:\n\nlibrary(tuber)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(httr)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:httr':\n\n    content\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n# This one works today #\nyt_oauth(\"449459649595-3nqbg06e1ij0i2184ulc19q33lc08tvo.apps.googleusercontent.com\", \"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\", token = \"\")\n\n### Step 3: Download YouTube Data\n#### Here’s an example of collecting data on the “US election 2024.”\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\nAuto-refreshing stale OAuth token.\n\n#### Display the first few rows\nvideo_data &lt;- head(yt_uselection2024)\n\nsubset_video_data &lt;- video_data %&gt;%\n  select(video_id, channelId, title)\n\nprint(subset_video_data)\n\n     video_id                channelId\n1 uT9s4BXcv6w UCXIJgqnII2ZOINSWNOGFThA\n2 X4FtUJdKFCE UCeY0bbntWzzVIaj2z3QigXg\n3 9olb6OvXjKg UCupvZG-5ko_eiXAupbDfxWw\n4 BJkf7e_cZ68 UCH1oRy1dINbMVp3UFWrKP0w\n5 ORJI0-VSykQ UCeY0bbntWzzVIaj2z3QigXg\n6 0EPtWxTJMYc UCeY0bbntWzzVIaj2z3QigXg\n                                                                    title\n1        Donald Trump speaks after winning the 2024 Presidential Election\n2 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW\n3                     Trump wins 2024 presidential election, CNN projects\n4                     What polls say about the 2024 presidential election\n5               Possible paths to a win in the 2024 presidential election\n6 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW"
  },
  {
    "objectID": "assignment5.html#word-cloud-of-video-titles",
    "href": "assignment5.html#word-cloud-of-video-titles",
    "title": "Assignment 5",
    "section": "2. Word Cloud of Video Titles",
    "text": "2. Word Cloud of Video Titles\nThe most frequently used words in video titles are extracted and visualized.\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n\nFigure 2: Word Cloud of Video Titles – Most frequent words in YouTube video titles about the 2024 U.S. election."
  },
  {
    "objectID": "assignment5.html#analyzing-video-publication-dates",
    "href": "assignment5.html#analyzing-video-publication-dates",
    "title": "Assignment 5",
    "section": "3. Analyzing Video Publication Dates",
    "text": "3. Analyzing Video Publication Dates\nThe following code extracts publish dates and plots the frequency of videos published over time.\n\n### 4.2. Plot Video Publish Dates\n# Format publish dates and aggregate data\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n\nFigure 3: Video Publication Timeline – Distribution of videos published over time."
  },
  {
    "objectID": "assignment5.html#identifying-top-youtube-channels",
    "href": "assignment5.html#identifying-top-youtube-channels",
    "title": "Assignment 5",
    "section": "4. Identifying Top YouTube Channels",
    "text": "4. Identifying Top YouTube Channels\nThe top 10 channels with the highest number of videos related to the election are visualized.\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\nSelecting by n\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")"
  },
  {
    "objectID": "assignment5.html#analyzing-cnns-youtube-channel",
    "href": "assignment5.html#analyzing-cnns-youtube-channel",
    "title": "Assignment 5",
    "section": "5. Analyzing CNN’s YouTube Channel",
    "text": "5. Analyzing CNN’s YouTube Channel\nThe code extracts CNN’s channel statistics, including:\n\nTotal views\nSubscriber count\nTotal videos\n\n\n## Required Libraries\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(ggplot2)\n\n## CNN Channel ID\ncnn_channel_id &lt;- \"UCupvZG-5ko_eiXAupbDfxWw\"\n\n## get channel stats\ncnn_data &lt;- get_channel_stats(channel_id = \"UCupvZG-5ko_eiXAupbDfxWw\", mine = NULL)\n\nChannel Title: CNN \nNo. of Views: 17428751276 \nNo. of Subscribers: 17700000 \nNo. of Videos: 170170 \n\ncnn_stats = cnn_data$statistics\nhead(cnn_stats)\n\n$viewCount\n[1] \"17428751276\"\n\n$subscriberCount\n[1] \"17700000\"\n\n$hiddenSubscriberCount\n[1] FALSE\n\n$videoCount\n[1] \"170170\""
  },
  {
    "objectID": "assignment5.html#youtube-comments-sentiment-analysis",
    "href": "assignment5.html#youtube-comments-sentiment-analysis",
    "title": "Assignment 5",
    "section": "6. YouTube Comments Sentiment Analysis",
    "text": "6. YouTube Comments Sentiment Analysis\nThe code collects comments from a specific CNN video and processes the text for analysis.\n\nAnalyze the stats and comments:\n\nvideo_id =  \"Yzb5LGwt6LA\"\n\n## Get Video Statistics\nvideo_stats &lt;- get_stats(video_id)\ncat(\"Video Stats:\\n\")\n\nVideo Stats:\n\nhead(video_stats)\n\n$id\n[1] \"Yzb5LGwt6LA\"\n\n$viewCount\n[1] \"170169\"\n\n$likeCount\n[1] \"1797\"\n\n$favoriteCount\n[1] \"0\"\n\n$commentCount\n[1] \"695\"\n\n\n\nvideo_id =  \"Yzb5LGwt6LA\"\nvideocomments &lt;- get_all_comments(video_id)\nhead(videocomments$textOriginal)\n\n[1] \"Hollywood CNN! Welcome fake news!\"                                                                                                                                                                                   \n[2] \"6:28 damn this dude was completely  wrong\"                                                                                                                                                                           \n[3] \"Omg\"                                                                                                                                                                                                                 \n[4] \"Abandoned equipments - tanks,  APCs, airplanes ,etc - showed that Assad ‘s army is in complete disarray.\"                                                                                                            \n[5] \"May the innocent people be saved by God.\"                                                                                                                                                                            \n[6] \"The winner of the conflict is not the takfiris but Toyota, the favorite 'warhorse' of the Islamists which runs faster than the Russian tanks of Assad which enabled the Islamists to reach Aleppo in no time at all.\"\n\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n\nCan you use quanteda to analyze the text data from YouTube comments?\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"&gt;\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n      feature frequency rank docfreq group\n1         not        76    1      67   all\n2      rebels        76    1      70   all\n3       syria        67    3      55   all\n4          us        55    4      46   all\n5      people        53    5      32   all\n6          no        43    6      32   all\n7      russia        41    7      35   all\n8         cnn        40    8      31   all\n9         war        40    8      36   all\n10       like        36   10      29   all\n11        has        35   11      20   all\n12 terrorists        35   11      30   all\n13         we        34   13      28   all\n14        now        33   14      31   all\n15      there        32   15      27   all\n16       when        32   15      27   all\n17        who        31   17      29   all\n18        why        29   18      27   all\n19    because        27   19      22   all\n20       isis        27   19      25   all\n21       them        25   21      22   all\n22      putin        25   21      19   all\n23         he        25   21      17   all\n24       just        25   21      23   all\n25     spirit        25   21       2   all\n26      world        25   21      21   all\n27     syrian        24   27      18   all\n\n\n\n# Visualize the Most Frequent Words\ntstat_freq %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\", title = \"Top 15 Words in YouTube Comments\") +\n  theme_minimal()\n\n\n\n\nFigure 6: Most Frequent Words in YouTube Comments – A ranking of the 15 most common words used in YouTube comments on election-related videos."
  },
  {
    "objectID": "assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "href": "assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "title": "Assignment 5",
    "section": "7. Generating a Word Cloud of YouTube Comments",
    "text": "7. Generating a Word Cloud of YouTube Comments\n\n# Create a Word Cloud\nset.seed(132)\ntextplot_wordcloud(dfm_nonzero, max_words = 100)\n\n\n\n\nFigure 7: YouTube Comment Word Cloud – Visual representation of frequently used words in YouTube comments."
  },
  {
    "objectID": "assignment5.html#discussion",
    "href": "assignment5.html#discussion",
    "title": "Assignment 5",
    "section": "Discussion:",
    "text": "Discussion:\n\nAssignment Reflection:\nThis assignment required extensive effort to successfully retrieve and analyze YouTube comments from a CNN video. To achieve this, I leveraged the tuber package for data collection and quanteda for text analysis, with additional coding assistance from ChatGPT and reference to online documentation.\nThe comment processing involved tokenizing words, removing emojis, filtering out usernames, and eliminating common stopwords to ensure meaningful analysis. By visualizing the most frequently used words, I was able to identify dominant themes in the discussion.\nThe analysis revealed a predominantly negative sentiment, with frequent mentions of terms related to terrorism, conflict in the Middle East, and Syria. This suggests that the video’s content likely revolves around geopolitical tensions, aligning with CNN’s coverage focus. The findings highlight how YouTube comments can reflect public sentiment and engagement with political topics, making social media a powerful tool for understanding audience reactions to news coverage."
  },
  {
    "objectID": "nlp_assignment05.html",
    "href": "nlp_assignment05.html",
    "title": "nlp_assignment05",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "nlp_assignment05.html#old-model",
    "href": "nlp_assignment05.html#old-model",
    "title": "nlp_assignment05",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "href": "nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "title": "nlp_assignment05",
    "section": "New Model with improved predictive ability:",
    "text": "New Model with improved predictive ability:\n\n# Install and load required packages\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"doParallel\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\nif (length(new_packages)) install.packages(new_packages)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(ranger)\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Data ingestion & preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\nset.seed(123)\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Define Preprocessing Recipe with enhancements\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  # Normalize the text (using NFC form)\n  step_text_normalization(text, normalization_form = \"nfkc\") %&gt;%\n  # Tokenize text into words\n  step_tokenize(text, token = \"words\") %&gt;%\n  # Remove stopwords (default language is English)\n  step_stopwords(text) %&gt;%\n  # Create n-grams (unigrams and bigrams)\n  step_ngram(text, num_tokens = 2, min_num_tokens = 1) %&gt;%\n  # Filter tokens to limit the number of features\n  step_tokenfilter(text, max_tokens = 1000, min_times = 2) %&gt;%\n  # Create TF-IDF features\n  step_tfidf(text) %&gt;%\n  # Normalize predictors (if needed)\n  step_normalize(all_predictors())\n\n# Model Specification: Random Forest tuned on mtry and min_n (trees fixed at 1000)\nrf_spec &lt;- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n# Create workflow\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Set up 5-fold cross-validation with stratification\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# Define a grid for tuning mtry and min_n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Register parallel backend to speed up tuning\ndoParallel::registerDoParallel()\n\n# Tune model with cross-validation and evaluate using accuracy and kappa\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\nWarning: ! tune detected a parallel backend registered with foreach but no backend\n  registered with future.\nℹ Support for parallel processing with foreach was soft-deprecated in tune\n  1.2.1.\nℹ See ?parallelism (`?tune::parallelism()`) to learn more.\n\n# Review best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     2 Preprocessor1_Model01\n\n# Finalize workflow with the best hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% fit(data = train_data)\n\nWarning: max_tokens was set to 1000, but only 236 was available and selected.\n\n# Evaluate final model on the test set\nfinal_preds &lt;- final_fit %&gt;% \n  predict(new_data = test_data) %&gt;% \n  bind_cols(test_data)\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.883\n2 kap      multiclass     0.870\n\n# Display the confusion matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             0           0       0      0\n  Entertainment       0         0             6           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       6      0\n  Health              4         0             0           0       0      6\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          6      0\n  Travel               0      0          0      6\n\n# Predict on new samples and display the results\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples)\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Sports     \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Finance    \n\n\n\nHow I improved the predication ability:\nThe new model is better because it achieves an accuracy of about 88.3% and a kappa of around 0.87, which clearly shows it is making more reliable predictions and generalizes well to new data. We changed the preprocessing by normalizing the text to maintain consistency, incorporating n-grams to capture context beyond single words, and tuning the token filtering settings to retain a more representative vocabulary while cutting out noise. In addition, we refined the hyperparameter tuning process for the random forest by focusing on key parameters such as mtry and min_n. Together, these adjustments have resulted in a model that fits the data much more effectively and performs significantly better than the previous version.\n\nThe model before preformed with:\naccuracy multiclass 0.733\nkap multiclass 0.704\n\n\nWhere as now it preforms with:\naccuracy multiclass 0.883\nkap multiclass 0.870\nDisclaimer: I Used chat GPT 03-mini-high to help with optimization and to make better predictions*"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-csv-data-analysis",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-csv-data-analysis",
    "title": "",
    "section": "1. Google Trends CSV Data & Analysis",
    "text": "1. Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n\nLoading the Google Trends CSV Data\nThe dataset is read into R, and unnecessary rows are removed to clean the data.\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\n\n\nPlotting Google Search Trends Over Time\nThis plot visualizes the search interest over time for Trump, Harris, and Election, with key event dates highlighted.\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\nFigure 1: Google Trends Search Interest Over Time: Search interest for ‘Trump,’ ‘Harris,’ and ‘Election’ from July to November 2024, highlighting key political events influencing search spikes.\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-using-gtrendsr-packageusing",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-using-gtrendsr-packageusing",
    "title": "",
    "section": "2. Google Trends Using “gtrendsR” PackageUsing",
    "text": "2. Google Trends Using “gtrendsR” PackageUsing\nn this method, the gtrendsR package in R was used to directly query Google Trends for real-time search interest data on “Trump,” “Harris,” and “Election.” Instead of manually downloading a CSV, this approach allows for automated data retrieval over a specified time range.\n\nFetching Data from Google Trends API\nThis code retrieves real-time Google search interest data directly from Google’s API.\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\nFigure 2: Google Trends Data Retrieved via API: Real-time search interest trends retrieved using the gtrendsR package, offering an automated alternative to manual CSV downloads.\n\n\n\n\nKey Advantages\n\nAutomated Data Collection → Eliminates the need for manual downloads.\nReal-Time Updates → Ensures the latest data can be pulled dynamically.\nReproducibility → Allows future analysis with updated data."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#discussion-differences-between-the-two-methods",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#discussion-differences-between-the-two-methods",
    "title": "",
    "section": "Discussion: Differences between the two methods:",
    "text": "Discussion: Differences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html",
    "href": "pages/EPPS_6302/FinalProject/final_project.html",
    "title": "",
    "section": "",
    "text": "Our study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\n\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\n\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#project-summary",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#project-summary",
    "title": "",
    "section": "",
    "text": "Our study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\n\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\n\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#contact-me",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#contact-me",
    "title": "",
    "section": "Contact Me",
    "text": "Contact Me\nIf you have any questions about this project, please feel free to contact me at:\nEmail: oliver.myers@utdallas.edu"
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research."
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html#course-overview",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research."
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html#course-project",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nThis project explores the relationship between Google Trends search interest and movie ratings on IMDb and Rotten Tomatoes. By analyzing the drop rate of search interest over the first 21 days post-release, we assess whether early online engagement correlates with audience reception. Using web scraping, API integration, and regression analysis in R, this study applies data collection, cleaning, and predictive modeling to uncover insights into digital engagement and consumer behavior."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html",
    "title": "",
    "section": "",
    "text": "This assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "title": "",
    "section": "",
    "text": "This assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#collecting-youtube-video-data",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#collecting-youtube-video-data",
    "title": "",
    "section": "1. Collecting YouTube Video Data",
    "text": "1. Collecting YouTube Video Data\n\nRun YouTubenews01.R (prerequisites: YouTube developer API)\nThe code first authenticates the YouTube API using yt_oauth(). Then, it searches for videos related to “US election 2024” and extracts:\n\nlibrary(tuber)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(httr)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:httr':\n\n    content\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n# This one works today #\nyt_oauth(\"449459649595-3nqbg06e1ij0i2184ulc19q33lc08tvo.apps.googleusercontent.com\", \"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\", token = \"\")\n\n### Step 3: Download YouTube Data\n#### Here’s an example of collecting data on the “US election 2024.”\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\n#### Display the first few rows\nvideo_data &lt;- head(yt_uselection2024)\n\nsubset_video_data &lt;- video_data %&gt;%\n  select(video_id, channelId, title)\n\nprint(subset_video_data)\n\n     video_id                channelId\n1 A3AXszRgX7I UCO0akufu9MOzyz3nvGIXAAw\n2 X4FtUJdKFCE UCeY0bbntWzzVIaj2z3QigXg\n3 BJkf7e_cZ68 UCH1oRy1dINbMVp3UFWrKP0w\n4 uT9s4BXcv6w UCXIJgqnII2ZOINSWNOGFThA\n5 ORJI0-VSykQ UCeY0bbntWzzVIaj2z3QigXg\n6 9olb6OvXjKg UCupvZG-5ko_eiXAupbDfxWw\n                                                                    title\n1                                  Donald Trump wins the 2024 US election\n2 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW\n3                     What polls say about the 2024 presidential election\n4        Donald Trump speaks after winning the 2024 Presidential Election\n5               Possible paths to a win in the 2024 presidential election\n6                     Trump wins 2024 presidential election, CNN projects"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#word-cloud-of-video-titles",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#word-cloud-of-video-titles",
    "title": "",
    "section": "2. Word Cloud of Video Titles",
    "text": "2. Word Cloud of Video Titles\nThe most frequently used words in video titles are extracted and visualized.\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n\nFigure 2: Word Cloud of Video Titles – Most frequent words in YouTube video titles about the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-video-publication-dates",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-video-publication-dates",
    "title": "",
    "section": "3. Analyzing Video Publication Dates",
    "text": "3. Analyzing Video Publication Dates\nThe following code extracts publish dates and plots the frequency of videos published over time.\n\n### 4.2. Plot Video Publish Dates\n# Format publish dates and aggregate data\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n\nFigure 3: Video Publication Timeline – Distribution of videos published over time."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#identifying-top-youtube-channels",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#identifying-top-youtube-channels",
    "title": "",
    "section": "4. Identifying Top YouTube Channels",
    "text": "4. Identifying Top YouTube Channels\nThe top 10 channels with the highest number of videos related to the election are visualized.\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\nSelecting by n\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-cnns-youtube-channel",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-cnns-youtube-channel",
    "title": "",
    "section": "5. Analyzing CNN’s YouTube Channel",
    "text": "5. Analyzing CNN’s YouTube Channel\nThe code extracts CNN’s channel statistics, including:\n\nTotal views\nSubscriber count\nTotal videos\n\n\n## Required Libraries\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(ggplot2)\n\n## CNN Channel ID\ncnn_channel_id &lt;- \"UCupvZG-5ko_eiXAupbDfxWw\"\n\n## get channel stats\ncnn_data &lt;- get_channel_stats(channel_id = \"UCupvZG-5ko_eiXAupbDfxWw\", mine = NULL)\n\nChannel Title: CNN \nNo. of Views: 17776317625 \nNo. of Subscribers: 17900000 \nNo. of Videos: 170959 \n\ncnn_stats = cnn_data$statistics\nhead(cnn_stats)\n\n$viewCount\n[1] \"17776317625\"\n\n$subscriberCount\n[1] \"17900000\"\n\n$hiddenSubscriberCount\n[1] FALSE\n\n$videoCount\n[1] \"170959\""
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#youtube-comments-sentiment-analysis",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#youtube-comments-sentiment-analysis",
    "title": "",
    "section": "6. YouTube Comments Sentiment Analysis",
    "text": "6. YouTube Comments Sentiment Analysis\nThe code collects comments from a specific CNN video and processes the text for analysis.\n\nAnalyze the stats and comments:\n\nvideo_id =  \"Yzb5LGwt6LA\"\n\n## Get Video Statistics\nvideo_stats &lt;- get_stats(video_id)\ncat(\"Video Stats:\\n\")\n\nVideo Stats:\n\nhead(video_stats)\n\n$id\n[1] \"Yzb5LGwt6LA\"\n\n$viewCount\n[1] \"170244\"\n\n$likeCount\n[1] \"1791\"\n\n$favoriteCount\n[1] \"0\"\n\n$commentCount\n[1] \"694\"\n\n\n\nvideo_id =  \"Yzb5LGwt6LA\"\nvideocomments &lt;- get_all_comments(video_id)\nhead(videocomments$textOriginal)\n\n[1] \"Hollywood CNN! Welcome fake news!\"                                                                                                                                                                                   \n[2] \"6:28 damn this dude was completely  wrong\"                                                                                                                                                                           \n[3] \"Omg\"                                                                                                                                                                                                                 \n[4] \"Abandoned equipments - tanks,  APCs, airplanes ,etc - showed that Assad ‘s army is in complete disarray.\"                                                                                                            \n[5] \"May the innocent people be saved by God.\"                                                                                                                                                                            \n[6] \"The winner of the conflict is not the takfiris but Toyota, the favorite 'warhorse' of the Islamists which runs faster than the Russian tanks of Assad which enabled the Islamists to reach Aleppo in no time at all.\"\n\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n\nCan you use quanteda to analyze the text data from YouTube comments?\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"&gt;\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n      feature frequency rank docfreq group\n1      rebels        76    1      70   all\n2         not        75    2      66   all\n3       syria        66    3      54   all\n4          us        54    4      45   all\n5      people        53    5      32   all\n6          no        41    6      31   all\n7         cnn        40    7      31   all\n8         war        40    7      36   all\n9      russia        40    7      34   all\n10       like        36   10      29   all\n11 terrorists        35   11      30   all\n12        has        34   12      19   all\n13        now        33   13      31   all\n14         we        33   13      27   all\n15      there        32   15      27   all\n16       when        32   15      27   all\n17        who        31   17      29   all\n18        why        28   18      26   all\n19    because        27   19      22   all\n20       isis        27   19      25   all\n21       them        25   21      22   all\n22      putin        25   21      19   all\n23         he        25   21      17   all\n24       just        25   21      23   all\n25     spirit        25   21       2   all\n26      world        25   21      21   all\n27     syrian        24   27      18   all\n\n\n\n# Visualize the Most Frequent Words\ntstat_freq %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\", title = \"Top 15 Words in YouTube Comments\") +\n  theme_minimal()\n\n\n\n\nFigure 6: Most Frequent Words in YouTube Comments – A ranking of the 15 most common words used in YouTube comments on election-related videos."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "title": "",
    "section": "7. Generating a Word Cloud of YouTube Comments",
    "text": "7. Generating a Word Cloud of YouTube Comments\n\n# Create a Word Cloud\nset.seed(132)\ntextplot_wordcloud(dfm_nonzero, max_words = 100)\n\n\n\n\nFigure 7: YouTube Comment Word Cloud – Visual representation of frequently used words in YouTube comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#discussion",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#discussion",
    "title": "",
    "section": "Discussion:",
    "text": "Discussion:\n\nAssignment Reflection:\nThis assignment required extensive effort to successfully retrieve and analyze YouTube comments from a CNN video. To achieve this, I leveraged the tuber package for data collection and quanteda for text analysis, with additional coding assistance from ChatGPT and reference to online documentation.\nThe comment processing involved tokenizing words, removing emojis, filtering out usernames, and eliminating common stopwords to ensure meaningful analysis. By visualizing the most frequently used words, I was able to identify dominant themes in the discussion.\nThe analysis revealed a predominantly negative sentiment, with frequent mentions of terms related to terrorism, conflict in the Middle East, and Syria. This suggests that the video’s content likely revolves around geopolitical tensions, aligning with CNN’s coverage focus. The findings highlight how YouTube comments can reflect public sentiment and engagement with political topics, making social media a powerful tool for understanding audience reactions to news coverage."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#course-overview",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#assignments",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#assignments",
    "title": "",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!"
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#course-project",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nHere is my proposal for the final project in this course. I will be exploring the topic of “Forecasting User Sentiment in Mobile Apps: A Knowledge Mining Approach”. This project will leverage sentiment analysis, NLP, and predictive modeling to forecast user sentiment in mobile apps, helping developers improve user experience and app ratings.\nMore to come after the completion of the final project"
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#course-overview",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#assignments",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#assignments",
    "title": "",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!"
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#course-project",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nThis project focuses on designing a relational database and interactive dashboard for the Texas Public Safety Association (TPSA) to evaluate the effectiveness of scoring rubrics in competitive events. By integrating student scores, rubric details, event types, and conference data, the system will enable data-driven insights into rubric fairness and effectiveness over time. Using SQL, PostgreSQL, and a Shiny-based web dashboard, this project will provide TPSA staff with an intuitive tool to refine scoring criteria, ensuring fairer and more accurate assessments across events.\nMore to come after the completion of the final project"
  },
  {
    "objectID": "pages/EPPS_6354/Assignment01/epps.6354.assignment1.html",
    "href": "pages/EPPS_6354/Assignment01/epps.6354.assignment1.html",
    "title": "",
    "section": "",
    "text": "# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\nWhat problems do you encounter when working with the data set?\nThere is a few missing values that result in a NA from the data set.\n\n\nHow to deal with missing values?\nFollowing the assignment and example online, to resolve this issue"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html",
    "title": "",
    "section": "",
    "text": "This assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#analyzing-political-discourse-using-text-data",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#analyzing-political-discourse-using-text-data",
    "title": "",
    "section": "",
    "text": "This assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#biden-xi-summit-twitter-analysis",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#biden-xi-summit-twitter-analysis",
    "title": "",
    "section": "1. Biden-Xi Summit Twitter Analysis",
    "text": "1. Biden-Xi Summit Twitter Analysis\n\nLoading Twitter Data\nThe dataset consists of tweets discussing the Biden-Xi summit (November 2021). We load the dataset using readr and extract the tweet text.\n\n\nPreprocessing the Text Data\nWe tokenize the tweet text, remove punctuation, and create a document-feature matrix (DFM), which converts text into a structured numerical format. then to analyze discussion topics, we extract hashtags from tweets and identify the most frequently used ones. Finaly, to visualize the relationships between hashtags, we create a feature co-occurrence matrix (FCM) and plot a network graph.\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\nFigure 1: Hashtag Network: The network visualization highlights key discussion topics related to the summit. Central hashtags like #biden and #china dominate the conversation, while #humanrights and #uyghurs indicate concerns over human rights issues.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#u.s.-presidential-inaugural-speeches",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#u.s.-presidential-inaugural-speeches",
    "title": "",
    "section": "2. U.S. Presidential Inaugural Speeches",
    "text": "2. U.S. Presidential Inaugural Speeches\nThis section examines U.S. presidential inaugural speeches over time, analyzing their linguistic trends and thematic focus.\n\nAnalyzing Early Inaugural Speeches & Keyword Trends Over Time\nWe create a document-feature matrix (DFM) for speeches from 1789 to 1826, removing common stopwords. We analyze post-1949 speeches and generate x-ray plots for key terms like liberty.\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\nFigure 2: Keyword Frequency of ‘Liberty’: This visualization highlights the usage pattern of ‘liberty’ in presidential speeches, showing how its prominence fluctuates over time.\n\n\n\n\n\n\nComparing Key Terms in Presidential Speeches\nWe generate x-ray plots for three key words: foreign, we, and god.\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\nFigure 3: Word Frequency Comparison: ‘Foreign’ was more common during Cold War-era speeches. ‘We’ is frequently used by presidents emphasizing unity (e.g., Obama, Biden). ‘God’ appears consistently toward the end of speeches, reflecting a tradition of invoking divine guidance.\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#wordfish-scaling-model",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#wordfish-scaling-model",
    "title": "",
    "section": "3. Wordfish Scaling Model",
    "text": "3. Wordfish Scaling Model\nThe Wordfish model is an unsupervised text scaling method that estimates document positions based on word frequencies.\n\nApplying Wordfish to the 2010 Irish Budget Speeches\nWe use Wordfish to analyze Irish parliamentary speeches and estimate ideological positions.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\nFigure 4: Wordfish model with document positions: This plot visualizes word scaling based on frequency and distribution in Irish Budget 2010 speeches. Highlighted terms like government, economy, and deficit shape document positioning, while central clustering suggests shared vocabulary. Words at the extremes indicate stronger differentiation in political discourse.\n\n\n\n\n\n\nVisualizing Word Positions\nThis plot highlights the relative importance of words in different political contexts.\n\n# Load necessary libraries\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\nFigure 5: Estimated Document Positions by Political Party: This plot shows estimated ideological positions of politicians based on word usage. Fianna Fáil (FF) leans right, Sinn Féin (SF) and Labour (LAB) lean left, while Fine Gael (FG) and the Greens vary. Black dots represent individual positions, with confidence intervals highlighting linguistic and ideological differences.\n\n\n\n\n\n\nScaling Political Documents by Party\nWe visualize document positions grouped by political party.\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\nFigure 6: Estimated Document Positions by Political Party: This plot visualizes document positions using Correspondence Analysis (CA), grouping speeches by political party. It highlights linguistic differences across parties, mapping ideological tendencies based on word usage in Irish Budget 2010 speeches.\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772.\n\n\n\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html",
    "title": "",
    "section": "",
    "text": "This assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "title": "",
    "section": "",
    "text": "This assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website.\n\n\n\n\n\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $639,593         2025-03-21 30,165 \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-u.s.-dollar-table",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-u.s.-dollar-table",
    "title": "",
    "section": "2. Scraping U.S. Dollar Table",
    "text": "2. Scraping U.S. Dollar Table\nThis section extracts a table from the Wikipedia page on U.S. currency, removing unnecessary image columns.\n\n\n\nFigure: Wikipedia Page on the U.S. Dollar – The Wikipedia entry for the United States dollar (USD), detailing its history, value, and international use. The right panel displays various denominations of U.S. banknotes.\n\n\n\nReading the U.S. Dollar Wikipedia Page\nThe script extracts the third table using XPath and removes unnecessary columns. This cleaned dataset helps track changes in U.S. banknotes over time.\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(rvest)\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/United_States_dollar'\n\n# Read the webpage\nusd_page &lt;- read_html(url)\n\n# Extract the third table on the page using XPath (skipping image columns 2 and 3)\nusd_table &lt;- usd_page %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[3]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nusd_data &lt;- usd_table[[1]]\n\n# Remove columns 2 and 3 (image columns)\nusd_data &lt;- usd_data %&gt;%\n  select(-`Front`, -`Reverse`)\n\n# Rename columns for clarity\ncolnames(usd_data) &lt;- c(\"Denomination\", \"Portrait\", \"Reverse_Motif\", \n                        \"First_Series\", \"Latest_Series\", \"Circulation\")\n\n# Clean up the data (if necessary)\nusd_data &lt;- usd_data %&gt;%\n  filter(!is.na(Denomination) & Denomination != \"\")  # Remove empty rows\n\n# View the cleaned and structured data\nhead(usd_data)\n\n# A tibble: 6 × 6\n  Denomination   Portrait   Reverse_Motif First_Series Latest_Series Circulation\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n1 One dollar     George Wa… Great Seal o… Series 1963… Series 2021[… Wide       \n2 Two dollars    Thomas Je… Declaration … Series 1976  Series 2017A  Limited[48]\n3 Five dollars   Abraham L… Lincoln Memo… Series 2006  Series 2021[… Wide       \n4 Ten dollars    Alexander… Treasury Bui… Series 2004A Series 2017A  Wide       \n5 Twenty dollars Andrew Ja… White House   Series 2004  Series 2017A  Wide       \n6 Fifty dollars  Ulysses S… United State… Series 2004  Series 2017A  Wide"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#downloading-government-documents",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#downloading-government-documents",
    "title": "",
    "section": "3. Downloading Government Documents",
    "text": "3. Downloading Government Documents\nThis section automates the bulk download of government bills related to water policy using “https://www.govinfo.gov/app/search/”.\n\n\n\nFigure: GovInfo Search Portal – The homepage of GovInfo, a U.S. government website for accessing official documents. Users can search for records using the search bar or browse by category, date, committee, or author.\n\n\n\nReading the Government Search Results\nThe script downloads 10 Congressional bills using a loop with error handling.\n\nlibrary(purrr)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\nsetwd(\"/Users/olivermyers/MyWebsite/govtdata.assignent04\")\n\n# csv downloaed from https://www.govinfo.gov/app/search/ and searching \"water\", filtering congressonal bills from 2024\n\n# Read the CSV file without skipping rows\ngovfiles &lt;- read.csv(file = \"/Users/olivermyers/MyWebsite/govinfo-search-results.csv\", skip = 2, header = FALSE)\n\ncolnames(govfiles) &lt;- govfiles[1, ]\ngovfiles &lt;- govfiles[-1, ]\nrownames(govfiles) &lt;- NULL\ncolnames(govfiles) &lt;- make.names(colnames(govfiles), unique = TRUE)\nhead(govfiles$packageId)\n\n[1] \"BILLS-118hr5770rh\"  \"BILLS-118hr5770rfs\" \"BILLS-118hr5770eh\" \n[4] \"BILLS-118hr8096ih\"  \"BILLS-118s4188is\"   \"BILLS-118hr7065ih\" \n\n# Preparing for bulk download of government documents\ngovfiles$id &lt;- govfiles$packageId\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# saving files into govdata.assignent04 folder\nsave_dir &lt;- \"/Users/olivermyers/MyWebsite/govtdata.assignent04\"\n\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    # Ensure the file path includes a proper separator\n    destfile &lt;- file.path(save_dir, paste0(\"govfiles_\", id, \".pdf\"))\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to avoid server throttling\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n\n## Download the first 10 from the csv file\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;%  # Change to limit to the first 10 files\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 24.14704 secs\n\n# List and print all files in the directory\nall_files &lt;- list.files(path = save_dir, full.names = FALSE)  \nprint(\"Files in the govtdata.assignent04 directory:\")\n\n[1] \"Files in the govtdata.assignent04 directory:\"\n\nhead(all_files)\n\n[1] \"govfiles_BILLS-118hr10150ih.pdf\" \"govfiles_BILLS-118hr3675rh.pdf\" \n[3] \"govfiles_BILLS-118hr5770eh.pdf\"  \"govfiles_BILLS-118hr5770rfs.pdf\"\n[5] \"govfiles_BILLS-118hr5770rh.pdf\"  \"govfiles_BILLS-118hr7021ih.pdf\""
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#discussion",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#discussion",
    "title": "",
    "section": "Discussion:",
    "text": "Discussion:\n\nSimple report on difficulties encountered in the scraping process:\nScraping data using the first method, rvest, was initially a bit challenging for me. The need to inspect elements on the webpage and copy the XPath IDs to make the code work was a new concept. Additionally, some parts of the code were not as straightforward compared to the second method. That said, I found rvest to be significantly more useful in the long run because it allows for automated web scraping of large amounts of data from various webpage elements. Once I became familiar with the process, I appreciated the potential for efficiently formatting and organizing scraped data, even if it was tricky to set up at first.\nThe second method, on the other hand, was easier to use but felt less practical. This approach requires manually finding and downloading the necessary list yourself, which limits its automation capabilities. Initially, I encountered issues with downloading the files into the correct folder, but after consulting ChatGPT, I resolved the problem and successfully downloaded the files to the appropriate directory.\nIn conclusion, both methods have their advantages and can produce highly usable data. However, in my personal opinion, the rvest method stands out for its versatility and ability to scrape and format large-scale data efficiently. Although it requires more time and effort to understand and implement correctly, its potential for automating repetitive scraping tasks makes it the more valuable option overall. This could then be improved with more automation and cleaning steps build into the flow when using rvest."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html",
    "title": "",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#old-model",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#old-model",
    "title": "",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "title": "",
    "section": "New Model with improved predictive ability:",
    "text": "New Model with improved predictive ability:\n\n# Install and load required packages\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"doParallel\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\nif (length(new_packages)) install.packages(new_packages)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(ranger)\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Data ingestion & preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\nset.seed(123)\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Define Preprocessing Recipe with enhancements\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  # Normalize the text (using NFC form)\n  step_text_normalization(text, normalization_form = \"nfkc\") %&gt;%\n  # Tokenize text into words\n  step_tokenize(text, token = \"words\") %&gt;%\n  # Remove stopwords (default language is English)\n  step_stopwords(text) %&gt;%\n  # Create n-grams (unigrams and bigrams)\n  step_ngram(text, num_tokens = 2, min_num_tokens = 1) %&gt;%\n  # Filter tokens to limit the number of features\n  step_tokenfilter(text, max_tokens = 1000, min_times = 2) %&gt;%\n  # Create TF-IDF features\n  step_tfidf(text) %&gt;%\n  # Normalize predictors (if needed)\n  step_normalize(all_predictors())\n\n# Model Specification: Random Forest tuned on mtry and min_n (trees fixed at 1000)\nrf_spec &lt;- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n# Create workflow\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Set up 5-fold cross-validation with stratification\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# Define a grid for tuning mtry and min_n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Register parallel backend to speed up tuning\ndoParallel::registerDoParallel()\n\n# Tune model with cross-validation and evaluate using accuracy and kappa\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\nWarning: ! tune detected a parallel backend registered with foreach but no backend\n  registered with future.\nℹ Support for parallel processing with foreach was soft-deprecated in tune\n  1.2.1.\nℹ See ?parallelism (`?tune::parallelism()`) to learn more.\n\n# Review best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     2 Preprocessor1_Model01\n\n# Finalize workflow with the best hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% fit(data = train_data)\n\nWarning: max_tokens was set to 1000, but only 236 was available and selected.\n\n# Evaluate final model on the test set\nfinal_preds &lt;- final_fit %&gt;% \n  predict(new_data = test_data) %&gt;% \n  bind_cols(test_data)\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.883\n2 kap      multiclass     0.870\n\n# Display the confusion matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             0           0       0      0\n  Entertainment       0         0             6           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       6      0\n  Health              4         0             0           0       0      6\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          6      0\n  Travel               0      0          0      6\n\n# Predict on new samples and display the results\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples)\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Sports     \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Finance    \n\n\n\nHow I improved the predication ability:\nThe new model is better because it achieves an accuracy of about 88.3% and a kappa of around 0.87, which clearly shows it is making more reliable predictions and generalizes well to new data. We changed the preprocessing by normalizing the text to maintain consistency, incorporating n-grams to capture context beyond single words, and tuning the token filtering settings to retain a more representative vocabulary while cutting out noise. In addition, we refined the hyperparameter tuning process for the random forest by focusing on key parameters such as mtry and min_n. Together, these adjustments have resulted in a model that fits the data much more effectively and performs significantly better than the previous version.\n\nThe model before preformed with:\naccuracy multiclass 0.733\nkap multiclass 0.704\n\n\nWhere as now it preforms with:\naccuracy multiclass 0.883\nkap multiclass 0.870\nDisclaimer: I Used chat GPT 03-mini-high to help with optimization and to make better predictions*"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf",
    "title": "",
    "section": "Project Proposal (PDF)",
    "text": "Project Proposal (PDF)"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-foreign-reserve-data",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-foreign-reserve-data",
    "title": "",
    "section": "1. Scraping Foreign Reserve Data",
    "text": "1. Scraping Foreign Reserve Data\n\nReading the Wikipedia Page using the rvest package\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $639,593         2025-03-21 30,165 \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#final-project-presentation-slides",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#final-project-presentation-slides",
    "title": "",
    "section": "Final Project Presentation (Slides)",
    "text": "Final Project Presentation (Slides)\n\n\nHere is the code for the data collection, cleaning and analysis:\n\n\n Data Collection Code (R) \n\n\n Analysis Code (Stata)"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#final-project-paper-pdf",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#final-project-paper-pdf",
    "title": "",
    "section": "Final Project Paper (PDF)",
    "text": "Final Project Paper (PDF)\n\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-1",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-1",
    "title": "",
    "section": "Project Proposal (PDf)",
    "text": "Project Proposal (PDf)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#project-progress-report-slides",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#project-progress-report-slides",
    "title": "",
    "section": "Project Progress Report (Slides)",
    "text": "Project Progress Report (Slides)"
  },
  {
    "objectID": "pages/About/aboutme.html",
    "href": "pages/About/aboutme.html",
    "title": "",
    "section": "",
    "text": "Hi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nUX Research and Design Portfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html",
    "href": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html",
    "title": "",
    "section": "",
    "text": "Go to live Shiny app website »"
  },
  {
    "objectID": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html#live-preview-of-my-app",
    "href": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html#live-preview-of-my-app",
    "title": "",
    "section": "Live Preview of My App",
    "text": "Live Preview of My App\n\n\n\n\n\n Launch Live Shiny App » \n\nAbout this dashboard\nA dynamic instructor dashboard that connects to your local PostgreSQL server and lets you:\n- Filter by department or instructor\n- Sort salaries highest → lowest\n- Visualize trends over time with interactive bar charts\n\n\nPreview Outline\n\nHeader – Course selector + connection status\n\nData Table – Searchable list of instructors & salaries\n\nSalary Chart – Clickable bars drill down to details\n\nAlternate View – Swap in another variable (e.g. research funding)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-presentation-slides",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-presentation-slides",
    "title": "",
    "section": "Final Presentation (Slides)",
    "text": "Final Presentation (Slides)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-app",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-app",
    "title": "",
    "section": "Final Project (APP)",
    "text": "Final Project (APP)\nGo to live Shiny app website »\n\nAbout This App: TPSA Rubric Evaluation Dashboard\nAn interactive R Shiny dashboard connected to a PostgreSQL database, designed to help the Texas Public Safety Association (TPSA) analyze and refine competitive event scoring rubrics. This tool allows TPSA staff to:\n\nSelect & View specific TPSA conferences and competitive events.\nAnalyze Performance by viewing aggregated student scores for each event within a conference.\nDrill Down into detailed rubric criteria for individual competitive events.\nVisualize Effectiveness with interactive bar charts displaying average scores per criterion, color-coded to indicate performance status.\nExplore Data in sortable and searchable tables showing rubric criteria and scores.\nIdentify Trends to objectively assess competitive event rigor and track the impact of rubric modifications over time.\nAccess Cloud Data from a PostgreSQL database hosted on Railway, populated with TPSA’s student scores, rubric details, and event information."
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-2",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-2",
    "title": "",
    "section": "Project Proposal (PDf)",
    "text": "Project Proposal (PDf)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-progress-report-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-progress-report-pdf",
    "title": "",
    "section": "Project Progress Report (Pdf)",
    "text": "Project Progress Report (Pdf)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-presentation-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-presentation-pdf",
    "title": "",
    "section": "Final Project Presentation (PDF)",
    "text": "Final Project Presentation (PDF)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-app",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-app",
    "title": "",
    "section": "Final Project (APP)",
    "text": "Final Project (APP)\nGo to live Shiny app website »\n\nAbout This App: App Review UX Heuristic Analyzer\nAn interactive R Shiny dashboard designed to mine user experience (UX) insights from Apple App Store reviews. This tool allows you to:\n\nSearch & Select iOS apps directly from the App Store via API integration.\nScrape & Analyze recent user reviews for any selected app.\nCategorize Feedback by automatically tagging review sentences with Nielsen’s 10 Usability Heuristics based on keyword matching.\nGauge Sentiment associated with specific heuristic violations to understand issue severity.\nVisualize Trends with interactive charts showing heuristic mention frequency and priority scores.\nFilter Insights by app version and review date to track changes over time.\nDrill Down into specific user comments related to each heuristic, with relevant keywords highlighted."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "library(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\n\n\nx &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9969523\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#create-object-using-the-assignment-operator--",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#using-function",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#using---operators",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#matrix-operations",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9969523\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#simple-descriptive-statistics-base",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpG49zLC/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#indexing-data-using",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#loading-data-from-github-remote",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#load-data-from-islr-website",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#linear-regression",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpG49zLC/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#multiple-linear-regression",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#qualitative-predictors",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "library(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\n\n\nx &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9957205\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#create-object-using-the-assignment-operator--",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#using-function",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#using---operators",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#matrix-operations",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9957205\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#simple-descriptive-statistics-base",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpWyoN2S/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#indexing-data-using",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#loading-data-from-github-remote",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#load-data-from-islr-website",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#linear-regression",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpWyoN2S/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#multiple-linear-regression",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#qualitative-predictors",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html",
    "href": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html",
    "title": "",
    "section": "",
    "text": "(https://datageneration.io/dataprogramming/r-programming.html#illustration)\n\n# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\n\n# Check the variable\nattach(TEDS_2016)\nhead(PartyID)\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\ntail(PartyID)\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\n\n\n# Run a frequency table of the Party ID variable using the descr package\n## install.packages(\"descr\")\nlibrary(descr)\nfreq(TEDS_2016$PartyID)\n\n\n\n\n\n\n\n\nTEDS_2016$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n##install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nTEDS_2016 %&gt;% \n  count(PartyID) %&gt;% \n  mutate(perc = n / nrow(TEDS_2016)) -&gt; T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#running-exploratory-analysis",
    "href": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#running-exploratory-analysis",
    "title": "",
    "section": "",
    "text": "(https://datageneration.io/dataprogramming/r-programming.html#illustration)\n\n# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\n\n# Check the variable\nattach(TEDS_2016)\nhead(PartyID)\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\ntail(PartyID)\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\n\n\n# Run a frequency table of the Party ID variable using the descr package\n## install.packages(\"descr\")\nlibrary(descr)\nfreq(TEDS_2016$PartyID)\n\n\n\n\n\n\n\n\nTEDS_2016$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n##install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nTEDS_2016 %&gt;% \n  count(PartyID) %&gt;% \n  mutate(perc = n / nrow(TEDS_2016)) -&gt; T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#assignment-02-questions",
    "href": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#assignment-02-questions",
    "title": "",
    "section": "Assignment 02 Questions:",
    "text": "Assignment 02 Questions:\n\nWhat problems do you encounter when working with the dataset?\n\n# Display the first 5 rows and first 9 columns\nhead(TEDS_2016[, 1:9], 5)\n\n# A tibble: 5 × 9\n  District      Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n  &lt;dbl+lbl&gt;     &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n1 201 [Yi Lan … 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n2 201 [Yi Lan … 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n3 201 [Yi Lan … 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n4 201 [Yi Lan … 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n5 201 [Yi Lan … 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n\n\nOne of the main challenges with the TEDS_2016 dataset is that many variables are coded numerically without accompanying labels, making interpretation difficult. For instance, the “sex” variable contains values like 1 and 2, but there is no immediate indication of which value corresponds to which gender. Similarly, variables such as “education” and “age” appear as numerical codes that are unclear without a codebook or variable dictionary. This lack of descriptive metadata makes it hard to conduct meaningful analysis without first decoding the variables. In addition there are missing values in various cells in our database giving us “N/A”.\n\n\nHow to deal with missing values?\nOne approach to dealing with missing values is to remove rows that contain them, which helps maintain the integrity of the analysis by ensuring that only complete cases are used. \n\n\n(Next step) Explore the relationship between Tondu and other variables including female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\nTo explore these relationships, I used grouped bar charts and boxplots to examine how political and demographic factors relate to Tondu preferences (views on unification vs. independence). For categorical variables like gender, DPP support, and Taiwanese identity, I used grouped bar charts to show the distribution of Tondu preferences. For continuous variables like age and income, I used boxplots to illustrate how median values and spreads differ across Tondu groups. This approach allows for both a categorical comparison and a clear view of variation within numeric data.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# remove rows with n/a data\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  filter(!is.na(Tondu))\n\n# Prepare Education levels only (leave Tondu as-is)\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  mutate(\n    edu_level = factor(edu, levels = 1:5,\n                       labels = c(\"Primary\", \"Middle\", \"High\", \"College\", \"Postgrad\"))\n  )\n\n# 1. Gender\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(female))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#FF9999\", \"#6699CC\"), labels = c(\"Male\", \"Female\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Gender\") +\n  ggtitle(\"Tondu preference by Gender\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 2. DPP Support\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(DPP))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#FFD700\", \"#228B22\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"DPP Support\") +\n  ggtitle(\"Tondu preference by DPP Support\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 3. Taiwanese Identity\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Taiwanese))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#D95F02\", \"#1B9E77\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Taiwanese Identity\") +\n  ggtitle(\"Tondu preference by Taiwanese Identity\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 4. Economic Perception\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Econ_worse))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#A6CEE3\", \"#FB9A99\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Economy Worse?\") +\n  ggtitle(\"Tondu preference by Economic Perception\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 5. Education\nggplot(TEDS_2016, aes(x = Tondu, fill = edu_level)) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Education Level\") +\n  ggtitle(\"Tondu preference by Education\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 1. Tondu vs Age (Boxplot)\nggplot(TEDS_2016, aes(x = Tondu, y = age, fill = Tondu)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(x = \"Tondu Preference\", y = \"Age\") +\n  ggtitle(\"Age Distribution by Tondu Preference\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1),\n        legend.position = \"none\")\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n# 2. Tondu vs Income (Boxplot)\nggplot(TEDS_2016, aes(x = Tondu, y = income, fill = Tondu)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(x = \"Tondu Preference\", y = \"Income\") +\n  ggtitle(\"Income Distribution by Tondu Preference\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1),\n        legend.position = \"none\")\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\nThe following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n# 3. Tondu vs Taiwanese Identity (Stacked bar)\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Taiwanese))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#E41A1C\", \"#377EB8\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Taiwanese Identity\") +\n  ggtitle(\"Tondu Preference by Taiwanese Identity\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 4. Tondu vs Economic Perception\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Econ_worse))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#A6CEE3\", \"#FB9A99\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Economy Worse?\") +\n  ggtitle(\"Tondu Preference by Economic Perception\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n(Next step) How about the votetsai variable (vote for DPP candidate Tsai Ing-wen)?\nTo explore the relationship between support for Tsai Ing-wen and national identity preferences, I created a grouped bar chart comparing Tondu responses by voting behavior. This visualization shows how views on unification versus independence differ between those who voted for Tsai and those who did not in the 2016 election.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filter and prep\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  filter(!is.na(Tondu), !is.na(votetsai_all)) %&gt;%\n  mutate(\n    votetsai_label = factor(votetsai_all,\n                            levels = c(0, 1),\n                            labels = c(\"Did Not Vote Tsai\", \"Voted Tsai\"))\n  )\n\n# Plot: Grouped bar chart\nggplot(TEDS_2016, aes(x = Tondu, fill = votetsai_label)) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#E41A1C\", \"#377EB8\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Vote for Tsai?\") +\n  ggtitle(\"Tondu Preference by Vote for Tsai Ing-wen\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n(Next step) Generate frequency table and barchart of the Tondu variable. Assign labels to the variable using the following:\n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”, “Status quo, unif. in future”, “Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now”, “No respons\"))\n\n\nlibrary(descr)\n\nfreq(TEDS_2016$Tondu)\n\n\n\n\n\n\n\n\nTEDS_2016$Tondu \n      Frequency Percent\n1            23   1.595\n2           156  10.818\n3           459  31.831\n4           283  19.626\n5           327  22.677\n6            97   6.727\n9            97   6.727\nTotal      1442 100.000\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, labels = c(\n  \"Unification now\", \n  \"Status quo, unif. in future\", \n  \"Status quo, decide later\", \n  \"Status quo forever\", \n  \"Status quo, indep. in future\", \n  \"Independence now\", \n  \"No response\"\n))\n\nlibrary(ggplot2)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nggplot(TEDS_2016, aes(x = Tondu)) +\n  geom_bar(aes(y = (..count..) / sum(..count..), fill = Tondu)) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Views on Taiwan's Political Status (Tondu)\",\n    y = \"Percentage of Respondents\"\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\n    \"steelblue\", \"forestgreen\", \"khaki\", \"gray60\", \n    \"gold\", \"red\", \"darkgray\"\n  ))"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html",
    "href": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html",
    "title": "",
    "section": "",
    "text": "library(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\n\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n\nThe Biden–Xi summit data reveals distinct clusters of conversation around human rights and geopolitical tension. Hashtags like #uyghurs, #fentanyl, and #taiwan are tightly linked to #china, suggesting that topics like human rights abuses, drug trade, and territorial sovereignty were central to online discourse surrounding the summit."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#biden-xi-summit-data",
    "href": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#biden-xi-summit-data",
    "title": "",
    "section": "",
    "text": "library(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\n\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n\nThe Biden–Xi summit data reveals distinct clusters of conversation around human rights and geopolitical tension. Hashtags like #uyghurs, #fentanyl, and #taiwan are tightly linked to #china, suggesting that topics like human rights abuses, drug trade, and territorial sovereignty were central to online discourse surrounding the summit."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#us-presidential-inaugural-speeches",
    "href": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#us-presidential-inaugural-speeches",
    "title": "",
    "section": "US presidential inaugural speeches",
    "text": "US presidential inaugural speeches\n\n# Locate keywords-in-context\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"trade\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"american\"),\n  kwic(tokens_inaugural, pattern = \"people\"),\n  kwic(tokens_inaugural, pattern = \"trade\")\n)\n\n\n\n\n\n\n\n\n\nAny similarities and differences over time and among presidents?\nLooks like most presidents consistently used the word “people,” showing it’s a go-to way to connect with the public. But words like “trade” barely show up and only pop in during a few speeches, mostly from Reagan, Clinton, and Trumpm so it’s clearly not a top priority for everyone.\n\n\nWhat is Wordfish?\nWordfish is a text scaling algorithm used in political science and computational social science to estimate latent positions of actors based on the word frequencies in their texts. It assumes that word usage varies systematically with political position and applies a statistical model to uncover a one-dimensional scale from the textual data without needing predefined categories.\n\n# Create a dfm from the inaugural subset\ndfm_inaug &lt;- tokens(data_corpus_inaugural_subset, remove_punct = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"en\")) %&gt;%\n  dfm()\n\n# Optional: trim sparse words to reduce noise\ndfm_inaug_trim &lt;- dfm_trim(dfm_inaug, min_termfreq = 5)\n\n# Run Wordfish scaling\nwordfish_model &lt;- textmodel_wordfish(dfm_inaug_trim, dir = c(1, nrow(dfm_inaug_trim)))\n\n# View estimated positions (theta scores)\nhead(wordfish_model$theta)\n\n[1] -2.439182610 -2.338764710 -0.902685412 -0.156921580 -0.002233262\n[6]  0.263172586\n\n# Plot the positions with ggplot2\nwf_df &lt;- data.frame(\n  President = docnames(dfm_inaug_trim),\n  Position = wordfish_model$theta\n)\n\nggplot(wf_df, aes(x = reorder(President, Position), y = Position)) +\n  geom_col(fill = \"#4F81BD\") +\n  coord_flip() +\n  xlab(\"President\") +\n  ylab(\"Estimated Position (Wordfish)\") +\n  ggtitle(\"Presidential Speech Positions via Wordfish\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHow to compare positions?\nTo compare positions, you can apply scaling methods like Wordfish or Wordshoal in quanteda, which place documents or speakers on an inferred ideological or policy spectrum. These methods work by analyzing differences in word usage across documents and estimating relative positions based on how discriminative certain words are, allowing you to visualize shifts in tone, ideology, or topic emphasis between individuals, parties, or time periods.\n\n\nCreate a corpus using government documents selected from the govinfo.gov website (usesample program govtdata01.R)\n\n# --- Part 0: Load Libraries & Setup ---\n# Make sure these are installed: install.packages(c(\"quanteda\", \"quanteda.textmodels\", \"quanteda.textstats\", \"quanteda.textplots\", \"readtext\", \"purrr\", \"magrittr\", \"jsonlite\", \"dplyr\", \"data.table\"))\n\n# For text analysis\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(readtext) # For reading PDFs\n\n\nAttaching package: 'readtext'\n\n\nThe following object is masked from 'package:quanteda':\n\n    texts\n\n# For data handling and downloading (from your script)\nlibrary(purrr)\nlibrary(magrittr) \n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n# --- Part 1: Configuration - USER NEEDS TO SET THESE ---\n\n# 1.1 Set your working directory (where your R script and downloaded_pdfs folder are)\n#     Replace \"your/path/to/EPPS6323_Assign4\" with your actual path\n#     Example for Windows: setwd(\"C:/Users/YourName/Documents/EPPS6323_Assign4\")\n#     Example for Mac/Linux: setwd(\"/Users/YourName/Documents/EPPS6323_Assign4\")\n# setwd(\"your/path/to/EPPS6323_Assign4\") # IMPORTANT: Uncomment and set this!\nif (getwd() == \"/\") { # A simple check if you're in the root, likely not intended\n  stop(\"Please set your working directory using setwd() before proceeding.\")\n}\nprint(paste(\"Current working directory:\", getwd()))\n\n[1] \"Current working directory: /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04\"\n\n# 1.2 Path to the JSON metadata file you downloaded from GovInfo\n#     OR the direct URL to the JSON if GovInfo provided one.\n#     If you downloaded the file, put it in your working directory.\n#     Example: json_metadata_file_path &lt;- \"govinfo-search-results-foreign-affairs.json\"\n#     Example: json_metadata_url &lt;- \"https_some_direct_url_from_govinfo.json\" # Less common for search results\njson_metadata_file_path &lt;- \"YOUR_JSON_METADATA_FILE.json\" # &lt;--- !!! REPLACE THIS !!! \n# For demonstration, I'll use the one from your script, but you should get your own for the specific search\n# For this example to run out-of-the-box for others, let's use the GitHub URL from your sample.\n# IN YOUR ACTUAL ASSIGNMENT, USE THE JSON FROM YOUR SPECIFIC GovInfo SEARCH.\njson_metadata_url &lt;- \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\nprint(paste(\"Using JSON metadata from:\", json_metadata_url))\n\n[1] \"Using JSON metadata from: https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n\n# 1.3 Directory to save the downloaded PDFs\n#     This should be a subfolder within your working directory.\nsave_dir &lt;- file.path(getwd(), \"downloaded_pdfs\")\nif (!dir.exists(save_dir)) {\n  dir.create(save_dir, recursive = TRUE)\n  print(paste(\"Created directory:\", save_dir))\n} else {\n  print(paste(\"Save directory already exists:\", save_dir))\n}\n\n[1] \"Save directory already exists: /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs\"\n\n# 1.4 Number of documents to download (keep it small for testing, e.g., 3-5)\nnum_docs_to_download &lt;- 3\n\n# --- Part 2: Load Metadata (Adapted from govtdata01.R) ---\nmessage(\"Loading JSON metadata...\")\n\nLoading JSON metadata...\n\n# Try reading from URL first, then from local file if path was given for local\nif (exists(\"json_metadata_url\") && startsWith(json_metadata_url, \"http\")) {\n  gf_list1 &lt;- jsonlite::read_json(json_metadata_url)\n} else if (exists(\"json_metadata_file_path\") && file.exists(json_metadata_file_path)) {\n  gf_list1 &lt;- jsonlite::read_json(json_metadata_file_path)\n} else {\n  stop(\"Please provide a valid json_metadata_url or json_metadata_file_path.\")\n}\n\n# Extract the list of documents\ngovfiles_df &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\nif (nrow(govfiles_df) == 0) {\n  stop(\"No documents found in the JSON metadata. Check your JSON file/URL or search query on GovInfo.\")\n}\nprint(paste(\"Loaded metadata for\", nrow(govfiles_df), \"documents.\"))\n\n[1] \"Loaded metadata for 1000 documents.\"\n\n# head(govfiles_df) #  (e.g., title, packageId, pdfLink, lastModified, dateIssued)\n\n# --- Part 3: Prepare for Bulk Download (Adapted from govtdata01.R) ---\n# Ensure the necessary columns exist\nif (!\"pdfLink\" %in% names(govfiles_df) || !\"packageId\" %in% names(govfiles_df)) {\n  stop(\"The JSON metadata does not contain 'pdfLink' or 'packageId'. Check the structure. Available columns: \", paste(names(govfiles_df), collapse=\", \"))\n}\n\npdf_govfiles_url &lt;- govfiles_df$pdfLink\n# Use packageId for a unique, meaningful ID. If not present, use row index.\nif (\"packageId\" %in% names(govfiles_df)) {\n  pdf_govfiles_id &lt;- govfiles_df$packageId \n} else {\n  # Fallback if packageId is missing, though it's usually there.\n  # Using row number as ID might lead to less descriptive filenames.\n  pdf_govfiles_id &lt;- 1:nrow(govfiles_df)\n  warning(\"packageId not found in metadata, using row index as ID.\")\n}\n\n\n# Trim to the number of documents we want to download\nif (nrow(govfiles_df) &lt; num_docs_to_download) {\n  warning(paste(\"Requested to download\", num_docs_to_download, \n                \"but only\", nrow(govfiles_df), \"are available. Downloading all available.\"))\n  num_docs_to_download &lt;- nrow(govfiles_df)\n}\n\npdf_govfiles_url_subset &lt;- head(pdf_govfiles_url, num_docs_to_download)\npdf_govfiles_id_subset &lt;- head(pdf_govfiles_id, num_docs_to_download)\ngovfiles_df_subset &lt;- head(govfiles_df, num_docs_to_download)\n\n\n# --- Part 4: Download PDFs (Adapted from govtdata01.R) ---\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id, save_directory) {\n  tryCatch({\n    # Sanitize id to make it a valid filename component\n    # Replace non-alphanumeric characters (except hyphen and underscore) with underscore\n    sanitized_id &lt;- gsub(\"[^a-zA-Z0-9_-]\", \"_\", id)\n    destfile &lt;- file.path(save_directory, paste0(\"govfile_\", sanitized_id, \".pdf\"))\n    \n    # Check if file already exists to avoid re-downloading (optional)\n    if (file.exists(destfile)) {\n      return(paste(\"File already exists (skipped):\", destfile))\n    }\n    \n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep\n    return(paste(\"Successfully downloaded:\", destfile))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download from:\", url, \"Error:\", e$message))\n  })\n}\n\nmessage(paste(\"Starting downloads of\", num_docs_to_download, \"PDFs to\", save_dir, \"...\"))\n\nStarting downloads of 3 PDFs to /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs ...\n\nstart.time &lt;- Sys.time()\n\n# Use mapply for iterating over two lists (url and id) simultaneously\n# purrr::map2_chr is also a good alternative\ndownload_results &lt;- mapply(download_govfiles_pdf, \n                           pdf_govfiles_url_subset, \n                           pdf_govfiles_id_subset,\n                           MoreArgs = list(save_directory = save_dir),\n                           SIMPLIFY = TRUE,\n                           USE.NAMES = FALSE)\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\nmessage(\"Finished downloads.\")\n\nFinished downloads.\n\nprint(time.taken)\n\nTime difference of 0.00540185 secs\n\nprint(download_results)\n\n[1] \"File already exists (skipped): /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs/govfile_BILLS-118sres890is.pdf\" \n[2] \"File already exists (skipped): /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs/govfile_BILLS-118sjres114is.pdf\"\n[3] \"File already exists (skipped): /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs/govfile_BILLS-118sres805ats.pdf\"\n\n# --- Part 5: Read Downloaded PDFs into R using readtext ---\nmessage(\"Reading downloaded PDFs into R...\")\n\nReading downloaded PDFs into R...\n\n# List all PDF files in the save_dir\npdf_files_to_read &lt;- list.files(path = save_dir, pattern = \"\\\\.pdf$\", full.names = TRUE)\n\nif (length(pdf_files_to_read) == 0) {\n  stop(\"No PDF files found in the download directory. Check download step.\")\n}\n\n# Read the text from PDF files\n# This can take some time, and PDF text extraction quality varies.\n# readtext uses pdftools::pdf_text internally for PDFs.\n# We will use the packageId as the doc_id for better linking with metadata\n# We need to extract the ID from the filename to match with our metadata\ndoc_ids_from_filenames &lt;- gsub(\"govfile_ Tuttavia_ Tuttavia_ Tuttavia_ Tuttavia_|.pdf\", \"\", basename(pdf_files_to_read))\n# doc_ids_from_filenames should now match pdf_govfiles_id_subset (after sanitization in download function)\n\n# To ensure correct doc_id assignment, let's build it more robustly\n# We know which IDs we attempted to download: pdf_govfiles_id_subset\n# We can construct the expected filenames\nexpected_filenames_base &lt;- paste0(\"govfile_\", gsub(\"[^a-zA-Z0-9_-]\", \"_\", pdf_govfiles_id_subset), \".pdf\")\nexpected_fullpaths &lt;- file.path(save_dir, expected_filenames_base)\n\n# Filter for only those files that were actually downloaded and exist\nactual_files_to_read &lt;- expected_fullpaths[file.exists(expected_fullpaths)]\nactual_ids_for_corpus &lt;- pdf_govfiles_id_subset[file.exists(expected_fullpaths)]\ngovfiles_df_for_corpus &lt;- govfiles_df_subset[file.exists(expected_fullpaths), ]\n\n\nif (length(actual_files_to_read) &gt; 0) {\n  gov_texts &lt;- readtext(actual_files_to_read, \n                        docvarsfrom = \"filenames\", # we'll add proper docvars later\n                        docnames = actual_ids_for_corpus) # Use our original IDs\n  print(paste(\"Successfully read\", nrow(gov_texts), \"PDF files.\"))\n  # print(head(gov_texts)) # Display first few characters of each document\n} else {\n  stop(\"No PDF files were successfully downloaded or found to read.\")\n}\n\n[1] \"Successfully read 3 PDF files.\"\n\n# --- Part 6: Create a quanteda Corpus ---\nmessage(\"Creating quanteda corpus...\")\n\nCreating quanteda corpus...\n\n# The 'text' column from readtext output contains the text\ngov_corpus &lt;- corpus(gov_texts) # text_field = \"text\" is default\n\n# Add document variables (docvars) from our metadata\n# Make sure the order of govfiles_df_for_corpus matches the order of documents in gov_corpus\n# Since we used actual_ids_for_corpus for docnames, we can match on that.\n# docnames(gov_corpus) should be the same as actual_ids_for_corpus\n\n# Select relevant columns from govfiles_df_for_corpus to be docvars\n# Common useful docvars: title, dateIssued, packageId (which is already docname)\n# Ensure packageId is a column in govfiles_df_for_corpus if you want to use it for merging\nif (!\"packageId\" %in% names(govfiles_df_for_corpus)) {\n  govfiles_df_for_corpus$packageId &lt;- actual_ids_for_corpus # Add it if missing\n}\n\n# Ensure govfiles_df_for_corpus docvars are aligned with corpus docnames\ndocvars_to_add &lt;- govfiles_df_for_corpus[match(docnames(gov_corpus), govfiles_df_for_corpus$packageId), ]\n\n# Add all columns from docvars_to_add as document variables\n# Exclude 'text' if it accidentally got in there, and packageId if we want to keep docnames unique\n# Also exclude pdfLink, etc. that might not be directly useful as docvars.\ncols_for_docvars &lt;- c(\"title\", \"granuleClass\", \"lastModified\", \"dateIssued\", \"branch\", \"suDocClassNumber\", \"governmentAuthor1\", \"pages\")\n# Keep only columns that actually exist in docvars_to_add\ncols_for_docvars &lt;- intersect(names(docvars_to_add), cols_for_docvars)\nif (length(cols_for_docvars) &gt; 0) {\n    docvars(gov_corpus) &lt;- cbind(docvars(gov_corpus), docvars_to_add[, cols_for_docvars, drop = FALSE])\n}\n\n\n# Display corpus summary\nprint(summary(gov_corpus, n = num_docs_to_download)) # Show summary for all downloaded docs\n\nCorpus consisting of 3 documents, showing 3 documents:\n\n                            Text Types Tokens Sentences docvar1\n govfile_BILLS-118sjres114is.pdf   291    625        12 govfile\n govfile_BILLS-118sres805ats.pdf   559   1631        16 govfile\n  govfile_BILLS-118sres890is.pdf   333    884        14 govfile\n             docvar2 title\n BILLS-118sjres114is  &lt;NA&gt;\n BILLS-118sres805ats  &lt;NA&gt;\n  BILLS-118sres890is  &lt;NA&gt;\n\nprint(paste(\"Corpus created with\", ndoc(gov_corpus), \"documents.\"))\n\n[1] \"Corpus created with 3 documents.\"\n\n# You can now proceed with further analysis using this 'gov_corpus' object\n# For example:\n# gov_dfm &lt;- dfm(tokens(gov_corpus, remove_punct = TRUE, remove_numbers = TRUE) %&gt;%\n#                tokens_remove(stopwords(\"en\")))\n# print(gov_dfm[, 1:10])\n\nmessage(\"Assignment 6 task (corpus creation) complete.\")\n\nAssignment 6 task (corpus creation) complete.\n\nmessage(\"The corpus object is named 'gov_corpus'.\")\n\nThe corpus object is named 'gov_corpus'."
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-code",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-code",
    "title": "",
    "section": "Final Project Code",
    "text": "Final Project Code"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-report-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-report-pdf",
    "title": "",
    "section": "Final Project Report (PDF)",
    "text": "Final Project Report (PDF)"
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#final-project",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#final-project",
    "title": "",
    "section": "Final Project",
    "text": "Final Project"
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#course-project-app-review-ux-heuristic-analyzer",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#course-project-app-review-ux-heuristic-analyzer",
    "title": "",
    "section": "Course Project: App Review UX Heuristic Analyzer",
    "text": "Course Project: App Review UX Heuristic Analyzer\nThis final project for EPPS 6323 Knowledge Mining involved the development of the “App Review UX Heuristic Analyzer,” an interactive R Shiny application designed to extract actionable insights from Apple App Store user reviews.\nMoving beyond initial explorations into predictive sentiment forecasting, this project focused on applying heuristic evaluation principles, Natural Language Processing (NLP) for keyword matching, and sentiment analysis to provide a robust framework for understanding user experience pain points. The application allows users to:\n\nSearch for iOS apps and retrieve their metadata.\nScrape recent user reviews for selected applications.\nAutomatically categorize review sentences based on Nielsen’s 10 Usability Heuristics by matching them against predefined keyword sets.\nAnalyze the sentiment associated with heuristic-specific comments to gauge issue severity.\nExplore these insights through interactive visualizations and data tables, with options to filter by app version.\n\nThe goal was to create a practical tool that empowers UX researchers, product managers, and developers to quickly identify and prioritize usability issues directly from user feedback, facilitating data-driven improvements to mobile applications. The project demonstrates a knowledge mining approach by transforming unstructured user comments into structured, categorized, and actionable intelligence."
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#final-project",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#final-project",
    "title": "",
    "section": "Final Project",
    "text": "Final Project"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-paper-pdf",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-paper-pdf",
    "title": "",
    "section": "Final Project Paper (PDF)",
    "text": "Final Project Paper (PDF)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-app-1",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-app-1",
    "title": "",
    "section": "Final Project (APP)",
    "text": "Final Project (APP)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#live-preview-of-my-app",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#live-preview-of-my-app",
    "title": "",
    "section": "Live Preview of My App",
    "text": "Live Preview of My App\n\n\n\n\n\n Launch Live Shiny App » \n\nAbout this dashboard\nA dynamic instructor dashboard that connects to your local PostgreSQL server and lets you:\n- Filter by department or instructor\n- Sort salaries highest → lowest\n- Visualize trends over time with interactive bar charts"
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#course-project-tpsa-rubric-evaluation-database-and-interactive-app",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#course-project-tpsa-rubric-evaluation-database-and-interactive-app",
    "title": "",
    "section": "Course Project: TPSA Rubric Evaluation Database and Interactive App",
    "text": "Course Project: TPSA Rubric Evaluation Database and Interactive App\nThis final project for EPPS 6354 Information Management involved the design and implementation of a centralized PostgreSQL database and an interactive R Shiny web application for the Texas Public Safety Association (TPSA). The primary objective was to provide TPSA with a robust system to analyze and refine the scoring rubrics used in their diverse competitive events for high school students.\nThe project addressed TPSA’s challenge of manually assessing rubric effectiveness across numerous events and conferences. Key components and functionalities include:\n\nRelational Database Design: A PostgreSQL database schema was developed to integrate TPSA’s existing CSV datasets on student scores, rubric criteria, competitive events, conference details, and judge information. The database was hosted on the Railway cloud platform for broader accessibility.\nInteractive Shiny Dashboard: An R Shiny application serves as the analytical interface, allowing TPSA staff to:\n\nSelect specific conferences and competitive events via dropdown menus.\nView aggregated performance data and detailed breakdowns of rubric criteria.\nVisualize event and criteria scores using interactive bar charts (via highcharter), with color-coding to highlight performance status.\nExplore data in sortable and searchable tables (via reactable).\n\nDynamic SQL Queries: The Shiny app dynamically constructs and executes SQL queries against the PostgreSQL database (managed via DBI, RPostgres, and pool packages) to retrieve relevant performance data based on user selections.\nData-Driven Rubric Refinement: The system enables TPSA to objectively assess competitive event rigor, identify underperforming or problematic rubric criteria, and track the impact of rubric modifications over time.\n\nThe resulting solution provides TPSA with an efficient and insightful platform for data-driven decision-making, aiming to enhance the fairness, consistency, and effectiveness of their rubric evaluation process. This project demonstrates the practical application of database design, data management, and interactive visualization techniques to solve real-world organizational challenges."
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-progress-report-slides",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-progress-report-slides",
    "title": "",
    "section": "Project Progress Report (Slides)",
    "text": "Project Progress Report (Slides)"
  },
  {
    "objectID": "index.html#epps-6302-data-collection-production",
    "href": "index.html#epps-6302-data-collection-production",
    "title": "",
    "section": "EPPS 6302 Data Collection & Production",
    "text": "EPPS 6302 Data Collection & Production"
  },
  {
    "objectID": "index.html#epps-6323-knowledge-mining",
    "href": "index.html#epps-6323-knowledge-mining",
    "title": "",
    "section": "EPPS 6323 Knowledge Mining",
    "text": "EPPS 6323 Knowledge Mining"
  },
  {
    "objectID": "index.html#epps-6354-information-management",
    "href": "index.html#epps-6354-information-management",
    "title": "",
    "section": "EPPS 6354 Information Management",
    "text": "EPPS 6354 Information Management"
  },
  {
    "objectID": "index.html#epps-6354-information-management-1",
    "href": "index.html#epps-6354-information-management-1",
    "title": "",
    "section": "EPPS 6354 Information Management",
    "text": "EPPS 6354 Information Management"
  },
  {
    "objectID": "index.html#epps-6302-data-collection-production-1",
    "href": "index.html#epps-6302-data-collection-production-1",
    "title": "",
    "section": "EPPS 6302 Data Collection & Production",
    "text": "EPPS 6302 Data Collection & Production"
  },
  {
    "objectID": "index.html#epps-6323-knowledge-mining-1",
    "href": "index.html#epps-6323-knowledge-mining-1",
    "title": "",
    "section": "EPPS 6323 Knowledge Mining",
    "text": "EPPS 6323 Knowledge Mining"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/6302.01.html",
    "href": "pages/EPPS_6302/Assignment01/6302.01.html",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/6302.01.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "href": "pages/EPPS_6302/Assignment01/6302.01.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/6302.01.html#google-trends-csv-data-analysis",
    "href": "pages/EPPS_6302/Assignment01/6302.01.html#google-trends-csv-data-analysis",
    "title": "",
    "section": "1. Google Trends CSV Data & Analysis",
    "text": "1. Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n\nLoading the Google Trends CSV Data\nThe dataset is read into R, and unnecessary rows are removed to clean the data.\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\n\n\nPlotting Google Search Trends Over Time\nThis plot visualizes the search interest over time for Trump, Harris, and Election, with key event dates highlighted.\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\nFigure 1: Google Trends Search Interest Over Time: Search interest for ‘Trump,’ ‘Harris,’ and ‘Election’ from July to November 2024, highlighting key political events influencing search spikes.\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/6302.01.html#google-trends-using-gtrendsr-packageusing",
    "href": "pages/EPPS_6302/Assignment01/6302.01.html#google-trends-using-gtrendsr-packageusing",
    "title": "",
    "section": "2. Google Trends Using “gtrendsR” PackageUsing",
    "text": "2. Google Trends Using “gtrendsR” PackageUsing\nn this method, the gtrendsR package in R was used to directly query Google Trends for real-time search interest data on “Trump,” “Harris,” and “Election.” Instead of manually downloading a CSV, this approach allows for automated data retrieval over a specified time range.\n\nFetching Data from Google Trends API\nThis code retrieves real-time Google search interest data directly from Google’s API.\n\n# EPPS 6302: Google Trends data\n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# The category for \"Food & Drink\" is 71.\n# We are searching for the specified terms, across all time, for web searches.\ndrink_trends &lt;- gtrends(\n  keyword = c(\"Mocktail\", \"Non-alcoholic\", \"Spirit Free\"),\n  time = \"all\",\n  gprop = \"web\",\n  geo = \"US\",\n  category = 71\n)\n\ndrink_trends_int &lt;- drink_trends$interest_over_time\n\ndrink_trends_int &lt;- drink_trends_int %&gt;%\n  mutate_at(\"hits\", ~ifelse(. == \"&lt;1\", 0.5, .)) %&gt;%\n  mutate(n_hits = as.numeric(hits)) %&gt;%\n  mutate(d_date = lubridate::as_date(date))\n\ndrink_trends_int %&gt;%\n  ggplot(aes(x = d_date,\n             y = n_hits,\n             color = keyword,\n             fill = keyword)) +\n  geom_area(alpha = 0.3, position = \"identity\") +\n  # Add these two lines to remove the padding\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_x_date(expand = c(0, 0)) +\n  labs(title = \"Non-alcoholic Drink Search Trends\",\n       subtitle = \"Comparing Google search interest (Food & Drink Categroy)\",\n       x = \"Year\",\n       y = \"Relative Search Interest (0-100)\",\n       alt = paste(\"A line plot showing relative Google search interest over all time for the keywords: 'Mocktail', 'Non-alcoholic', and 'Spirit Free' within the Food & Drink category\")) +\n  theme_light()\n\n\n\n\nFigure 2: Google Trends Data Retrieved via API: Real-time search interest trends for non-alcoholic beverage terms, retrieved using the gtrendsR package.\n\n\n\n\nKey Advantages\n\nAutomated Data Collection → Eliminates the need for manual downloads.\nReal-Time Updates → Ensures the latest data can be pulled dynamically.\nReproducibility → Allows future analysis with updated data."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/6302.01.html#discussion-differences-between-the-two-methods",
    "href": "pages/EPPS_6302/Assignment01/6302.01.html#discussion-differences-between-the-two-methods",
    "title": "",
    "section": "Discussion: Differences between the two methods:",
    "text": "Discussion: Differences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html",
    "title": "",
    "section": "",
    "text": "This assignment analyzes the YouTube conversation surrounding the ‘Wicked’ movie using the  tuber  and quanteda R packages. The goal is to:\n\nSearch for videos related to “Wicked” published between October 2023 and October 2025.\nExtract video metadata, including titles, publication dates, and channels.\nVisualize data trends, including a cumulative publication timeline correlated with trailer releases and a breakdown of top content-creating channels.\nPerform an in-depth text analysis of comments from a specific popular video (YPCqTI0PVV0) to find top keywords, bigrams, and trigrams."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#analyzing-youtube-hype-for-the-wicked-movie",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#analyzing-youtube-hype-for-the-wicked-movie",
    "title": "",
    "section": "",
    "text": "This assignment analyzes the YouTube conversation surrounding the ‘Wicked’ movie using the  tuber  and quanteda R packages. The goal is to:\n\nSearch for videos related to “Wicked” published between October 2023 and October 2025.\nExtract video metadata, including titles, publication dates, and channels.\nPerform text analysis on video titles to identify key themes.\nVisualize data trends, including a cumulative publication timeline correlated with trailer releases and a breakdown of top content-creating channels.\nPerform an in-depth text analysis of comments from a specific popular video (YPCqTI0PVV0) to find top keywords, bigrams, and trigrams.\nConduct a targeted analysis of video titles between major trailer releases.\n\n\n\n\n\n\n\nPackages\n\n\n\n\n\n\nlibrary(tuber)\nlibrary(tidyverse)      \nlibrary(lubridate)      \nlibrary(stringi)       \nlibrary(wordcloud)      \nlibrary(tm)            \nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(ggplot2)\nlibrary(zoo)  \nlibrary(DT)"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#collecting-wicked-video-data",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#collecting-wicked-video-data",
    "title": "",
    "section": "1. Collecting ‘Wicked’ Video Data",
    "text": "1. Collecting ‘Wicked’ Video Data\n\nRun YouTubenews01.R (prerequisites: YouTube developer API)\nThe code first authenticates the YouTube API (Oauth code omitted). Then, it searches for up to 500 videos using the term “Wicked” within a specified date range.\n\n\n\n\n\n\nPackages\n\n\n\n\n\n\nlibrary(tuber)\nlibrary(tidyverse)      \nlibrary(lubridate)      \nlibrary(stringi)       \nlibrary(wordcloud)      \nlibrary(tm)            \nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(ggplot2)\nlibrary(zoo)  \nlibrary(DT)\nlibrary(svglite)\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nyt_oauth(\"yourYT_API\", \"X\", token = \"\")\n\nyt_Lets_Play &lt;- yt_search(term = \"Wicked\", \n                         published_before = \"2025-10-10T00:00:00Z\",\n                         published_after = \"2023-10-01T00:00:00Z\", \n                         max_results = 500)\n\nvideo_data_sample &lt;- yt_Lets_Play\nsubset_video_data &lt;- video_data_sample %&gt;%\n  select(video_id, channelTitle, title, publishedAt)\n\n\n\n\n\nprint(subset_video_data)\n## # A tibble: 500 × 4\n##    video_id    channelTitle           title                  publishedAt        \n##    &lt;chr&gt;       &lt;chr&gt;                  &lt;chr&gt;                  &lt;dttm&gt;             \n##  1 R2Xubj7lazE Universal Pictures     Wicked: For Good | Fi… 2025-09-24 12:15:30\n##  2 amgPXKrFZVg Wicked: For Good       Wicked | What Is This… 2024-11-22 16:15:27\n##  3 qeqj5GnoFUY Republic Records       Defying Gravity (From… 2024-11-22 05:00:42\n##  4 vt98AlBDI9Y Universal Pictures     Wicked: For Good | Of… 2025-06-05 00:55:09\n##  5 5znZFJWSZ7o Higher Quality Uploads Defying Gravity | Wic… 2025-03-05 18:58:36\n##  6 9zyPT0a7sx0 Movieclips             Wicked (2024) 4K - Th… 2025-03-01 14:00:15\n##  7 qemetmTzkeE Film Aesthetics        Defying Gravity by Cy… 2025-03-08 18:38:41\n##  8 mhKCRnUKp5U THEBLACKLABEL          ALLDAY PROJECT - ‘WIC… 2025-06-23 09:01:03\n##  9 6COmYeLsz4c Universal Pictures     Wicked - Official Tra… 2024-05-15 15:00:29\n## 10 kmnBhZ9AZhg Levi Plaifha           No One Mourns The Wic… 2025-02-14 01:56:09\n## # ℹ 490 more rows"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#word-cloud-of-video-titles",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#word-cloud-of-video-titles",
    "title": "",
    "section": "3. Word Cloud of Video Titles",
    "text": "3. Word Cloud of Video Titles\nThe most frequently used words in the titles of the 500 retrieved videos are extracted, cleaned, and visualized to show primary topics.\n\ntitles &lt;- yt_Lets_Play$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[^a-z ]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\ntitles_clean &lt;- titles_clean[titles_clean != \"\"]\n\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\nmy_stopwords &lt;- c(tm::stopwords(\"en\"), \"wicked\", \"x\", \"I\", \"l\", \"now\", \n                  \"every\", \"VS\", \"movie\", \"best\", \"day\")\n\nword_freq_df &lt;- word_freq_df %&gt;%\n  filter(!word %in% my_stopwords) %&gt;% \n  filter(freq &gt;= 3)\n\nset.seed(132)\nwordcloud(\n  words = word_freq_df$word,\n  freq = word_freq_df$freq,\n  max.words = 100,\n  random.order = FALSE,\n  colors = c(WICKED_EMERALD, WICKED_GREEN, WICKED_GOLD)\n)"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#analyzing-video-publication-trend",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#analyzing-video-publication-trend",
    "title": "",
    "section": "2. Analyzing Video Publication Trend",
    "text": "2. Analyzing Video Publication Trend\nThis plot shows the cumulative growth of video uploads over time. This method clearly visualizes the impact of major marketing events on content creation.\n\n\n\n\n\n\n\n\n\nFigure 1: Daily Video Publication Trend, The number of ‘Wicked’ videos published per day, with markers indicating trailer releases. The numbered markers on the chart correspond to major marketing and release events: 1 (2024-05-15): The release of the first full-length, official trailer for Wicked: Part One. 2 (2024-11-22): The official U.S. theatrical release date for Wicked: Part One. 3 (2025-09-05): The release of a new trailer for the sequel, Wicked: For Good (Part 2)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\ntrailer_dates &lt;- data.frame(\n  date = as.Date(c(\"2024-05-15\", \"2024-11-22\", \"2025-09-05\")),\n  label = c(\"1\", \"2\", \"3\")\n)\n\nyt_sm &lt;- yt_Lets_Play %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\nall_dates_for_seq &lt;- c(yt_sm$publish_date, trailer_dates$date)\nx_axis_limits &lt;- c(min(all_dates_for_seq), max(all_dates_for_seq))\n\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_area(fill = WICKED_GREEN, alpha = 0.5) +\n  geom_line(color = WICKED_EMERALD, linewidth = 1) +\n  \n  geom_segment(\n    data = trailer_dates,\n    aes(x = date, xend = date, y = 0, yend = 30),\n    linetype = \"dashed\",\n    color = WICKED_GOLD,\n    linewidth = 1,\n    inherit.aes = FALSE\n  ) +\n\n  geom_point(\n    data = trailer_dates,\n    aes(x = date + 22, y = 25),\n    shape = 21, # Circle with border\n    size = 8,   # Size of the circle\n    fill = WICKED_GOLD,\n    color = WICKED_GOLD,\n    inherit.aes = FALSE\n  ) +\n  \n  # Layer 2: Black Label Text (geom_text)\n  geom_text(\n    data = trailer_dates,\n    aes(x = date + 22, y = 25, label = label),\n    color = \"black\",\n    size = 3.5,\n    fontface = \"bold\",\n    inherit.aes = FALSE\n  ) +\n  \n  labs(\n    title = \"Youtube Upload Trend for 'Wicked' Videos\",\n    x = \"Publication Date (MM-YY)\",\n    y = \"Number of Videos (n)\"\n  ) +\n  \n  scale_y_continuous(limits = c(0, 30), breaks = seq(0, 30, 10)) +\n  scale_x_date(\n    date_breaks = \"3 months\", \n    date_labels = \"%m-%y\",\n    limits = x_axis_limits \n  ) +\n  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 20, b = 10)), \n    axis.title.y = element_text(margin = margin(r = 20))  \n  )\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis analysis is based on the data available, and its conclusions may be limited by gaps or omissions in the dataset."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#identifying-top-youtube-channels",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#identifying-top-youtube-channels",
    "title": "",
    "section": "4. Identifying Top YouTube Channels",
    "text": "4. Identifying Top YouTube Channels\nThe top 10 channels with the highest number of videos in our search results are visualized in a themed bar chart.\n\ntop_channels &lt;- yt_Lets_Play %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10, n)\n\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_col(\n    fill = WICKED_GOLD,\n    color = NA, \n    width = 0.8 \n  ) +\n  coord_flip() +\n  geom_text(\n    aes(label = n), hjust = 1.2, color = WICKED_EMERALD,\n    size = 4, fontface = \"bold\"\n  ) +\n  labs(\n    title = \"Top 10 Channels Generating 'Wicked' Content\",\n    x = NULL, \n    y = \"Number of Videos in Search\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.y = element_blank(), \n    axis.title.x = element_text(margin = margin(t = 20, b = 10)),\n    panel.grid.major.y = element_blank(), \n    panel.grid.major.x = element_line(color = \"grey80\", linetype = \"dashed\")\n  )\n\n\n\n\nFigure 2: Top 10 Channels Generating ‘Wicked’ Content – Channels ranked by number of videos in the search results.Note: ‘Wicked: For Good’ is the official movie channel."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#in-depth-comment-analysis-video-id-ypcqti0pvv0",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#in-depth-comment-analysis-video-id-ypcqti0pvv0",
    "title": "",
    "section": "5. In-Depth Comment Analysis (Video ID: YPCqTI0PVV0)",
    "text": "5. In-Depth Comment Analysis (Video ID: YPCqTI0PVV0)\nWe now dive deeper into a single video, “Wicked | First Look” (ID: YPCqTI0PVV0), to analyze audience reception by processing its comments.\n\n\n# --- Section 4: Advanced Data Analysis (Comments) ---\n\n# --- LOAD DATA FROM CSV ---\n# This reads the data from the file you saved, skipping the API call.\nvideocomments &lt;- read_csv(\"wicked_video_comments.csv\")\n\n# Define emoji removal function\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;%\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%\n      remove_emojis() %&gt;%\n      str_squish()\n  )\n\n# --- quanteda analysis ---\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm(toks_comments)\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# --- Plot 4: Top Words in Comments ---\ntstat_freq %&gt;%\n  head(20) %&gt;% # Plot top 20\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = WICKED_EMERALD) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Frequency (Count)\",\n    title = \"Top 20 Keywords in Video Comments\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 20, b = 10)), \n  )\n\n\n\n\nFigure 3: Top Keywords in Video Comments – A ranking of the 20 most common words used in comments for video YPCqTI0PVV0."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#comment-word-cloud-n-gram-analysis",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#comment-word-cloud-n-gram-analysis",
    "title": "",
    "section": "6. Comment Word Cloud & N-Gram Analysis",
    "text": "6. Comment Word Cloud & N-Gram Analysis\nTo find more context, we visualize the comments as a word cloud and perform N-gram (multi-word phrase) analysis.\n\n6.1. Comment Word Cloud\n\n# --- Plot 5: Comment Word Cloud ---\nset.seed(132)\ntextplot_wordcloud(\n  dfm_nonzero,\n  max_words = 100,\n  color = c(WICKED_EMERALD, WICKED_GREEN, WICKED_GOLD)\n)\n\n\n\n\n\n\n\n\n\n\n6.2. N-Gram Phrase Analysis\nThis analysis reveals common multi-word phrases (bigrams and trigrams) to provide context beyond single keywords and identify specific themes expressed by the audience.\n\nBigramTrigram\n\n\n\n# --- Section 6: Advanced Comment Analysis (N-Grams) ---\ntoks_comments_no_stop &lt;- tokens_remove(toks_comments, pattern = custom_stopwords)\n\n# --- Bigram Analysis ---\ntoks_bigrams &lt;- tokens_ngrams(toks_comments_no_stop, n = 2)\ndfmat_bigrams &lt;- dfm(toks_bigrams)\ndfm_bigrams_nonzero &lt;- dfmat_bigrams[ntoken(dfmat_bigrams) &gt; 0,]\ntstat_freq_bigrams &lt;- textstat_frequency(dfm_bigrams_nonzero, n = 20)\n\n# Plot Top Bigrams\ntstat_freq_bigrams %&gt;%\n  head(15) %&gt;%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = WICKED_GOLD, color = WICKED_EMERALD, linewidth = 0.5) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Frequency\",\n    title = \"Top 15 Two-Word Phrases (Bigrams)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 20, b = 10)), \n  )\n\n\n\n\nFigure 4: Top 15 Two-Word Phrases (Bigrams) – Highlights common phrases like ‘Cynthia Erivo’ and ‘singing live’.\n\n\n\n\n\n\n\n# --- Trigram Analysis ---\ntoks_trigrams &lt;- tokens_ngrams(toks_comments_no_stop, n = 3)\ndfmat_trigrams &lt;- dfm(toks_trigrams)\ndfm_trigrams_nonzero &lt;- dfmat_trigrams[ntoken(dfmat_trigrams) &gt; 0,]\ntstat_freq_trigrams &lt;- textstat_frequency(dfm_trigrams_nonzero, n = 20)\n\n# Plot Top Trigrams\ntstat_freq_trigrams %&gt;%\n  head(15) %&gt;%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = WICKED_PINK, color = WICKED_EMERALD, linewidth = 0.5) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Frequency\",\n    title = \"Top 15 Three-Word Phrases (Trigrams)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 20, b = 10)), \n  )\n\n\n\n\nFigure 5: Top 15 Three-Word Phrases (Trigrams) – Identifies specific repeated sentiments and topics."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#discussion",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#discussion",
    "title": "",
    "section": "Discussion:",
    "text": "Discussion:\n\nAssignment Reflection:\nThis analysis successfully turned a simple data pull into a clear story about public anticipation. By pairing the “when” of video metadata with the “what” of comment analysis, we got a pretty sharp picture of the ‘Wicked’ hype machine.\nThe most striking takeaway is from the publication trend plot. The conversation was basically dormant and then exploded the very moment official trailers were released. It’s a stark visualization of how modern marketing events don’t just contribute to the conversation; they practically create it from scratch.\nSo, what was everyone talking about? The comment analysis (Figure 3) leaves no doubt: the cast. The charts are completely dominated by “Ariana,” “Grande,” “Cynthia,” and “Erivo.” But the n-gram analysis (Figures 4 and 5) tells the real story. The top phrases weren’t just names; they were “singing live” and “ariana grande singing.” This is a fantastic insight. It shows the audience wasn’t just idly chatting about the stars; they were intensely focused on the authenticity of their vocal performances. This was clearly the #1 topic of interest and debate for the community.\nCombined with the “Top Channels” data, which shows a healthy mix of official promotion and organic fan commentary, the data paints a clear picture: official marketing provides the spark, but the fire of the hype is fueled by a very specific public debate about the cast’s performance."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#top-yt-posting-channels",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#top-yt-posting-channels",
    "title": "",
    "section": "3. Top YT Posting Channels",
    "text": "3. Top YT Posting Channels\nThe top 10 channels with the highest number of videos in our search results are visualized in a themed bar chart.\n\n\n\n\n\n\nFigure 2: Top 10 Channels Generating ‘Wicked’ Content – Channels ranked by number of videos in the search results.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ntop_channels &lt;- yt_Lets_Play %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10, n)\n\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_col(\n    fill = WICKED_GOLD,\n    color = NA, \n    width = 0.8 \n  ) +\n  coord_flip() +\n  geom_text(\n    aes(label = n), hjust = 1.2, color = WICKED_EMERALD,\n    size = 4, fontface = \"bold\"\n  ) +\n  labs(\n    title = \"Top 10 Channels Generating 'Wicked' Content\",\n    x = NULL, \n    y = \"Number of Videos in Search\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.y = element_blank(), \n    axis.title.x = element_text(margin = margin(t = 20, b = 10)),\n    panel.grid.major.y = element_blank(), \n    panel.grid.major.x = element_line(color = \"grey80\", linetype = \"dashed\"),\n  )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n‘Wicked: For Good’ is the official movie channel. It’s worth remembering that back in 2024, this channel was used for the first movie and likely had a different name (such as ‘Wicked: Part One’) before being updated for the sequel."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#yt-comment-analysis",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#yt-comment-analysis",
    "title": "",
    "section": "4. YT Comment Analysis",
    "text": "4. YT Comment Analysis\nWe now dive deeper into a single video, “Wicked | First Look” (ID: YPCqTI0PVV0), to analyze audience reception by processing its comments.\n\n\n\n\n\n\nFigure 3: Top Keywords in Video Comments – A ranking of the 20 most common words used in comments for video YPCqTI0PVV0.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\nvideocomments &lt;- read_csv(\"wicked_video_comments.csv\")\n\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;%\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%\n      remove_emojis() %&gt;%\n      str_squish()\n  )\n\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\", \"just\", \"like\", \"there\", \"my\", \"she\", \"he\", \"we\", \"when\", \"who\", \"more\", \"no\", \"out\", \"get\", \"up\", \"me\", \"would\", \"than\", \"some\", \"them\", \"been\", \"because\", \"did\", \"were\", \"had\", \"can't\", \"my\")\n\ndfmat_comments &lt;- dfm(toks_comments)\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\ntstat_freq %&gt;%\n  head(20) %&gt;% # Plot top 20\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = WICKED_EMERALD) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Frequency (Count)\",\n    title = \"Top 20 Keywords in Video Comments\",\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 20, b = 10)), \n  )\n\n\n\n\n\n\n4. N-Gram Analysis\nThis analysis reveals common multi-word phrases (bigrams and trigrams) to provide context beyond single keywords and identify specific themes expressed by the audience.\n\nBigramTrigram\n\n\n\n\n\n\n\n\nFigure 4: Top 15 Two-Word Phrases (Bigrams) – Highlights common phrases like ‘Cynthia Erivo’ and ‘singing live’.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# --- Section 6: Advanced Comment Analysis (N-Grams) ---\ntoks_comments_no_stop &lt;- tokens_remove(toks_comments, pattern = custom_stopwords)\n\n# --- Bigram Analysis ---\ntoks_bigrams &lt;- tokens_ngrams(toks_comments_no_stop, n = 2)\ndfmat_bigrams &lt;- dfm(toks_bigrams)\ndfm_bigrams_nonzero &lt;- dfmat_bigrams[ntoken(dfmat_bigrams) &gt; 0,]\ntstat_freq_bigrams &lt;- textstat_frequency(dfm_bigrams_nonzero, n = 20)\n\n# Plot Top Bigrams\ntstat_freq_bigrams %&gt;%\n  head(15) %&gt;%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = WICKED_GOLD, color = WICKED_EMERALD, linewidth = 0.5) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Frequency\",\n    title = \"Top 15 Two-Word Phrases (Bigrams)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 20, b = 10)), \n  )\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Top 15 Three-Word Phrases (Trigrams) – Identifies specific repeated sentiments and topics.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# --- Trigram Analysis ---\ntoks_trigrams &lt;- tokens_ngrams(toks_comments_no_stop, n = 3)\ndfmat_trigrams &lt;- dfm(toks_trigrams)\ndfm_trigrams_nonzero &lt;- dfmat_trigrams[ntoken(dfmat_trigrams) &gt; 0,]\ntstat_freq_trigrams &lt;- textstat_frequency(dfm_trigrams_nonzero, n = 20)\n\n# Plot Top Trigrams\ntstat_freq_trigrams %&gt;%\n  head(15) %&gt;%\n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_col(fill = WICKED_PINK, color = WICKED_EMERALD, linewidth = 0.5) +\n  coord_flip() +\n  labs(\n    x = NULL,\n    y = \"Frequency\",\n    title = \"Top 15 Three-Word Phrases (Trigrams)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(margin = margin(t = 20, b = 10)), \n  )"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/new.assignment5.html#top-yt-posting-channels-1",
    "href": "pages/EPPS_6302/Assignment04/new.assignment5.html#top-yt-posting-channels-1",
    "title": "",
    "section": "3. Top YT Posting Channels",
    "text": "3. Top YT Posting Channels\nThe top 10 channels with the highest number of videos in our search results are visualized in a themed bar chart.\n\n\n\n\n\n\nFigure 2: Top 10 Channels Generating ‘Wicked’ Content – Channels ranked by number of videos in the search results.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ntop_channels &lt;- yt_Lets_Play %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10, n)\n\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_col(\n    fill = WICKED_GOLD,\n    color = NA, \n    width = 0.8 \n  ) +\n  coord_flip() +\n  geom_text(\n    aes(label = n), hjust = 1.2, color = WICKED_EMERALD,\n    size = 4, fontface = \"bold\"\n  ) +\n  labs(\n    title = \"Top 10 Channels Generating 'Wicked' Content\",\n    x = NULL, \n    y = \"Number of Videos in Search\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    axis.title.y = element_blank(), \n    axis.title.x = element_text(margin = margin(t = 20, b = 10)),\n    panel.grid.major.y = element_blank(), \n    panel.grid.major.x = element_line(color = \"grey80\", linetype = \"dashed\"),\n  )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n‘Wicked: For Good’ is the official movie channel. It’s worth remembering that back in 2024, this channel was used for the first movie and likely had a different name (such as ‘Wicked: Part One’) before being updated for the sequel."
  },
  {
    "objectID": "pages/CoLab/cardsort.html",
    "href": "pages/CoLab/cardsort.html",
    "title": "",
    "section": "",
    "text": "This project analyzes data from a closed card sorting exercise to evaluate a proposed information architecture (IA) for a new e-commerce website. We tasked 20 users with sorting 15 product “cards” into 5 predefined categories.\nThe goal was to answer: 1. Do users intuitively understand our proposed categories? 2. Which products are a perfect fit for this structure? 3. Which products cause confusion, and why? 4. What data-driven recommendations can we make to improve the site navigation?"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#heatmaps-of-similarity-and-distance-matrices",
    "href": "pages/CoLab/cardsort.html#heatmaps-of-similarity-and-distance-matrices",
    "title": "",
    "section": "2. Heatmaps of Similarity and Distance Matrices",
    "text": "2. Heatmaps of Similarity and Distance Matrices\nplt.figure(figsize=(10, 8))\nsns.heatmap(similarity_df, annot=True, cmap='viridis', fmt=\".2f\")\nplt.title('Card Similarity Heatmap (Proportion of Users Grouping Same)')\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(distance_df, annot=True, cmap='plasma_r', fmt=\".2f\")\nplt.title('Card Distance Heatmap (1 - Similarity)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nLine Plot 1\n\n\n\n\n\n\n\nLine Plot 2"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#hierarchical-clustering-and-dendrogram",
    "href": "pages/CoLab/cardsort.html#hierarchical-clustering-and-dendrogram",
    "title": "",
    "section": "3. Hierarchical Clustering and Dendrogram",
    "text": "3. Hierarchical Clustering and Dendrogram\nWe use the distance matrix for clustering.\n\nlinked = linkage(distance_df, 'average')\n\nplt.figure(figsize=(12, 7))\ndendrogram(linked,\n           orientation='top',\n           labels=cards,\n           distance_sort='descending',\n           show_leaf_counts=True)\nplt.title('Card Similarity Dendrogram')\nplt.xlabel('Cards')\nplt.ylabel('Distance (1 - Similarity)')\nplt.tight_layout()\nplt.show()\n\nthresholds = [0.3, 0.5, 0.7]\n\nfor threshold in thresholds:\n    clusters = fcluster(linked, threshold, criterion='distance')\n    print(f\"\\nClusters at Distance Threshold = {threshold}:\")\n    cluster_dict = {}\n    for card, cluster_id in zip(cards, clusters):\n        if cluster_id not in cluster_dict:\n            cluster_dict[cluster_id] = []\n        cluster_dict[cluster_id].append(card)\n\n    for cluster_id, card_list in cluster_dict.items():\n        print(f\"  Cluster {cluster_id}: {', '.join(card_list)}\")\n\nnum_suggested_clusters = 5\nsuggested_clusters = fcluster(linked, num_suggested_clusters, criterion='maxclust')\n\n/var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T/ipykernel_30292/2420212685.py:1: ClusterWarning:\n\nThe symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n\n\n\n\n\n\n\n\n\n\n\nClusters at Distance Threshold = 0.3:\n  Cluster 4: Card 1\n  Cluster 13: Card 2\n  Cluster 10: Card 3\n  Cluster 3: Card 4\n  Cluster 1: Card 5\n  Cluster 5: Card 6\n  Cluster 6: Card 7\n  Cluster 8: Card 8\n  Cluster 9: Card 9\n  Cluster 11: Card 10\n  Cluster 15: Card 11\n  Cluster 7: Card 12\n  Cluster 14: Card 13\n  Cluster 2: Card 14\n  Cluster 12: Card 15\n\nClusters at Distance Threshold = 0.5:\n  Cluster 4: Card 1\n  Cluster 13: Card 2\n  Cluster 10: Card 3\n  Cluster 3: Card 4\n  Cluster 1: Card 5\n  Cluster 5: Card 6\n  Cluster 6: Card 7\n  Cluster 8: Card 8\n  Cluster 9: Card 9\n  Cluster 11: Card 10\n  Cluster 15: Card 11\n  Cluster 7: Card 12\n  Cluster 14: Card 13\n  Cluster 2: Card 14\n  Cluster 12: Card 15\n\nClusters at Distance Threshold = 0.7:\n  Cluster 4: Card 1\n  Cluster 13: Card 2\n  Cluster 10: Card 3\n  Cluster 3: Card 4\n  Cluster 1: Card 5\n  Cluster 5: Card 6\n  Cluster 6: Card 7\n  Cluster 8: Card 8\n  Cluster 9: Card 9\n  Cluster 11: Card 10\n  Cluster 15: Card 11\n  Cluster 7: Card 12\n  Cluster 14: Card 13\n  Cluster 2: Card 14\n  Cluster 12: Card 15\n\n\n\n# 7. Category Placement Frequency\n# How often was each card placed into each specific category?\n\ncard_category_placement = pd.DataFrame(0, index=cards, columns=categories)\n\nfor user_assignment_row in user_assignments:\n    for card_idx, category_idx in enumerate(user_assignment_row):\n        card_name = cards[card_idx]\n        category_name = categories[category_idx]\n        card_category_placement.loc[card_name, category_name] += 1\n\nprint(\"\\nCard Placement Frequency into Categories:\")\nprint(card_category_placement)\n\n\nCard Placement Frequency into Categories:\n         Category A  Category B  Category C  Category D  Category E\nCard 1            4           5           8           1           2\nCard 2            6           2           3           8           1\nCard 3            5           2           4           3           6\nCard 4            4           6           6           2           2\nCard 5            2           3           6           7           2\nCard 6            4           5           5           4           2\nCard 7            4           1           5           4           6\nCard 8            4           3           2           6           5\nCard 9            7           1           3           6           3\nCard 10           6           1           5           6           2\nCard 11           5           5           4           4           2\nCard 12           6           1           3           4           6\nCard 13           4           2           5           6           3\nCard 14           2           6           1           7           4\nCard 15           6           3           3           6           2\n\n\n\n# Normalize this by the number of users to get proportions\ncard_category_placement_prop = card_category_placement / num_users\n\nprint(\"\\nCard Placement Proportion into Categories:\")\nprint(card_category_placement_prop)\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(card_category_placement_prop, annot=True, cmap='YlGnBu', fmt=\".2f\")\nplt.title('Card Placement Proportion by Category')\nplt.xlabel('Category')\nplt.ylabel('Card')\nplt.tight_layout()\nplt.show()\n\n\nCard Placement Proportion into Categories:\n         Category A  Category B  Category C  Category D  Category E\nCard 1         0.20        0.25        0.40        0.05        0.10\nCard 2         0.30        0.10        0.15        0.40        0.05\nCard 3         0.25        0.10        0.20        0.15        0.30\nCard 4         0.20        0.30        0.30        0.10        0.10\nCard 5         0.10        0.15        0.30        0.35        0.10\nCard 6         0.20        0.25        0.25        0.20        0.10\nCard 7         0.20        0.05        0.25        0.20        0.30\nCard 8         0.20        0.15        0.10        0.30        0.25\nCard 9         0.35        0.05        0.15        0.30        0.15\nCard 10        0.30        0.05        0.25        0.30        0.10\nCard 11        0.25        0.25        0.20        0.20        0.10\nCard 12        0.30        0.05        0.15        0.20        0.30\nCard 13        0.20        0.10        0.25        0.30        0.15\nCard 14        0.10        0.30        0.05        0.35        0.20\nCard 15        0.30        0.15        0.15        0.30        0.10"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#most-agreed-upon-category-for-each-card",
    "href": "pages/CoLab/cardsort.html#most-agreed-upon-category-for-each-card",
    "title": "",
    "section": "8. Most Agreed Upon Category for Each Card",
    "text": "8. Most Agreed Upon Category for Each Card\nFor each card, what was the category it was most frequently placed into, and what was the agreement percentage?\n\nmost_agreed_category = []\nagreement_percentage = []\n\nfor card_name in cards:\n    card_idx = cards.index(card_name)\n    \n    # --- THIS IS THE FIX ---\n    # Get all assignments for the current card\n    card_assignments = user_assignments[:, card_idx]\n    # Find the most common category index (the mode)\n    counts = np.bincount(card_assignments)\n    most_common_category_idx = np.argmax(counts)\n    # --- END FIX ---\n\n    most_common_category_name = categories[most_common_category_idx]\n    count_in_most_common_category = np.sum(card_assignments == most_common_category_idx)\n    agreement_perc = (count_in_most_common_category / num_users) * 100\n\n    most_agreed_category.append(f\"{most_common_category_name} ({agreement_perc:.1f}%)\")\n    agreement_percentage.append(agreement_perc)\n\nmost_agreed_df = pd.DataFrame({'Card': cards,\n                               'Most Agreed Category (Agreement %)': most_agreed_category,\n                               'Agreement Percentage': agreement_percentage})\n\nmost_agreed_df = most_agreed_df.sort_values(by='Agreement Percentage', ascending=False)\n\nprint(\"\\nMost Agreed Upon Category for Each Card:\")\nprint(most_agreed_df)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=most_agreed_df['Card'], y=most_agreed_df['Agreement Percentage'], palette='coolwarm')\nplt.title('Agreement Percentage for Most Frequent Category Assignment per Card')\nplt.xlabel('Card')\nplt.ylabel('Agreement Percentage (%)')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\n\nMost Agreed Upon Category for Each Card:\n       Card Most Agreed Category (Agreement %)  Agreement Percentage\n0    Card 1                 Category C (40.0%)                  40.0\n1    Card 2                 Category D (40.0%)                  40.0\n4    Card 5                 Category D (35.0%)                  35.0\n8    Card 9                 Category A (35.0%)                  35.0\n13  Card 14                 Category D (35.0%)                  35.0\n2    Card 3                 Category E (30.0%)                  30.0\n3    Card 4                 Category B (30.0%)                  30.0\n6    Card 7                 Category E (30.0%)                  30.0\n7    Card 8                 Category D (30.0%)                  30.0\n9   Card 10                 Category A (30.0%)                  30.0\n11  Card 12                 Category A (30.0%)                  30.0\n12  Card 13                 Category D (30.0%)                  30.0\n14  Card 15                 Category A (30.0%)                  30.0\n5    Card 6                 Category B (25.0%)                  25.0\n10  Card 11                 Category A (25.0%)                  25.0\n\n\n/var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T/ipykernel_30292/3561829871.py:32: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\nFigure 4: Agreement percentage for the most frequently assigned category for each card."
  },
  {
    "objectID": "pages/CoLab/cardsort.html#category-size-distribution",
    "href": "pages/CoLab/cardsort.html#category-size-distribution",
    "title": "",
    "section": "9. Category Size Distribution",
    "text": "9. Category Size Distribution\nHow many cards were placed into each category on average? (This doesn’t make sense for closed sort as much, as categories are fixed, but we can see how many times each category was chosen by users.)\n\ncategory_choice_counts = np.zeros(num_categories)\nfor user_assignment_row in user_assignments:\n    for category_idx in user_assignment_row:\n        category_choice_counts[category_idx] += 1\n\ncategory_choice_df = pd.DataFrame({'Category': categories, 'Total Choices': category_choice_counts})\ncategory_choice_df['Average Choices per Card'] = category_choice_df['Total Choices'] / num_cards\ncategory_choice_df = category_choice_df.sort_values(by='Total Choices', ascending=False)\n\nprint(\"\\nTotal times each category was chosen by users:\")\nprint(category_choice_df[['Category', 'Total Choices']])\n\nplt.figure(figsize=(8, 5))\nsns.barplot(x=category_choice_df['Category'], y=category_choice_df['Total Choices'], palette='rocket')\nplt.title('Total Times Each Category Was Chosen Across All Users and Cards')\nplt.xlabel('Category')\nplt.ylabel('Total Number of Choices')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nAnalysis Complete. Review the visualizations and printed tables.\")\n\n\nTotal times each category was chosen by users:\n     Category  Total Choices\n3  Category D           74.0\n0  Category A           69.0\n2  Category C           63.0\n4  Category E           48.0\n1  Category B           46.0\n\n\n/var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T/ipykernel_30292/960051315.py:14: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis Complete. Review the visualizations and printed tables."
  },
  {
    "objectID": "pages/CoLab/cardsort.html#most-confusing-cards",
    "href": "pages/CoLab/cardsort.html#most-confusing-cards",
    "title": "",
    "section": "6. Most Confusing Cards",
    "text": "6. Most Confusing Cards\nCards that were placed into many different categories by different users.\n\nsuggested_cluster_dict = {}\nfor card, cluster_id in zip(cards, suggested_clusters):\n    if cluster_id not in suggested_cluster_dict:\n        suggested_cluster_dict[cluster_id] = []\n    suggested_cluster_dict[cluster_id].append(card)\n\nfor cluster_id, card_list in suggested_cluster_dict.items():\n    print(f\"  Suggested Group {cluster_id}: {', '.join(card_list)}\")\n\ncard_category_counts = {}\nfor card_idx, card_name in enumerate(cards):\n    unique_categories_for_card = np.unique(user_assignments[:, card_idx])\n    card_category_counts[card_name] = len(unique_categories_for_card)\n\nconfusing_cards_df = pd.DataFrame.from_dict(card_category_counts, orient='index', columns=['Unique Categories Assigned'])\nconfusing_cards_df = confusing_cards_df.sort_values(by='Unique Categories Assigned', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=confusing_cards_df.index, y=confusing_cards_df['Unique Categories Assigned'], palette='viridis')\nplt.title('Number of Unique Categories Assigned per Card')\nplt.xlabel('Card')\nplt.ylabel('Number of Unique Categories')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\n  Suggested Group 1: Card 1, Card 4, Card 5, Card 6, Card 14\n  Suggested Group 5: Card 2, Card 11, Card 13\n  Suggested Group 3: Card 3, Card 8, Card 9\n  Suggested Group 2: Card 7, Card 12\n  Suggested Group 4: Card 10, Card 15\n\n\n/var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T/ipykernel_30084/3072791879.py:19: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\n# 7. Category Placement Frequency\n# How often was each card placed into each specific category?\n\ncard_category_placement = pd.DataFrame(0, index=cards, columns=categories)\n\nfor user_assignment_row in user_assignments:\n    for card_idx, category_idx in enumerate(user_assignment_row):\n        card_name = cards[card_idx]\n        category_name = categories[category_idx]\n        card_category_placement.loc[card_name, category_name] += 1\n\nprint(\"\\nCard Placement Frequency into Categories:\")\nprint(card_category_placement)\n\n\nCard Placement Frequency into Categories:\n         Category A  Category B  Category C  Category D  Category E\nCard 1            4           5           8           1           2\nCard 2            6           2           3           8           1\nCard 3            5           2           4           3           6\nCard 4            4           6           6           2           2\nCard 5            2           3           6           7           2\nCard 6            4           5           5           4           2\nCard 7            4           1           5           4           6\nCard 8            4           3           2           6           5\nCard 9            7           1           3           6           3\nCard 10           6           1           5           6           2\nCard 11           5           5           4           4           2\nCard 12           6           1           3           4           6\nCard 13           4           2           5           6           3\nCard 14           2           6           1           7           4\nCard 15           6           3           3           6           2\n\n\n\n# Normalize this by the number of users to get proportions\ncard_category_placement_prop = card_category_placement / num_users\n\nprint(\"\\nCard Placement Proportion into Categories:\")\nprint(card_category_placement_prop)\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(card_category_placement_prop, annot=True, cmap='YlGnBu', fmt=\".2f\")\nplt.title('Card Placement Proportion by Category')\nplt.xlabel('Category')\nplt.ylabel('Card')\nplt.tight_layout()\nplt.show()\n\n\nCard Placement Proportion into Categories:\n         Category A  Category B  Category C  Category D  Category E\nCard 1         0.20        0.25        0.40        0.05        0.10\nCard 2         0.30        0.10        0.15        0.40        0.05\nCard 3         0.25        0.10        0.20        0.15        0.30\nCard 4         0.20        0.30        0.30        0.10        0.10\nCard 5         0.10        0.15        0.30        0.35        0.10\nCard 6         0.20        0.25        0.25        0.20        0.10\nCard 7         0.20        0.05        0.25        0.20        0.30\nCard 8         0.20        0.15        0.10        0.30        0.25\nCard 9         0.35        0.05        0.15        0.30        0.15\nCard 10        0.30        0.05        0.25        0.30        0.10\nCard 11        0.25        0.25        0.20        0.20        0.10\nCard 12        0.30        0.05        0.15        0.20        0.30\nCard 13        0.20        0.10        0.25        0.30        0.15\nCard 14        0.10        0.30        0.05        0.35        0.20\nCard 15        0.30        0.15        0.15        0.30        0.10"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#lower-triangular-similarity-matrix-for-the-card-sort-data",
    "href": "pages/CoLab/cardsort.html#lower-triangular-similarity-matrix-for-the-card-sort-data",
    "title": "",
    "section": "lower triangular similarity matrix for the card sort data",
    "text": "lower triangular similarity matrix for the card sort data\n\nplt.figure(figsize=(10, 8))\n\nmask = np.triu(np.ones_like(similarity_df, dtype=bool))\n\nsns.heatmap(similarity_df,\n            annot=True,       # Show values\n            cmap='viridis',   # Color map\n            fmt=\".2f\",        # Format annotations\n            mask=mask,        # Apply the mask\n            cbar=True,        # Show color bar\n            xticklabels=similarity_df.columns, # Explicitly set x-axis labels\n            yticklabels=similarity_df.index)   # Explicitly set y-axis labels\n\nax = plt.gca()\n\nax.set_yticklabels(similarity_df.index, rotation=0)\n\nax.set_xticklabels(similarity_df.columns, rotation=90)\n\nax.xaxis.tick_top() # Move x-axis ticks and labels to the top\nax.xaxis.set_label_position('top') # Set the label position to top\n\nplt.title('Lower Triangular Card Similarity Matrix')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#card-category-ideas",
    "href": "pages/CoLab/cardsort.html#card-category-ideas",
    "title": "",
    "section": "💡 Card & Category Ideas",
    "text": "💡 Card & Category Ideas\nFor this project, we’re simulating data for a new electronics e-commerce site.\n\nOur 5 Proposed Categories:\n\nComputers & Laptops\nPhones & Accessories\nTV & Home Theater\nCameras & Drones\nSmart Home & Audio\n\nThe 15 Cards We Tested:\n\nClear Items: Laptop, Smartphone, 4K Television, DSLR Camera, Smart Speaker\nAmbiguous Items: Gaming Mouse (Computers or Accessories?), Phone Case (Phones or Accessories?), Soundbar (TV or Audio?), Streaming Stick (TV or Smart Home?), Security Camera (Cameras or Smart Home?), Headphones (Phones or Audio?), Drone (Cameras or Hobbies?), Smartwatch (Phones or Smart Home?), VR Headset (Computers or TV?), Portable Charger (Phones or Accessories?)\n\n\n\n\n# Install necessary libraries\n!pip install scikit-learn plotly matplotlib seaborn\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\nnum_users = 20\nnum_cards = 15\nnum_categories = 5\n\ncards = [f'Card {i+1}' for i in range(num_cards)]\ncategories = [f'Category {chr(65+i)}' for i in range(num_categories)]\n\nnp.random.seed(42)\nuser_assignments = np.random.randint(0, num_categories, size=(num_users, num_cards))\n\ndf_assignments = pd.DataFrame(user_assignments, columns=cards)\ndf_assignments['User'] = [f'User {i+1}' for i in range(num_users)]\ndf_assignments = df_assignments.set_index('User')\n\nco_occurrence_matrix = np.zeros((num_cards, num_cards))\n\nfor i in range(num_cards):\n    for j in range(num_cards):\n        # Count how many users assigned card i and card j to the same category\n        co_occurrence_matrix[i, j] = np.sum(user_assignments[:, i] == user_assignments[:, j])\n\nco_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=cards, columns=cards)\n\nsimilarity_matrix = co_occurrence_matrix / num_users\nsimilarity_df = pd.DataFrame(similarity_matrix, index=cards, columns=cards)\n\n\ndistance_matrix = 1 - similarity_matrix\ndistance_df = pd.DataFrame(distance_matrix, index=cards, columns=cards)\n\nRequirement already satisfied: scikit-learn in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (1.6.1)\nRequirement already satisfied: plotly in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (6.4.0)\nRequirement already satisfied: matplotlib in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (3.10.7)\nRequirement already satisfied: seaborn in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (0.13.2)\nRequirement already satisfied: numpy&gt;=1.19.5 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (2.2.5)\nRequirement already satisfied: scipy&gt;=1.6.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib&gt;=1.2.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: narwhals&gt;=1.15.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from plotly) (2.10.2)\nRequirement already satisfied: packaging in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from plotly) (25.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (1.4.9)\nRequirement already satisfied: pillow&gt;=8 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing&gt;=3 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from seaborn) (2.3.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#setup-and-preprocessing",
    "href": "pages/CoLab/cardsort.html#setup-and-preprocessing",
    "title": "",
    "section": "1. Setup and Preprocessing",
    "text": "1. Setup and Preprocessing\n\n# Install necessary libraries\n!pip install scikit-learn plotly matplotlib seaborn\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\nnum_users = 20\nnum_cards = 15\nnum_categories = 5\n\ncards = [f'Card {i+1}' for i in range(num_cards)]\ncategories = [f'Category {chr(65+i)}' for i in range(num_categories)]\n\nnp.random.seed(42)\nuser_assignments = np.random.randint(0, num_categories, size=(num_users, num_cards))\n\ndf_assignments = pd.DataFrame(user_assignments, columns=cards)\ndf_assignments['User'] = [f'User {i+1}' for i in range(num_users)]\ndf_assignments = df_assignments.set_index('User')\n\nco_occurrence_matrix = np.zeros((num_cards, num_cards))\n\nfor i in range(num_cards):\n    for j in range(num_cards):\n        # Count how many users assigned card i and card j to the same category\n        co_occurrence_matrix[i, j] = np.sum(user_assignments[:, i] == user_assignments[:, j])\n\nco_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=cards, columns=cards)\n\nsimilarity_matrix = co_occurrence_matrix / num_users\nsimilarity_df = pd.DataFrame(similarity_matrix, index=cards, columns=cards)\n\n\ndistance_matrix = 1 - similarity_matrix\ndistance_df = pd.DataFrame(distance_matrix, index=cards, columns=cards)\n\nRequirement already satisfied: scikit-learn in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (1.6.1)\nRequirement already satisfied: plotly in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (6.4.0)\nRequirement already satisfied: matplotlib in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (3.10.7)\nRequirement already satisfied: seaborn in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (0.13.2)\nRequirement already satisfied: numpy&gt;=1.19.5 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (2.2.5)\nRequirement already satisfied: scipy&gt;=1.6.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib&gt;=1.2.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: narwhals&gt;=1.15.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from plotly) (2.10.2)\nRequirement already satisfied: packaging in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from plotly) (25.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (1.4.9)\nRequirement already satisfied: pillow&gt;=8 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing&gt;=3 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pandas&gt;=1.2 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from seaborn) (2.3.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /Users/olivermyers/.virtualenvs/r-reticulate/lib/python3.12/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#the-big-picture-user-agreement-dendrogram",
    "href": "pages/CoLab/cardsort.html#the-big-picture-user-agreement-dendrogram",
    "title": "",
    "section": "2. The Big Picture: User Agreement Dendrogram",
    "text": "2. The Big Picture: User Agreement Dendrogram\nWe start with the most important visualization: the dendrogram. This chart shows the overall grouping consensus from all 20 users. Cards connected by a low horizontal line were frequently grouped together, while cards connected by high lines were not. This is our primary tool for visualizing the users’ “mental model.”\n# `linkage` performs the clustering on our distance matrix\nlinked = linkage(distance_matrix, 'average')\n\nplt.figure(figsize=(12, 8))\ndendrogram(linked,\n           orientation='right', # 'right' is often easier to read\n           labels=cards,\n           distance_sort='descending')\n\nplt.title('Card Similarity Dendrogram', fontsize=16)\nplt.xlabel('Distance (1 - Similarity)', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n# --- Suggested Groupings ---\n# We can also \"cut\" the tree to see the data-driven clusters\nnum_suggested_clusters = 5\nsuggested_clusters = fcluster(linked, num_suggested_clusters, criterion='maxclust')\n\nprint(f\"\\nData-Driven Groups (if cut at {num_suggested_clusters} clusters):\")\nsuggested_cluster_dict = {}\nfor card, cluster_id in zip(cards, suggested_clusters):\n    if cluster_id not in suggested_cluster_dict:\n        suggested_cluster_dict[cluster_id] = []\n    suggested_cluster_dict[cluster_id].append(card)\n\nfor cluster_id, card_list in sorted(suggested_cluster_dict.items()):\n    print(f\"  Group {cluster_id}: {', '.join(card_list)}\")\n    \n    \n\nmost_agreed_category = []\nagreement_percentage = []\n\nfor card_name in cards:\n    card_idx = cards.index(card_name)\n    \n    # Get all assignments for the current card\n    card_assignments = user_assignments[:, card_idx]\n    \n    # Find the most common category index (the mode)\n    counts = np.bincount(card_assignments)\n    most_common_category_idx = np.argmax(counts)\n\n    most_common_category_name = categories[most_common_category_idx]\n    count_in_most_common_category = np.sum(card_assignments == most_common_category_idx)\n    agreement_perc = (count_in_most_common_category / num_users) * 100\n\n    most_agreed_category.append(f\"{most_common_category_name} ({agreement_perc:.1f}%)\")\n    agreement_percentage.append(agreement_perc)\n\nmost_agreed_df = pd.DataFrame({\n    'Card': cards,\n    'Most Agreed Category (Agreement %)': most_agreed_category,\n    'Agreement Percentage': agreement_percentage\n})\n\nmost_agreed_df = most_agreed_df.sort_values(by='Agreement Percentage', ascending=False)\n\nprint(\"\\nMost Agreed Upon Category for Each Card:\")\nprint(most_agreed_df[['Card', 'Most Agreed Category (Agreement %)', 'Agreement Percentage']].to_string(index=False))\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nsns.barplot(x=most_agreed_df['Agreement Percentage'], y=most_agreed_df['Card'], palette='summer', orient='h')\nplt.title('Agreement Percentage for Most Frequent Category per Card', fontsize=16)\nplt.xlabel('Agreement Percentage (%)', fontsize=12)\nplt.ylabel('Card', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n/var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T/ipykernel_30292/3006550968.py:2: ClusterWarning:\n\nThe symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n\n\n\n/var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T/ipykernel_30292/3006550968.py:65: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Line Plot 1\n\n\n\n\n\n\nData-Driven Groups (if cut at 5 clusters):\n  Group 1: Card 1, Card 4, Card 5, Card 6, Card 14\n  Group 2: Card 7, Card 12\n  Group 3: Card 3, Card 8, Card 9\n  Group 4: Card 10, Card 15\n  Group 5: Card 2, Card 11, Card 13\n\nMost Agreed Upon Category for Each Card:\n   Card Most Agreed Category (Agreement %)  Agreement Percentage\n Card 1                 Category C (40.0%)                  40.0\n Card 2                 Category D (40.0%)                  40.0\n Card 5                 Category D (35.0%)                  35.0\n Card 9                 Category A (35.0%)                  35.0\nCard 14                 Category D (35.0%)                  35.0\n Card 3                 Category E (30.0%)                  30.0\n Card 4                 Category B (30.0%)                  30.0\n Card 7                 Category E (30.0%)                  30.0\n Card 8                 Category D (30.0%)                  30.0\nCard 10                 Category A (30.0%)                  30.0\nCard 12                 Category A (30.0%)                  30.0\nCard 13                 Category D (30.0%)                  30.0\nCard 15                 Category A (30.0%)                  30.0\n Card 6                 Category B (25.0%)                  25.0\nCard 11                 Category A (25.0%)                  25.0\n\n\n\n\n\n\n\n\n\n\nFigure 2: Line Plot 2"
  },
  {
    "objectID": "pages/CoLab/cardsort.html#high-agreement-whats-working",
    "href": "pages/CoLab/cardsort.html#high-agreement-whats-working",
    "title": "",
    "section": "3. High Agreement: What’s Working?",
    "text": "3. High Agreement: What’s Working?\nThis chart identifies our “easy wins.” It shows the percentage of users who agreed on the most popular category for each card. Cards with high bars (e.g., &gt;80%) have a clear “home” and their proposed placement is successful."
  },
  {
    "objectID": "pages/CoLab/cardsort.html#deep-dive-diagnosing-the-problem-cards",
    "href": "pages/CoLab/cardsort.html#deep-dive-diagnosing-the-problem-cards",
    "title": "",
    "section": "5. Deep Dive: Diagnosing the “Problem Cards”",
    "text": "5. Deep Dive: Diagnosing the “Problem Cards”\nNow we can diagnose why the confusing cards were a problem. This heatmap shows the proportion of users who placed each card into each specific category. It’s the perfect tool for spotting ambiguity—for example, a card’s row being “split” 50/50 between two different categories.\n\n# Calculate placement frequency\ncard_category_placement = pd.DataFrame(0, index=cards, columns=categories)\n\nfor user_assignment_row in user_assignments:\n    for card_idx, category_idx in enumerate(user_assignment_row):\n        card_name = cards[card_idx]\n        category_name = categories[category_idx]\n        card_category_placement.loc[card_name, category_name] += 1\n\n# Normalize to get proportions\ncard_category_placement_prop = card_category_placement / num_users\n\nprint(\"\\nCard Placement Proportion into Categories:\")\nprint(card_category_placement_prop.head())\n\n# Plot the heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(card_category_placement_prop, annot=True, cmap='YlGnBu', fmt=\".2f\")\nplt.title('Card Placement Proportion by Category', fontsize=16)\nplt.xlabel('Category', fontsize=12)\nplt.ylabel('Card', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\nCard Placement Proportion into Categories:\n        Category A  Category B  Category C  Category D  Category E\nCard 1        0.20        0.25        0.40        0.05        0.10\nCard 2        0.30        0.10        0.15        0.40        0.05\nCard 3        0.25        0.10        0.20        0.15        0.30\nCard 4        0.20        0.30        0.30        0.10        0.10\nCard 5        0.10        0.15        0.30        0.35        0.10\n\n\n\n\n\n\n\n\nFigure 3: Heatmap showing the proportion of users who placed each card into each category."
  }
]