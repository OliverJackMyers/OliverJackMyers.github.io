[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oliver Myers’s Github Data Website",
    "section": "",
    "text": "Welcome Friend!\nHello and welcome to my website! My name is Oliver Myers, and I am excited to share my journey in data science and UX research with you.\n\nLocation: Dallas, Texas\n\nRole: Mixed-Methods User Experience Researcher\n\nSchool: University of Texas at Dallas\n\nDegree: Master of Science in Applied Cognition and Neuroscience, focusing on Human-Computer Interaction\n\n\n\n\nUX Research Portfolio:\nIf you are looking for my UX Research Portfolio here is the link Portfolio: OliverJackMyers.com\n\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "About Me",
    "section": "",
    "text": "About Me\nHi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nPortfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "I analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\n\n\n\nJuly 14, 2024:\n\nTrump’s Peak: This date marks the first attempted assassination at a Trump rally, leading to a significant spike in search interest.\n\nJuly 21, 2024:\n\nHarris Begins to Trend: Following President Biden’s decision to drop out of the race, interest in Kamala Harris starts to increase.\n\nJuly 22, 2024:\n\nHarris Surpasses Trump: Harris peaks above Trump as she announces her candidacy for president.\n\nAugust 6, 2024:\n\nHarris’s Peak Over Trump: Harris reaches another peak after announcing Tim Walz as her running mate.\n\nAugust 23, 2024:\n\nAcceptance Speech: Harris experiences another spike in search interest during her acceptance speech at the DNC, where she becomes the Democratic front-runner for the 2024 presidential election.\n\nSeptember 11, 2024:\n\nSimultaneous Peaks: Both Trump and Harris see significant spikes as they attend the 9/11 Memorial event in New York City.\n\nSeptember 15, 2024:\n\nTrump’s Peak: A second attempted assassination at Trump’s international golf course results in another surge in interest for Trump.\n\nElection Momentum:\n\nAs the dates approach Election Day, search interest for all three terms—Trump, Harris, and Election—steadily increases."
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Analyzing Biden-Xi summit data\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n\n\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "CopyOfassignment2.html",
    "href": "CopyOfassignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Method 1: Analyze Google Trends search term data for “Trump”, “Kamala Harris” and “Election”\n\ngoogle_trends_data &lt;- read.csv(\"~/Desktop/Trump.Harris.Google.TrendsData.Method1.csv\")\n\nMethod 2: Using gtrendsR Package to collect data\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\ninstall.packages(\"gtrendsR\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//Rtmpa9c09Q/downloaded_packages\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n#plot(HarrisTrumpElection_interest$hits, type = \"l\", main = \"Google Trends Data for Trump, Harris, and Election\",\n#     xlab = \"Time\", ylab = \"Search Interest\")\n\n\n## Install package\n#install.packages(\"gtrendsR\")\n\n#library(gtrendsR)\n\n#res &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"))\n#plot(res)\n\nDifferences between the two methods: In the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Run YouTubenews01.R (prerequisites: YouTube developer API):\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(gridExtra)\nlibrary(httr)\nlibrary(tm)\n\nyt_oauth(\"*****\", \"*****\", token = \"\")\n\n\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\n#### Display the first few rows\nhead(yt_uselection2024)\n\n\n     video_id                channelId\n1 1DjhOk0nrpA UC_gUM8rL-Lrg6O3adPW9K1g\n2 M4bJYlyK6ms UCupvZG-5ko_eiXAupbDfxWw\n3 ZvAth0bni08 UCTvZcgH_rsvaGuZ8mU5jpnA\n4 Pwn8LVR7Ufc UC_gUM8rL-Lrg6O3adPW9K1g\n5 SMXJ-qx54x4 UCLXo7UDZvByw2ixzpQCufnA\n6 3yWTbnviLUE UCO0akufu9MOzyz3nvGIXAAw\n                                                                             title\n1            USA News: Kamala Harris Team Still Raising Funds? | World News | WION\n2 Democratic lawmaker: Hunter Biden&#39;s pardon &#39;will be used against us&#39;\n3                                Trump responds to Biden pardoning his son, Hunter\n4          Donald Trump&#39;s Warning To BRICS Nation Including India | World News\n5                                    America’s shift to the right, in data #shorts\n6            Donald Trump ensuring Hamas will ‘pay the price’ for holding hostages\n\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n\n\n\n\n\n\n### 4.2. Plot Video Publish Dates\n# Format publish dates and aggregate data\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n\n\n\n\n\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")\n\n\n\n\n\n\n\n\n\n\n\nRepeat the data collection of CNN’s channel stats, video stats and comments:\n## Required Libraries\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(quanteda)\nlibrary(ggplot2)\n\n## CNN Channel ID\ncnn_channel_id &lt;- \"UCupvZG-5ko_eiXAupbDfxWw\"\n\n## get channel stats\ncnn_data &lt;- get_channel_stats(channel_id = \"UCupvZG-5ko_eiXAupbDfxWw\", mine = NULL)\ncnn_stats = cnn_data$statistics\nhead(cnn_stats)\n\n\nChannel Title: CNN \nNo. of Views: 17009316432 \nNo. of Subscribers: 17300000 \nNo. of Videos: 168275 \n\n\n$viewCount\n[1] \"17009316432\"\n\n$subscriberCount\n[1] \"17300000\"\n\n$hiddenSubscriberCount\n[1] FALSE\n\n$videoCount\n[1] \"168275\"\n\n\nvideo_id =  \"Yzb5LGwt6LA\"\n\n## Get Video Statistics\nvideo_stats &lt;- get_stats(video_id)\ncat(\"Video Stats:\\n\")\nhead(video_stats)\n\n\nAnalyze the stats and comments:\n\n\nVideo Stats:\n\n\n$id\n[1] \"Yzb5LGwt6LA\"\n\n$viewCount\n[1] \"145812\"\n\n$likeCount\n[1] \"1590\"\n\n$favoriteCount\n[1] \"0\"\n\n$commentCount\n[1] \"700\"\n\n\nvideocomments &lt;- get_all_comments(video_id)\nhead(videocomments)\n\n\n[1] \"IN SYRIA SEEMS SUFIANI RULER (ENEMY OF IMAM MAHDI ALAIHISSALAM) COMING BY BACKING OF WEST ! \\\"HE WILL HAVE  THREE STICKS\\\" INDICATES THREE FORCES - LAND , AIR AND NAVAL . HE WILL CONQUER EGYPT TOO .\"                    \n[2] \"Anyone remember that place was full of snipers?\"                                                                                                                                                                           \n[3] \"Usually cnn knows about these things before they happen?? If you go by title.. surprisingly surprised cnn..\"                                                                                                               \n[4] \"\\\"their various bakers\\\"\\nWtf r u talking about??!!!\"                                                                                                                                                                      \n[5] \"طبيعي ان نرى قلعت حلب عندما يعود الغرب لعصورهم الوصطى ويحتل القدس  الصليبيين اليهود😂\"                                                                                                                                     \n[6] \"I wonder who is financing those rebels. Pretty sure that they were a part of Al Qaeda once. In the end it does not matter. The result stays the same. Many innocent people will suffer for the political gains of the few.\"\n\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n\nCan you use quanteda to analyze the text data from YouTube comments?\n\n\n        feature frequency rank docfreq group\n1        rebels        81    1      74   all\n2           not        76    2      67   all\n3         syria        64    3      53   all\n4            us        54    4      45   all\n5        people        52    5      31   all\n6        russia        47    6      38   all\n7    terrorists        42    7      33   all\n8           war        39    8      35   all\n9            no        39    8      29   all\n10           we        38   10      31   all\n11          cnn        37   11      28   all\n12         like        37   11      30   all\n13          has        35   13      20   all\n14         when        35   13      30   all\n15          who        33   15      31   all\n16           he        29   16      18   all\n17          why        29   16      27   all\n18        there        29   16      25   all\n19          now        29   16      27   all\n20         just        26   20      24   all\n21         isis        26   20      24   all\n22        world        26   20      22   all\n23      because        25   23      20   all\n24       spirit        25   23       2   all\n25    terrorist        24   25      18   all\n26   artificial        24   25       1   all\n27 intelligence        24   25       1   all\n\n\n# Visualize the Most Frequent Words\ntstat_freq %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\", title = \"Top 15 Words in YouTube Comments\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Create a Word Cloud\nset.seed(132)\ntextplot_wordcloud(dfm_nonzero, max_words = 100)\n\n\n\n\n\n\n\n\n\n\nAssignment Reflection:\nIt took considerable effort, but I successfully downloaded the comments from a CNN YouTube video. To analyze the data, I used ChatGPT for coding assistance along with the quanteda package (tutorials) and the tuber package (documentation).\nI processed the comments by tokenizing the words, removing emojis and usernames, and filtering out filler words. This allowed me to create visualizations of the most frequently used words. The analysis revealed that the comments were predominantly negative, suggesting the video likely discusses terrorism in the Middle East and Syria, based on the word frequencies and the context of a CNN video."
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Use rvest_wiki01.R to scrape Foreign reserve data:\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(scales) \n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $623,983         2025-01-17 1,880  \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;\n\n\n\n\nModify the program to scrape other tables in Wikipedia:\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/United_States_dollar'\n\n# Read the webpage\nusd_page &lt;- read_html(url)\n\n# Extract the third table on the page using XPath (skipping image columns 2 and 3)\nusd_table &lt;- usd_page %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[3]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nusd_data &lt;- usd_table[[1]]\n\n# Remove columns 2 and 3 (image columns)\nusd_data &lt;- usd_data %&gt;%\n  select(-`Front`, -`Reverse`)\n\n# Rename columns for clarity\ncolnames(usd_data) &lt;- c(\"Denomination\", \"Portrait\", \"Reverse_Motif\", \n                        \"First_Series\", \"Latest_Series\", \"Circulation\")\n\n# Clean up the data (if necessary)\nusd_data &lt;- usd_data %&gt;%\n  filter(!is.na(Denomination) & Denomination != \"\")  # Remove empty rows\n\n# View the cleaned and structured data\nhead(usd_data)\n\n\n# A tibble: 6 × 6\n  Denomination   Portrait   Reverse_Motif First_Series Latest_Series Circulation\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n1 One dollar     George Wa… Great Seal o… Series 1963… Series 2021[… Wide       \n2 Two dollars    Thomas Je… Declaration … Series 1976  Series 2017A  Limited[48]\n3 Five dollars   Abraham L… Lincoln Memo… Series 2006  Series 2021[… Wide       \n4 Ten dollars    Alexander… Treasury Bui… Series 2004A Series 2017A  Wide       \n5 Twenty dollars Andrew Ja… White House   Series 2004  Series 2017A  Wide       \n6 Fifty dollars  Ulysses S… United State… Series 2004  Series 2017A  Wide       \n\n\n\n\nSearch government documents using this link: https://www.govinfo.gov/app/search/ and Download ten documents\nsetwd(\"/Users/olivermyers/MyWebsite/govtdata.assignent04\")\n\n# csv downloaed from https://www.govinfo.gov/app/search/ and searching \"water\", filtering congressonal bills from 2024\n\n# Read the CSV file without skipping rows\ngovfiles &lt;- read.csv(file = \"/Users/olivermyers/MyWebsite/govinfo-search-results.csv\", skip = 2, header = FALSE)\n\ncolnames(govfiles) &lt;- govfiles[1, ]\ngovfiles &lt;- govfiles[-1, ]\nrownames(govfiles) &lt;- NULL\ncolnames(govfiles) &lt;- make.names(colnames(govfiles), unique = TRUE)\nhead(govfiles)\n\n# Preparing for bulk download of government documents\ngovfiles$id &lt;- govfiles$packageId\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# saving files into govdata.assignent04 folder\nsave_dir &lt;- \"/Users/olivermyers/MyWebsite/govtdata.assignent04\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    # Ensure the file path includes a proper separator\n    destfile &lt;- file.path(save_dir, paste0(\"govfiles_\", id, \".pdf\"))\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to avoid server throttling\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n## Download the first 10 from the csv file\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\nresults &lt;- 1:10 %&gt;%  # Change to limit to the first 10 files\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\n# List and print all files in the directory\nall_files &lt;- list.files(path = save_dir, full.names = FALSE)  \nprint(\"Files in the govtdata.assignent04 directory:\")\nhead(all_files)\n\n\n[1] \"BILLS-118hr5770rh\"  \"BILLS-118hr5770rfs\" \"BILLS-118hr5770eh\" \n[4] \"BILLS-118hr8096ih\"  \"BILLS-118s4188is\"   \"BILLS-118hr7065ih\" \n\n\nTime difference of 24.63205 secs\n\n\n[1] \"Files in the govtdata.assignent04 directory:\"\n\n\n[1] \"govfiles_BILLS-118hr10150ih.pdf\" \"govfiles_BILLS-118hr3675rh.pdf\" \n[3] \"govfiles_BILLS-118hr5770eh.pdf\"  \"govfiles_BILLS-118hr5770rfs.pdf\"\n[5] \"govfiles_BILLS-118hr5770rh.pdf\"  \"govfiles_BILLS-118hr7021ih.pdf\" \n\n\n\n\nSimple report on difficulties encountered in the scraping process:\nScraping data using the first method, rvest, was initially a bit challenging for me. The need to inspect elements on the webpage and copy the XPath IDs to make the code work was a new concept. Additionally, some parts of the code were not as straightforward compared to the second method. That said, I found rvest to be significantly more useful in the long run because it allows for automated web scraping of large amounts of data from various webpage elements. Once I became familiar with the process, I appreciated the potential for efficiently formatting and organizing scraped data, even if it was tricky to set up at first.\nThe second method, on the other hand, was easier to use but felt less practical. This approach requires manually finding and downloading the necessary list yourself, which limits its automation capabilities. Initially, I encountered issues with downloading the files into the correct folder, but after consulting ChatGPT, I resolved the problem and successfully downloaded the files to the appropriate directory.\nIn conclusion, both methods have their advantages and can produce highly usable data. However, in my personal opinion, the rvest method stands out for its versatility and ability to scrape and format large-scale data efficiently. Although it requires more time and effort to understand and implement correctly, its potential for automating repetitive scraping tasks makes it the more valuable option overall. This could then be improved with more automation and cleaning steps build into the flow when using rvest."
  },
  {
    "objectID": "finalProject.html",
    "href": "finalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nUX Research and Design Portfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "index.html#epps-6302-data-analysis-and-visualization",
    "href": "index.html#epps-6302-data-analysis-and-visualization",
    "title": "Oliver Myers",
    "section": "",
    "text": "Here are my assignments and final project for the EPPS 6302 course:\n&lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"col\"&gt;\n      1 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      2 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      3 of 3\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "EPPS6302Final.html",
    "href": "EPPS6302Final.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "EPPS6302FinalProjects.html",
    "href": "EPPS6302FinalProjects.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "r"
  },
  {
    "objectID": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "href": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "title": "Assignment 2",
    "section": "",
    "text": "I analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\n\n\n\nJuly 14, 2024:\n\nTrump’s Peak: This date marks the first attempted assassination at a Trump rally, leading to a significant spike in search interest.\n\nJuly 21, 2024:\n\nHarris Begins to Trend: Following President Biden’s decision to drop out of the race, interest in Kamala Harris starts to increase.\n\nJuly 22, 2024:\n\nHarris Surpasses Trump: Harris peaks above Trump as she announces her candidacy for president.\n\nAugust 6, 2024:\n\nHarris’s Peak Over Trump: Harris reaches another peak after announcing Tim Walz as her running mate.\n\nAugust 23, 2024:\n\nAcceptance Speech: Harris experiences another spike in search interest during her acceptance speech at the DNC, where she becomes the Democratic front-runner for the 2024 presidential election.\n\nSeptember 11, 2024:\n\nSimultaneous Peaks: Both Trump and Harris see significant spikes as they attend the 9/11 Memorial event in New York City.\n\nSeptember 15, 2024:\n\nTrump’s Peak: A second attempted assassination at Trump’s international golf course results in another surge in interest for Trump.\n\nElection Momentum:\n\nAs the dates approach Election Day, search interest for all three terms—Trump, Harris, and Election—steadily increases."
  },
  {
    "objectID": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "href": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "title": "Assignment 2",
    "section": "Method 2: Using gtrendsR Package to collect data",
    "text": "Method 2: Using gtrendsR Package to collect data\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n\n\n\n\n\n\n\n\n\nDifferences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "Below are the resources for the EPPS 6302 Final Project"
  },
  {
    "objectID": "final_project.html#project-summary",
    "href": "final_project.html#project-summary",
    "title": "EPPS 6302 Final Project",
    "section": "Project Summary",
    "text": "Project Summary\nOur study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\nData Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram\n\n\n\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#data-collection-flow",
    "href": "final_project.html#data-collection-flow",
    "title": "EPPS 6302 Final Project",
    "section": "Data Collection Flow",
    "text": "Data Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "final_project.html#citations",
    "href": "final_project.html#citations",
    "title": "EPPS 6302 Final Project",
    "section": "Citations",
    "text": "Citations\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#contact-me",
    "href": "final_project.html#contact-me",
    "title": "EPPS 6302 Final Project",
    "section": "Contact Me",
    "text": "Contact Me\nIf you have any questions about this project, please feel free to contact me at:\nEmail: oliver.myers@utdallas.edu"
  },
  {
    "objectID": "final_project.html#links-to-external-resources",
    "href": "final_project.html#links-to-external-resources",
    "title": "EPPS 6302 Final Project",
    "section": "Links to External Resources",
    "text": "Links to External Resources\nBelow are the resources for the EPPS 6302 Final Project:\n\n\nFinal Project Paper \nFinal Presentation \nMovie Collection Code (R) \nAnalysis Code (Stata)"
  },
  {
    "objectID": "final_project.html#section",
    "href": "final_project.html#section",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "References\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "epps.6302.home.html",
    "href": "epps.6302.home.html",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6302.home.html#overview",
    "href": "epps.6302.home.html#overview",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6302.home.html#assignments",
    "href": "epps.6302.home.html#assignments",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs\n\n\nAssignment 5:Document Clustering\n\n\nFinal Project:Capstone Analysis"
  },
  {
    "objectID": "epps.6302.home.html#course-overview",
    "href": "epps.6302.home.html#course-overview",
    "title": "Knowledge Mining - EPPS6323",
    "section": "",
    "text": "Course Name: Knowledge Mining\nSemester: Spring 2025\nSchedule: Tuesdays, 4:00 PM - 6:45 PM\nLocation: ECSN 2.120\nInstructor: Karl Ho | Office: GR3.203 | Phone: 972-883-2017\nTeaching Assistant: Xingyuan Zhao | Office: GR 2.324\n\n\nThis course explores the interdisciplinary field of knowledge mining, integrating concepts from data mining, statistical learning, and machine learning. Topics include: - AI-assisted data analysis - Large Language Models (LLMs) - Generative AI - Text mining and visualization techniques - Ethical considerations in AI\n\n\n\nBy the end of the course, students will: 1. Understand the foundational principles of knowledge mining. 2. Apply advanced machine learning and AI techniques, including LLMs. 3. Develop practical skills in AI-driven tools for data analysis and visualization. 4. Evaluate ethical implications of AI applications. 5. Apply theoretical knowledge in workshops and case studies."
  },
  {
    "objectID": "epps.6302.assignment1.html",
    "href": "epps.6302.assignment1.html",
    "title": "OliverMyers.github.io",
    "section": "",
    "text": "Review\n# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\nWhat problems do you encounter when working with the dataset?\nThere is a few missing values that result in a NA from the dataset.\nHow to deal with missing values?\nFollowing the assignmnet and example online, to resolve this issue"
  },
  {
    "objectID": "epps.6302.home.html#grading-requirements",
    "href": "epps.6302.home.html#grading-requirements",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Participation: 10%\n\nAssignments (posted on website): 10%\n\nProject Proposal: 20% (Due February 25, 2025)\n\nProgress Reports: 30% (1st due March 25, 2025; 2nd due April 22, 2025)\n\nFinal Project & Presentation: 30% (Final due May 6, 2025)"
  },
  {
    "objectID": "epps.6302.home.html#course-topics",
    "href": "epps.6302.home.html#course-topics",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "This course covers an interdisciplinary approach to knowledge mining, integrating data science, AI, and machine learning with a focus on LLMs, generative AI, and text mining.\nKey topics include: - Data Science Foundations - Machine Learning for Knowledge Mining - Text Mining & NLP - Large Language Models (LLMs) & AI for Research - Causal & Predictive Modeling - Ethics in AI & Knowledge Mining\nFor more details, refer to the syllabus provided by Dr. Ho."
  },
  {
    "objectID": "epps.6302.home.html#assignments-1",
    "href": "epps.6302.home.html#assignments-1",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1: Data Preprocessing\n\n\nAssignment 2: Text Mining\n\n\nAssignment 3: Sentiment Analysis\n\n\nAssignment 4: Knowledge Graphs\n\n\nAssignment 5: Document Clustering\n\n\nFinal Project"
  },
  {
    "objectID": "epps.6302.home.html#assignments-2",
    "href": "epps.6302.home.html#assignments-2",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:\nData Preprocessing\n\n\nAssignment 2:\nText Mining\n\n\nAssignment 3:\nSentiment Analysis\n\n\nAssignment 4:\nKnowledge Graphs\n\n\nAssignment 5:\nDocument Clustering\n\n\nFinal Project:\nCapstone Analysis"
  },
  {
    "objectID": "epps.6302.home.html#assignments-3",
    "href": "epps.6302.home.html#assignments-3",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs\n\n\nAssignment 5:Document Clustering\n\n\nFinal Project:Capstone Analysis"
  }
]