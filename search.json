[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Welcome Friend!\nHello and welcome to my website! My name is Oliver Myers, and I am excited to share my journey in data science and UX research with you.\n\nLocation: Dallas, Texas\nRole: Mixed-Methods User Experience Researcher\nSchool: University of Texas at Dallas\nDegree: Master of Science in Applied Cognition and Neuroscience, focusing on Human-Computer Interaction\n\n\n\n\nUX Research Portfolio:\nIf you are looking for my UX Research Portfolio here is the link Portfolio: OliverJackMyers.com\n\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "About Me",
    "section": "",
    "text": "About Me\nHi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nPortfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 2"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 3"
  },
  {
    "objectID": "CopyOfassignment2.html",
    "href": "CopyOfassignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Method 1: Analyze Google Trends search term data for “Trump”, “Kamala Harris” and “Election”\n\ngoogle_trends_data &lt;- read.csv(\"~/Desktop/Trump.Harris.Google.TrendsData.Method1.csv\")\n\nMethod 2: Using gtrendsR Package to collect data\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\ninstall.packages(\"gtrendsR\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//Rtmpa9c09Q/downloaded_packages\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n#plot(HarrisTrumpElection_interest$hits, type = \"l\", main = \"Google Trends Data for Trump, Harris, and Election\",\n#     xlab = \"Time\", ylab = \"Search Interest\")\n\n\n## Install package\n#install.packages(\"gtrendsR\")\n\n#library(gtrendsR)\n\n#res &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"))\n#plot(res)\n\nDifferences between the two methods: In the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 5"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 4"
  },
  {
    "objectID": "finalProject.html",
    "href": "finalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nUX Research and Design Portfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "index.html#epps-6302-data-analysis-and-visualization",
    "href": "index.html#epps-6302-data-analysis-and-visualization",
    "title": "Oliver Myers",
    "section": "",
    "text": "Here are my assignments and final project for the EPPS 6302 course:\n&lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"col\"&gt;\n      1 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      2 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      3 of 3\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "EPPS6302Final.html",
    "href": "EPPS6302Final.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "EPPS6302FinalProjects.html",
    "href": "EPPS6302FinalProjects.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "r"
  },
  {
    "objectID": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "href": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "title": "Assignment 2",
    "section": "Method 1: Analyze Google Trends search term data for “Trump”, “Harris” and “Election”",
    "text": "Method 1: Analyze Google Trends search term data for “Trump”, “Harris” and “Election”\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\nKey Dates of Interest:\n\nJuly 14, 2024:\n\nTrump’s Peak: This date marks the first attempted assassination at a Trump rally, leading to a significant spike in search interest.\n\nJuly 21, 2024:\n\nHarris Begins to Trend: Following President Biden’s decision to drop out of the race, interest in Kamala Harris starts to increase.\n\nJuly 22, 2024:\n\nHarris Surpasses Trump: Harris peaks above Trump as she announces her candidacy for president.\n\nAugust 6, 2024:\n\nHarris’s Peak Over Trump: Harris reaches another peak after announcing Tim Walz as her running mate.\n\nAugust 23, 2024:\n\nAcceptance Speech: Harris experiences another spike in search interest during her acceptance speech at the DNC, where she becomes the Democratic front-runner for the 2024 presidential election.\n\nSeptember 11, 2024:\n\nSimultaneous Peaks: Both Trump and Harris see significant spikes as they attend the 9/11 Memorial event in New York City.\n\nSeptember 15, 2024:\n\nTrump’s Peak: A second attempted assassination at Trump’s international golf course results in another surge in interest for Trump.\n\nElection Momentum:\n\nAs the dates approach Election Day, search interest for all three terms—Trump, Harris, and Election—steadily increases."
  },
  {
    "objectID": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "href": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "title": "Assignment 2",
    "section": "Method 2: Using gtrendsR Package to collect data",
    "text": "Method 2: Using gtrendsR Package to collect data\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n\n\nDifferences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "Below are the resources for the EPPS 6302 Final Project"
  },
  {
    "objectID": "final_project.html#project-summary",
    "href": "final_project.html#project-summary",
    "title": "EPPS 6302 Final Project",
    "section": "Project Summary",
    "text": "Project Summary\nOur study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\nData Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram\n\n\n\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#data-collection-flow",
    "href": "final_project.html#data-collection-flow",
    "title": "EPPS 6302 Final Project",
    "section": "Data Collection Flow",
    "text": "Data Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "final_project.html#citations",
    "href": "final_project.html#citations",
    "title": "EPPS 6302 Final Project",
    "section": "Citations",
    "text": "Citations\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#contact-me",
    "href": "final_project.html#contact-me",
    "title": "EPPS 6302 Final Project",
    "section": "Contact Me",
    "text": "Contact Me\nIf you have any questions about this project, please feel free to contact me at:\nEmail: oliver.myers@utdallas.edu"
  },
  {
    "objectID": "final_project.html#links-to-external-resources",
    "href": "final_project.html#links-to-external-resources",
    "title": "EPPS 6302 Final Project",
    "section": "Links to External Resources",
    "text": "Links to External Resources\nBelow are the resources for the EPPS 6302 Final Project:\n\n\nFinal Project Paper \nFinal Presentation \nMovie Collection Code (R) \nAnalysis Code (Stata)"
  },
  {
    "objectID": "final_project.html#section",
    "href": "final_project.html#section",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "References\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "epps.6302.home.html",
    "href": "epps.6302.home.html",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research.\n\n\n\n\n\n\n\n\n\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing political discourse through text mining: Twitter trends, presidential speeches, and Wordfish modeling\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Foreign Reserve Data, U.S. Dollar Table, and Downloading Government Documents\n\n\n\n\n\n\n\n\n\n\n\n\nExploring YouTube Search Trends, Video Metadata, and Comment Analysis using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps.6302.home.html#overview",
    "href": "epps.6302.home.html#overview",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6302.home.html#assignments",
    "href": "epps.6302.home.html#assignments",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\n\n\n\n\nAssignment 2\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPPS 6302 Final Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignment 2:Data Preprocessing\n\n\nAssignment 3:Text Mining\n\n\nAssignment 4:Sentiment Analysis\n\n\nAssignment 5:Knowledge Graphs"
  },
  {
    "objectID": "epps.6302.home.html#course-overview",
    "href": "epps.6302.home.html#course-overview",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research.\n\n\n\n\n\n\n\n\n\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing political discourse through text mining: Twitter trends, presidential speeches, and Wordfish modeling\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Foreign Reserve Data, U.S. Dollar Table, and Downloading Government Documents\n\n\n\n\n\n\n\n\n\n\n\n\nExploring YouTube Search Trends, Video Metadata, and Comment Analysis using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps.6302.assignment1.html",
    "href": "epps.6302.assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\nWhat problems do you encounter when working with the dataset?\nThere is a few missing values that result in a NA from the dataset.\nHow to deal with missing values?\nFollowing the assignmnet and example online, to resolve this issue"
  },
  {
    "objectID": "epps.6302.home.html#grading-requirements",
    "href": "epps.6302.home.html#grading-requirements",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Participation: 10%\n\nAssignments (posted on website): 10%\n\nProject Proposal: 20% (Due February 25, 2025)\n\nProgress Reports: 30% (1st due March 25, 2025; 2nd due April 22, 2025)\n\nFinal Project & Presentation: 30% (Final due May 6, 2025)"
  },
  {
    "objectID": "epps.6302.home.html#course-topics",
    "href": "epps.6302.home.html#course-topics",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "This course covers an interdisciplinary approach to knowledge mining, integrating data science, AI, and machine learning with a focus on LLMs, generative AI, and text mining.\nKey topics include: - Data Science Foundations - Machine Learning for Knowledge Mining - Text Mining & NLP - Large Language Models (LLMs) & AI for Research - Causal & Predictive Modeling - Ethics in AI & Knowledge Mining\nFor more details, refer to the syllabus provided by Dr. Ho."
  },
  {
    "objectID": "epps.6302.home.html#assignments-1",
    "href": "epps.6302.home.html#assignments-1",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1: Data Preprocessing\n\n\nAssignment 2: Text Mining\n\n\nAssignment 3: Sentiment Analysis\n\n\nAssignment 4: Knowledge Graphs\n\n\nAssignment 5: Document Clustering\n\n\nFinal Project"
  },
  {
    "objectID": "epps.6302.home.html#assignments-2",
    "href": "epps.6302.home.html#assignments-2",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:\nData Preprocessing\n\n\nAssignment 2:\nText Mining\n\n\nAssignment 3:\nSentiment Analysis\n\n\nAssignment 4:\nKnowledge Graphs\n\n\nAssignment 5:\nDocument Clustering\n\n\nFinal Project:\nCapstone Analysis"
  },
  {
    "objectID": "epps.6302.home.html#assignments-3",
    "href": "epps.6302.home.html#assignments-3",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs\n\n\nAssignment 5:Document Clustering\n\n\nFinal Project:Capstone Analysis"
  },
  {
    "objectID": "particle-js/Particle.Test.html",
    "href": "particle-js/Particle.Test.html",
    "title": "Some particles",
    "section": "",
    "text": "This doc showcases how to use particle.js to get a nice header in your quarto document.\nlet’s dive in.\n\nWhat is particle.js\nIt’s a javascript library that draws stunning particles in a HTML document.\nYou can check it on github, and play with this little tool to find the configuration that is right for you.\n\n\nHow to use it in Quarto?\nIt is possible thanks to a “template partials”. It’s a quarto option that allows to replace the code of a section of the document.\nThe title-block partial can be used to customize the header, and inject some particles in it!\nFor more explanation, check the gallery of Quarto tips and tricks!"
  },
  {
    "objectID": "Particle.Test.html",
    "href": "Particle.Test.html",
    "title": "Some particles",
    "section": "",
    "text": "This doc showcases how to use particle.js to get a nice header in your quarto document.\nlet’s dive in.\n\nWhat is particle.js\nIt’s a javascript library that draws stunning particles in a HTML document.\nYou can check it on github, and play with this little tool to find the configuration that is right for you.\n\n\nHow to use it in Quarto?\nIt is possible thanks to a “template partials”. It’s a quarto option that allows to replace the code of a section of the document.\nThe title-block partial can be used to customize the header, and inject some particles in it!\nFor more explanation, check the gallery of Quarto tips and tricks!"
  },
  {
    "objectID": "particles.js-master/LICENSE.html",
    "href": "particles.js-master/LICENSE.html",
    "title": "Oliver Myers",
    "section": "",
    "text": "The MIT License (MIT)\nCopyright (c) 2015, Vincent Garreau\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "epps.6302.home.html#course-project",
    "href": "epps.6302.home.html#course-project",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "Course Project",
    "text": "Course Project\nThis project explores the relationship between Google Trends search interest and movie ratings on IMDb and Rotten Tomatoes. By analyzing the drop rate of search interest over the first 21 days post-release, we assess whether early online engagement correlates with audience reception. Using web scraping, API integration, and regression analysis in R, this study applies data collection, cleaning, and predictive modeling to uncover insights into digital engagement and consumer behavior.\n\n\nFinal Project Page:Page\n\n\nFinal Project Presentation:PDF\n\n\nProject Final Paper:PDF"
  },
  {
    "objectID": "epps.6354.home.html",
    "href": "epps.6354.home.html",
    "title": "Information Management (6354)",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "epps.6354.home.html#course-overview",
    "href": "epps.6354.home.html#course-overview",
    "title": "Information Management (6354)",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "epps.6354.home.html#assignments",
    "href": "epps.6354.home.html#assignments",
    "title": "Information Management (6354)",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:—-\n\n\nAssignment 2:—-\n\n\nAssignment 3:—-\n\n\nAssignment 4:—–"
  },
  {
    "objectID": "epps.6354.home.html#course-project",
    "href": "epps.6354.home.html#course-project",
    "title": "Information Management (6354)",
    "section": "Course Project",
    "text": "Course Project\nThis project focuses on designing a relational database and interactive dashboard for the Texas Public Safety Association (TPSA) to evaluate the effectiveness of scoring rubrics in competitive events. By integrating student scores, rubric details, event types, and conference data, the system will enable data-driven insights into rubric fairness and effectiveness over time. Using SQL, PostgreSQL, and a Shiny-based web dashboard, this project will provide TPSA staff with an intuitive tool to refine scoring criteria, ensuring fairer and more accurate assessments across events.\n\n\nProject Proposal Paper:PDF\n\n\nProject Proposal Slides:PDF\n\n\nMore to come after the completion of the final project"
  },
  {
    "objectID": "epps.6323.home.html",
    "href": "epps.6323.home.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6323.home.html#course-overview",
    "href": "epps.6323.home.html#course-overview",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6323.home.html#assignments",
    "href": "epps.6323.home.html#assignments",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs"
  },
  {
    "objectID": "epps.6323.home.html#course-project",
    "href": "epps.6323.home.html#course-project",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "Course Project",
    "text": "Course Project\nHere is my proposal for the final project in this course. I will be exploring the topic of “Forecasting User Sentiment in Mobile Apps: A Knowledge Mining Approach”. This project will leverage sentiment analysis, NLP, and predictive modeling to forecast user sentiment in mobile apps, helping developers improve user experience and app ratings.\n\n\nProject Proposal Paper:PDF\n\n\nProject Proposal Slides:PDF\n\n\nMore to come after the completion of the final project"
  },
  {
    "objectID": "assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "href": "assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "title": "Assignment 2",
    "section": "Google Trends Analysis of Political Search Interest (July–November 2024)",
    "text": "Google Trends Analysis of Political Search Interest (July–November 2024)\nThis assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "assignment2.html#method-1-google-trends-csv-data-analysis",
    "href": "assignment2.html#method-1-google-trends-csv-data-analysis",
    "title": "Assignment 2",
    "section": "Method 1: Google Trends CSV Data & Analysis",
    "text": "Method 1: Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#method-2-google-trends-using-gtrendsr-packageusing",
    "href": "assignment2.html#method-2-google-trends-using-gtrendsr-packageusing",
    "title": "Assignment 2",
    "section": "Method 2: Google Trends Using “gtrendsR” PackageUsing",
    "text": "Method 2: Google Trends Using “gtrendsR” PackageUsing\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion"
  },
  {
    "objectID": "assignment2.html#discussion",
    "href": "assignment2.html#discussion",
    "title": "Assignment 2",
    "section": "Discussion",
    "text": "Discussion\n\nDifferences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment2.html#method-1",
    "href": "assignment2.html#method-1",
    "title": "Assignment 2",
    "section": "Method 1:",
    "text": "Method 1:\n\nGoogle Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#method-2",
    "href": "assignment2.html#method-2",
    "title": "Assignment 2",
    "section": "Method 2:",
    "text": "Method 2:\n\nGoogle Trends Using “gtrendsR” PackageUsing\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion"
  },
  {
    "objectID": "assignment2.html#google-trends-csv-data-analysis",
    "href": "assignment2.html#google-trends-csv-data-analysis",
    "title": "Assignment 2",
    "section": "1. Google Trends CSV Data & Analysis",
    "text": "1. Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n\nLoading the Google Trends CSV Data\nThe dataset is read into R, and unnecessary rows are removed to clean the data.\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\n\n\nPlotting Google Search Trends Over Time\nThis plot visualizes the search interest over time for Trump, Harris, and Election, with key event dates highlighted.\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\nFigure 1: Google Trends Search Interest Over Time: Search interest for ‘Trump,’ ‘Harris,’ and ‘Election’ from July to November 2024, highlighting key political events influencing search spikes.\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#google-trends-using-gtrendsr-packageusing",
    "href": "assignment2.html#google-trends-using-gtrendsr-packageusing",
    "title": "Assignment 2",
    "section": "2. Google Trends Using “gtrendsR” PackageUsing",
    "text": "2. Google Trends Using “gtrendsR” PackageUsing\nn this method, the gtrendsR package in R was used to directly query Google Trends for real-time search interest data on “Trump,” “Harris,” and “Election.” Instead of manually downloading a CSV, this approach allows for automated data retrieval over a specified time range.\n\nFetching Data from Google Trends API\nThis code retrieves real-time Google search interest data directly from Google’s API.\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\nFigure 2: Google Trends Data Retrieved via API: Real-time search interest trends retrieved using the gtrendsR package, offering an automated alternative to manual CSV downloads.\n\n\n\n\nKey Advantages\n\nAutomated Data Collection → Eliminates the need for manual downloads.\nReal-Time Updates → Ensures the latest data can be pulled dynamically.\nReproducibility → Allows future analysis with updated data."
  },
  {
    "objectID": "assignment2.html#discussion-differences-between-the-two-methods",
    "href": "assignment2.html#discussion-differences-between-the-two-methods",
    "title": "Assignment 2",
    "section": "Discussion: Differences between the two methods:",
    "text": "Discussion: Differences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment3.html#analyzing-political-discourse-using-text-data",
    "href": "assignment3.html#analyzing-political-discourse-using-text-data",
    "title": "Assignment 3",
    "section": "Analyzing Political Discourse Using Text Data",
    "text": "Analyzing Political Discourse Using Text Data\nThis assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "assignment3.html#biden-xi-summit-twitter-analysis",
    "href": "assignment3.html#biden-xi-summit-twitter-analysis",
    "title": "Assignment 3",
    "section": "1. Biden-Xi Summit Twitter Analysis",
    "text": "1. Biden-Xi Summit Twitter Analysis\n\nLoading Twitter Data\nThe dataset consists of tweets discussing the Biden-Xi summit (November 2021). We load the dataset using readr and extract the tweet text.\n\n\nPreprocessing the Text Data\nWe tokenize the tweet text, remove punctuation, and create a document-feature matrix (DFM), which converts text into a structured numerical format. then to analyze discussion topics, we extract hashtags from tweets and identify the most frequently used ones. Finaly, to visualize the relationships between hashtags, we create a feature co-occurrence matrix (FCM) and plot a network graph.\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\nFigure 1: Hashtag Network: The network visualization highlights key discussion topics related to the summit. Central hashtags like #biden and #china dominate the conversation, while #humanrights and #uyghurs indicate concerns over human rights issues.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event."
  },
  {
    "objectID": "assignment3.html#u.s.-presidential-inaugural-speeches",
    "href": "assignment3.html#u.s.-presidential-inaugural-speeches",
    "title": "Assignment 3",
    "section": "2. U.S. Presidential Inaugural Speeches",
    "text": "2. U.S. Presidential Inaugural Speeches\nThis section examines U.S. presidential inaugural speeches over time, analyzing their linguistic trends and thematic focus.\n\nAnalyzing Early Inaugural Speeches & Keyword Trends Over Time\nWe create a document-feature matrix (DFM) for speeches from 1789 to 1826, removing common stopwords. We analyze post-1949 speeches and generate x-ray plots for key terms like liberty.\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\nFigure 2: Keyword Frequency of ‘Liberty’: This visualization highlights the usage pattern of ‘liberty’ in presidential speeches, showing how its prominence fluctuates over time.\n\n\n\n\n\n\nComparing Key Terms in Presidential Speeches\nWe generate x-ray plots for three key words: foreign, we, and god.\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\nFigure 3: Word Frequency Comparison: ‘Foreign’ was more common during Cold War-era speeches. ‘We’ is frequently used by presidents emphasizing unity (e.g., Obama, Biden). ‘God’ appears consistently toward the end of speeches, reflecting a tradition of invoking divine guidance.\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery."
  },
  {
    "objectID": "assignment3.html#wordfish-scaling-model",
    "href": "assignment3.html#wordfish-scaling-model",
    "title": "Assignment 3",
    "section": "3. Wordfish Scaling Model",
    "text": "3. Wordfish Scaling Model\nThe Wordfish model is an unsupervised text scaling method that estimates document positions based on word frequencies.\n\nApplying Wordfish to the 2010 Irish Budget Speeches\nWe use Wordfish to analyze Irish parliamentary speeches and estimate ideological positions.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\nFigure 4: Wordfish model with document positions: This plot visualizes word scaling based on frequency and distribution in Irish Budget 2010 speeches. Highlighted terms like government, economy, and deficit shape document positioning, while central clustering suggests shared vocabulary. Words at the extremes indicate stronger differentiation in political discourse.\n\n\n\n\n\n\nVisualizing Word Positions\nThis plot highlights the relative importance of words in different political contexts.\n\n# Load necessary libraries\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\nFigure 5: Estimated Document Positions by Political Party: This plot shows estimated ideological positions of politicians based on word usage. Fianna Fáil (FF) leans right, Sinn Féin (SF) and Labour (LAB) lean left, while Fine Gael (FG) and the Greens vary. Black dots represent individual positions, with confidence intervals highlighting linguistic and ideological differences.\n\n\n\n\n\n\nScaling Political Documents by Party\nWe visualize document positions grouped by political party.\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\nFigure 6: Estimated Document Positions by Political Party: This plot visualizes document positions using Correspondence Analysis (CA), grouping speeches by political party. It highlights linguistic differences across parties, mapping ideological tendencies based on word usage in Irish Budget 2010 speeches.\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772.\n\n\n\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties."
  },
  {
    "objectID": "assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "href": "assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "title": "Assignment 4",
    "section": "Automating Web Scraping for Economic and Government Data",
    "text": "Automating Web Scraping for Economic and Government Data\nThis assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website.\n\n\n1. Scraping Foreign Reserve Data\n\nReading the Wikipedia Page using the rvest package\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $638,261         2025-02-07 7,654  \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "assignment4.html#scraping-u.s.-dollar-table",
    "href": "assignment4.html#scraping-u.s.-dollar-table",
    "title": "Assignment 4",
    "section": "2. Scraping U.S. Dollar Table",
    "text": "2. Scraping U.S. Dollar Table\nThis section extracts a table from the Wikipedia page on U.S. currency, removing unnecessary image columns.\n\n\n\nFigure: Wikipedia Page on the U.S. Dollar – The Wikipedia entry for the United States dollar (USD), detailing its history, value, and international use. The right panel displays various denominations of U.S. banknotes.\n\n\n\nReading the U.S. Dollar Wikipedia Page\nThe script extracts the third table using XPath and removes unnecessary columns. This cleaned dataset helps track changes in U.S. banknotes over time.\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(rvest)\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/United_States_dollar'\n\n# Read the webpage\nusd_page &lt;- read_html(url)\n\n# Extract the third table on the page using XPath (skipping image columns 2 and 3)\nusd_table &lt;- usd_page %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[3]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nusd_data &lt;- usd_table[[1]]\n\n# Remove columns 2 and 3 (image columns)\nusd_data &lt;- usd_data %&gt;%\n  select(-`Front`, -`Reverse`)\n\n# Rename columns for clarity\ncolnames(usd_data) &lt;- c(\"Denomination\", \"Portrait\", \"Reverse_Motif\", \n                        \"First_Series\", \"Latest_Series\", \"Circulation\")\n\n# Clean up the data (if necessary)\nusd_data &lt;- usd_data %&gt;%\n  filter(!is.na(Denomination) & Denomination != \"\")  # Remove empty rows\n\n# View the cleaned and structured data\nhead(usd_data)\n\n# A tibble: 6 × 6\n  Denomination   Portrait   Reverse_Motif First_Series Latest_Series Circulation\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n1 One dollar     George Wa… Great Seal o… Series 1963… Series 2021[… Wide       \n2 Two dollars    Thomas Je… Declaration … Series 1976  Series 2017A  Limited[48]\n3 Five dollars   Abraham L… Lincoln Memo… Series 2006  Series 2021[… Wide       \n4 Ten dollars    Alexander… Treasury Bui… Series 2004A Series 2017A  Wide       \n5 Twenty dollars Andrew Ja… White House   Series 2004  Series 2017A  Wide       \n6 Fifty dollars  Ulysses S… United State… Series 2004  Series 2017A  Wide"
  },
  {
    "objectID": "assignment4.html#downloading-government-documents",
    "href": "assignment4.html#downloading-government-documents",
    "title": "Assignment 4",
    "section": "3. Downloading Government Documents",
    "text": "3. Downloading Government Documents\nThis section automates the bulk download of government bills related to water policy using “https://www.govinfo.gov/app/search/”.\n\n\n\nFigure: GovInfo Search Portal – The homepage of GovInfo, a U.S. government website for accessing official documents. Users can search for records using the search bar or browse by category, date, committee, or author.\n\n\n\nReading the Government Search Results\nThe script downloads 10 Congressional bills using a loop with error handling.\n\nlibrary(purrr)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\nsetwd(\"/Users/olivermyers/MyWebsite/govtdata.assignent04\")\n\n# csv downloaed from https://www.govinfo.gov/app/search/ and searching \"water\", filtering congressonal bills from 2024\n\n# Read the CSV file without skipping rows\ngovfiles &lt;- read.csv(file = \"/Users/olivermyers/MyWebsite/govinfo-search-results.csv\", skip = 2, header = FALSE)\n\ncolnames(govfiles) &lt;- govfiles[1, ]\ngovfiles &lt;- govfiles[-1, ]\nrownames(govfiles) &lt;- NULL\ncolnames(govfiles) &lt;- make.names(colnames(govfiles), unique = TRUE)\nhead(govfiles$packageId)\n\n[1] \"BILLS-118hr5770rh\"  \"BILLS-118hr5770rfs\" \"BILLS-118hr5770eh\" \n[4] \"BILLS-118hr8096ih\"  \"BILLS-118s4188is\"   \"BILLS-118hr7065ih\" \n\n# Preparing for bulk download of government documents\ngovfiles$id &lt;- govfiles$packageId\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# saving files into govdata.assignent04 folder\nsave_dir &lt;- \"/Users/olivermyers/MyWebsite/govtdata.assignent04\"\n\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    # Ensure the file path includes a proper separator\n    destfile &lt;- file.path(save_dir, paste0(\"govfiles_\", id, \".pdf\"))\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to avoid server throttling\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n\n## Download the first 10 from the csv file\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;%  # Change to limit to the first 10 files\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 25.64369 secs\n\n# List and print all files in the directory\nall_files &lt;- list.files(path = save_dir, full.names = FALSE)  \nprint(\"Files in the govtdata.assignent04 directory:\")\n\n[1] \"Files in the govtdata.assignent04 directory:\"\n\nhead(all_files)\n\n[1] \"govfiles_BILLS-118hr10150ih.pdf\" \"govfiles_BILLS-118hr3675rh.pdf\" \n[3] \"govfiles_BILLS-118hr5770eh.pdf\"  \"govfiles_BILLS-118hr5770rfs.pdf\"\n[5] \"govfiles_BILLS-118hr5770rh.pdf\"  \"govfiles_BILLS-118hr7021ih.pdf\""
  },
  {
    "objectID": "assignment4.html#discussion",
    "href": "assignment4.html#discussion",
    "title": "Assignment 4",
    "section": "Discussion:",
    "text": "Discussion:\n\nSimple report on difficulties encountered in the scraping process:\nScraping data using the first method, rvest, was initially a bit challenging for me. The need to inspect elements on the webpage and copy the XPath IDs to make the code work was a new concept. Additionally, some parts of the code were not as straightforward compared to the second method. That said, I found rvest to be significantly more useful in the long run because it allows for automated web scraping of large amounts of data from various webpage elements. Once I became familiar with the process, I appreciated the potential for efficiently formatting and organizing scraped data, even if it was tricky to set up at first.\nThe second method, on the other hand, was easier to use but felt less practical. This approach requires manually finding and downloading the necessary list yourself, which limits its automation capabilities. Initially, I encountered issues with downloading the files into the correct folder, but after consulting ChatGPT, I resolved the problem and successfully downloaded the files to the appropriate directory.\nIn conclusion, both methods have their advantages and can produce highly usable data. However, in my personal opinion, the rvest method stands out for its versatility and ability to scrape and format large-scale data efficiently. Although it requires more time and effort to understand and implement correctly, its potential for automating repetitive scraping tasks makes it the more valuable option overall. This could then be improved with more automation and cleaning steps build into the flow when using rvest."
  },
  {
    "objectID": "assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "href": "assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "title": "Assignment 5",
    "section": "Analyzing YouTube News Coverage on the 2024 U.S. Election",
    "text": "Analyzing YouTube News Coverage on the 2024 U.S. Election\nThis assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "assignment5.html#collecting-youtube-video-data",
    "href": "assignment5.html#collecting-youtube-video-data",
    "title": "Assignment 5",
    "section": "1. Collecting YouTube Video Data",
    "text": "1. Collecting YouTube Video Data\n\nRun YouTubenews01.R (prerequisites: YouTube developer API)\nThe code first authenticates the YouTube API using yt_oauth(). Then, it searches for videos related to “US election 2024” and extracts:\n\nlibrary(tuber)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(httr)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:httr':\n\n    content\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n# This one works today #\nyt_oauth(\"449459649595-3nqbg06e1ij0i2184ulc19q33lc08tvo.apps.googleusercontent.com\", \"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\", token = \"\")\n\n### Step 3: Download YouTube Data\n#### Here’s an example of collecting data on the “US election 2024.”\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\nAuto-refreshing stale OAuth token.\n\n#### Display the first few rows\nvideo_data &lt;- head(yt_uselection2024)\n\nsubset_video_data &lt;- video_data %&gt;%\n  select(video_id, channelId, title)\n\nprint(subset_video_data)\n\n     video_id                channelId\n1 uT9s4BXcv6w UCXIJgqnII2ZOINSWNOGFThA\n2 X4FtUJdKFCE UCeY0bbntWzzVIaj2z3QigXg\n3 9olb6OvXjKg UCupvZG-5ko_eiXAupbDfxWw\n4 BJkf7e_cZ68 UCH1oRy1dINbMVp3UFWrKP0w\n5 ORJI0-VSykQ UCeY0bbntWzzVIaj2z3QigXg\n6 0EPtWxTJMYc UCeY0bbntWzzVIaj2z3QigXg\n                                                                    title\n1        Donald Trump speaks after winning the 2024 Presidential Election\n2 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW\n3                     Trump wins 2024 presidential election, CNN projects\n4                     What polls say about the 2024 presidential election\n5               Possible paths to a win in the 2024 presidential election\n6 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW"
  },
  {
    "objectID": "assignment5.html#word-cloud-of-video-titles",
    "href": "assignment5.html#word-cloud-of-video-titles",
    "title": "Assignment 5",
    "section": "2. Word Cloud of Video Titles",
    "text": "2. Word Cloud of Video Titles\nThe most frequently used words in video titles are extracted and visualized.\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n\nFigure 2: Word Cloud of Video Titles – Most frequent words in YouTube video titles about the 2024 U.S. election."
  },
  {
    "objectID": "assignment5.html#analyzing-video-publication-dates",
    "href": "assignment5.html#analyzing-video-publication-dates",
    "title": "Assignment 5",
    "section": "3. Analyzing Video Publication Dates",
    "text": "3. Analyzing Video Publication Dates\nThe following code extracts publish dates and plots the frequency of videos published over time.\n\n### 4.2. Plot Video Publish Dates\n# Format publish dates and aggregate data\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n\nFigure 3: Video Publication Timeline – Distribution of videos published over time."
  },
  {
    "objectID": "assignment5.html#identifying-top-youtube-channels",
    "href": "assignment5.html#identifying-top-youtube-channels",
    "title": "Assignment 5",
    "section": "4. Identifying Top YouTube Channels",
    "text": "4. Identifying Top YouTube Channels\nThe top 10 channels with the highest number of videos related to the election are visualized.\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\nSelecting by n\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")"
  },
  {
    "objectID": "assignment5.html#analyzing-cnns-youtube-channel",
    "href": "assignment5.html#analyzing-cnns-youtube-channel",
    "title": "Assignment 5",
    "section": "5. Analyzing CNN’s YouTube Channel",
    "text": "5. Analyzing CNN’s YouTube Channel\nThe code extracts CNN’s channel statistics, including:\n\nTotal views\nSubscriber count\nTotal videos\n\n\n## Required Libraries\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(ggplot2)\n\n## CNN Channel ID\ncnn_channel_id &lt;- \"UCupvZG-5ko_eiXAupbDfxWw\"\n\n## get channel stats\ncnn_data &lt;- get_channel_stats(channel_id = \"UCupvZG-5ko_eiXAupbDfxWw\", mine = NULL)\n\nChannel Title: CNN \nNo. of Views: 17428751276 \nNo. of Subscribers: 17700000 \nNo. of Videos: 170170 \n\ncnn_stats = cnn_data$statistics\nhead(cnn_stats)\n\n$viewCount\n[1] \"17428751276\"\n\n$subscriberCount\n[1] \"17700000\"\n\n$hiddenSubscriberCount\n[1] FALSE\n\n$videoCount\n[1] \"170170\""
  },
  {
    "objectID": "assignment5.html#youtube-comments-sentiment-analysis",
    "href": "assignment5.html#youtube-comments-sentiment-analysis",
    "title": "Assignment 5",
    "section": "6. YouTube Comments Sentiment Analysis",
    "text": "6. YouTube Comments Sentiment Analysis\nThe code collects comments from a specific CNN video and processes the text for analysis.\n\nAnalyze the stats and comments:\n\nvideo_id =  \"Yzb5LGwt6LA\"\n\n## Get Video Statistics\nvideo_stats &lt;- get_stats(video_id)\ncat(\"Video Stats:\\n\")\n\nVideo Stats:\n\nhead(video_stats)\n\n$id\n[1] \"Yzb5LGwt6LA\"\n\n$viewCount\n[1] \"170169\"\n\n$likeCount\n[1] \"1797\"\n\n$favoriteCount\n[1] \"0\"\n\n$commentCount\n[1] \"695\"\n\n\n\nvideo_id =  \"Yzb5LGwt6LA\"\nvideocomments &lt;- get_all_comments(video_id)\nhead(videocomments$textOriginal)\n\n[1] \"Hollywood CNN! Welcome fake news!\"                                                                                                                                                                                   \n[2] \"6:28 damn this dude was completely  wrong\"                                                                                                                                                                           \n[3] \"Omg\"                                                                                                                                                                                                                 \n[4] \"Abandoned equipments - tanks,  APCs, airplanes ,etc - showed that Assad ‘s army is in complete disarray.\"                                                                                                            \n[5] \"May the innocent people be saved by God.\"                                                                                                                                                                            \n[6] \"The winner of the conflict is not the takfiris but Toyota, the favorite 'warhorse' of the Islamists which runs faster than the Russian tanks of Assad which enabled the Islamists to reach Aleppo in no time at all.\"\n\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n\nCan you use quanteda to analyze the text data from YouTube comments?\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"&gt;\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n      feature frequency rank docfreq group\n1         not        76    1      67   all\n2      rebels        76    1      70   all\n3       syria        67    3      55   all\n4          us        55    4      46   all\n5      people        53    5      32   all\n6          no        43    6      32   all\n7      russia        41    7      35   all\n8         cnn        40    8      31   all\n9         war        40    8      36   all\n10       like        36   10      29   all\n11        has        35   11      20   all\n12 terrorists        35   11      30   all\n13         we        34   13      28   all\n14        now        33   14      31   all\n15      there        32   15      27   all\n16       when        32   15      27   all\n17        who        31   17      29   all\n18        why        29   18      27   all\n19    because        27   19      22   all\n20       isis        27   19      25   all\n21       them        25   21      22   all\n22      putin        25   21      19   all\n23         he        25   21      17   all\n24       just        25   21      23   all\n25     spirit        25   21       2   all\n26      world        25   21      21   all\n27     syrian        24   27      18   all\n\n\n\n# Visualize the Most Frequent Words\ntstat_freq %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\", title = \"Top 15 Words in YouTube Comments\") +\n  theme_minimal()\n\n\n\n\nFigure 6: Most Frequent Words in YouTube Comments – A ranking of the 15 most common words used in YouTube comments on election-related videos."
  },
  {
    "objectID": "assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "href": "assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "title": "Assignment 5",
    "section": "7. Generating a Word Cloud of YouTube Comments",
    "text": "7. Generating a Word Cloud of YouTube Comments\n\n# Create a Word Cloud\nset.seed(132)\ntextplot_wordcloud(dfm_nonzero, max_words = 100)\n\n\n\n\nFigure 7: YouTube Comment Word Cloud – Visual representation of frequently used words in YouTube comments."
  },
  {
    "objectID": "assignment5.html#discussion",
    "href": "assignment5.html#discussion",
    "title": "Assignment 5",
    "section": "Discussion:",
    "text": "Discussion:\n\nAssignment Reflection:\nThis assignment required extensive effort to successfully retrieve and analyze YouTube comments from a CNN video. To achieve this, I leveraged the tuber package for data collection and quanteda for text analysis, with additional coding assistance from ChatGPT and reference to online documentation.\nThe comment processing involved tokenizing words, removing emojis, filtering out usernames, and eliminating common stopwords to ensure meaningful analysis. By visualizing the most frequently used words, I was able to identify dominant themes in the discussion.\nThe analysis revealed a predominantly negative sentiment, with frequent mentions of terms related to terrorism, conflict in the Middle East, and Syria. This suggests that the video’s content likely revolves around geopolitical tensions, aligning with CNN’s coverage focus. The findings highlight how YouTube comments can reflect public sentiment and engagement with political topics, making social media a powerful tool for understanding audience reactions to news coverage."
  },
  {
    "objectID": "nlp_assignment05.html",
    "href": "nlp_assignment05.html",
    "title": "nlp_assignment05",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "nlp_assignment05.html#old-model",
    "href": "nlp_assignment05.html#old-model",
    "title": "nlp_assignment05",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "href": "nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "title": "nlp_assignment05",
    "section": "New Model with improved predictive ability:",
    "text": "New Model with improved predictive ability:\n\n# Install and load required packages\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"doParallel\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\nif (length(new_packages)) install.packages(new_packages)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(ranger)\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Data ingestion & preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\nset.seed(123)\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Define Preprocessing Recipe with enhancements\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  # Normalize the text (using NFC form)\n  step_text_normalization(text, normalization_form = \"nfkc\") %&gt;%\n  # Tokenize text into words\n  step_tokenize(text, token = \"words\") %&gt;%\n  # Remove stopwords (default language is English)\n  step_stopwords(text) %&gt;%\n  # Create n-grams (unigrams and bigrams)\n  step_ngram(text, num_tokens = 2, min_num_tokens = 1) %&gt;%\n  # Filter tokens to limit the number of features\n  step_tokenfilter(text, max_tokens = 1000, min_times = 2) %&gt;%\n  # Create TF-IDF features\n  step_tfidf(text) %&gt;%\n  # Normalize predictors (if needed)\n  step_normalize(all_predictors())\n\n# Model Specification: Random Forest tuned on mtry and min_n (trees fixed at 1000)\nrf_spec &lt;- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n# Create workflow\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Set up 5-fold cross-validation with stratification\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# Define a grid for tuning mtry and min_n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Register parallel backend to speed up tuning\ndoParallel::registerDoParallel()\n\n# Tune model with cross-validation and evaluate using accuracy and kappa\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\nWarning: ! tune detected a parallel backend registered with foreach but no backend\n  registered with future.\nℹ Support for parallel processing with foreach was soft-deprecated in tune\n  1.2.1.\nℹ See ?parallelism (`?tune::parallelism()`) to learn more.\n\n# Review best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     2 Preprocessor1_Model01\n\n# Finalize workflow with the best hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% fit(data = train_data)\n\nWarning: max_tokens was set to 1000, but only 236 was available and selected.\n\n# Evaluate final model on the test set\nfinal_preds &lt;- final_fit %&gt;% \n  predict(new_data = test_data) %&gt;% \n  bind_cols(test_data)\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.883\n2 kap      multiclass     0.870\n\n# Display the confusion matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             0           0       0      0\n  Entertainment       0         0             6           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       6      0\n  Health              4         0             0           0       0      6\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          6      0\n  Travel               0      0          0      6\n\n# Predict on new samples and display the results\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples)\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Sports     \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Finance    \n\n\n\nHow I improved the predication ability:\nThe new model is better because it achieves an accuracy of about 88.3% and a kappa of around 0.87, which clearly shows it is making more reliable predictions and generalizes well to new data. We changed the preprocessing by normalizing the text to maintain consistency, incorporating n-grams to capture context beyond single words, and tuning the token filtering settings to retain a more representative vocabulary while cutting out noise. In addition, we refined the hyperparameter tuning process for the random forest by focusing on key parameters such as mtry and min_n. Together, these adjustments have resulted in a model that fits the data much more effectively and performs significantly better than the previous version.\n\nThe model before preformed with:\naccuracy multiclass 0.733\nkap multiclass 0.704\n\n\nWhere as now it preforms with:\naccuracy multiclass 0.883\nkap multiclass 0.870\nDisclaimer: I Used chat GPT 03-mini-high to help with optimization and to make better predictions*"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-csv-data-analysis",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-csv-data-analysis",
    "title": "",
    "section": "1. Google Trends CSV Data & Analysis",
    "text": "1. Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n\nLoading the Google Trends CSV Data\nThe dataset is read into R, and unnecessary rows are removed to clean the data.\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\n\n\nPlotting Google Search Trends Over Time\nThis plot visualizes the search interest over time for Trump, Harris, and Election, with key event dates highlighted.\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\nFigure 1: Google Trends Search Interest Over Time: Search interest for ‘Trump,’ ‘Harris,’ and ‘Election’ from July to November 2024, highlighting key political events influencing search spikes.\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-using-gtrendsr-packageusing",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-using-gtrendsr-packageusing",
    "title": "",
    "section": "2. Google Trends Using “gtrendsR” PackageUsing",
    "text": "2. Google Trends Using “gtrendsR” PackageUsing\nn this method, the gtrendsR package in R was used to directly query Google Trends for real-time search interest data on “Trump,” “Harris,” and “Election.” Instead of manually downloading a CSV, this approach allows for automated data retrieval over a specified time range.\n\nFetching Data from Google Trends API\nThis code retrieves real-time Google search interest data directly from Google’s API.\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\nFigure 2: Google Trends Data Retrieved via API: Real-time search interest trends retrieved using the gtrendsR package, offering an automated alternative to manual CSV downloads.\n\n\n\n\nKey Advantages\n\nAutomated Data Collection → Eliminates the need for manual downloads.\nReal-Time Updates → Ensures the latest data can be pulled dynamically.\nReproducibility → Allows future analysis with updated data."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#discussion-differences-between-the-two-methods",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#discussion-differences-between-the-two-methods",
    "title": "",
    "section": "Discussion: Differences between the two methods:",
    "text": "Discussion: Differences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html",
    "href": "pages/EPPS_6302/FinalProject/final_project.html",
    "title": "",
    "section": "",
    "text": "Our study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\n\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\n\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#project-summary",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#project-summary",
    "title": "",
    "section": "",
    "text": "Our study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\n\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\n\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#contact-me",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#contact-me",
    "title": "",
    "section": "Contact Me",
    "text": "Contact Me\nIf you have any questions about this project, please feel free to contact me at:\nEmail: oliver.myers@utdallas.edu"
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research."
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html#course-overview",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research."
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html#course-project",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nThis project explores the relationship between Google Trends search interest and movie ratings on IMDb and Rotten Tomatoes. By analyzing the drop rate of search interest over the first 21 days post-release, we assess whether early online engagement correlates with audience reception. Using web scraping, API integration, and regression analysis in R, this study applies data collection, cleaning, and predictive modeling to uncover insights into digital engagement and consumer behavior."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html",
    "title": "",
    "section": "",
    "text": "This assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "title": "",
    "section": "",
    "text": "This assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#collecting-youtube-video-data",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#collecting-youtube-video-data",
    "title": "",
    "section": "1. Collecting YouTube Video Data",
    "text": "1. Collecting YouTube Video Data\n\nRun YouTubenews01.R (prerequisites: YouTube developer API)\nThe code first authenticates the YouTube API using yt_oauth(). Then, it searches for videos related to “US election 2024” and extracts:\n\nlibrary(tuber)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(httr)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:httr':\n\n    content\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n# This one works today #\nyt_oauth(\"449459649595-3nqbg06e1ij0i2184ulc19q33lc08tvo.apps.googleusercontent.com\", \"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\", token = \"\")\n\n### Step 3: Download YouTube Data\n#### Here’s an example of collecting data on the “US election 2024.”\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\n#### Display the first few rows\nvideo_data &lt;- head(yt_uselection2024)\n\nsubset_video_data &lt;- video_data %&gt;%\n  select(video_id, channelId, title)\n\nprint(subset_video_data)\n\n     video_id                channelId\n1 A3AXszRgX7I UCO0akufu9MOzyz3nvGIXAAw\n2 X4FtUJdKFCE UCeY0bbntWzzVIaj2z3QigXg\n3 BJkf7e_cZ68 UCH1oRy1dINbMVp3UFWrKP0w\n4 uT9s4BXcv6w UCXIJgqnII2ZOINSWNOGFThA\n5 ORJI0-VSykQ UCeY0bbntWzzVIaj2z3QigXg\n6 9olb6OvXjKg UCupvZG-5ko_eiXAupbDfxWw\n                                                                    title\n1                                  Donald Trump wins the 2024 US election\n2 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW\n3                     What polls say about the 2024 presidential election\n4        Donald Trump speaks after winning the 2024 Presidential Election\n5               Possible paths to a win in the 2024 presidential election\n6                     Trump wins 2024 presidential election, CNN projects"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#word-cloud-of-video-titles",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#word-cloud-of-video-titles",
    "title": "",
    "section": "2. Word Cloud of Video Titles",
    "text": "2. Word Cloud of Video Titles\nThe most frequently used words in video titles are extracted and visualized.\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n\nFigure 2: Word Cloud of Video Titles – Most frequent words in YouTube video titles about the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-video-publication-dates",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-video-publication-dates",
    "title": "",
    "section": "3. Analyzing Video Publication Dates",
    "text": "3. Analyzing Video Publication Dates\nThe following code extracts publish dates and plots the frequency of videos published over time.\n\n### 4.2. Plot Video Publish Dates\n# Format publish dates and aggregate data\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n\nFigure 3: Video Publication Timeline – Distribution of videos published over time."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#identifying-top-youtube-channels",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#identifying-top-youtube-channels",
    "title": "",
    "section": "4. Identifying Top YouTube Channels",
    "text": "4. Identifying Top YouTube Channels\nThe top 10 channels with the highest number of videos related to the election are visualized.\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\nSelecting by n\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-cnns-youtube-channel",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-cnns-youtube-channel",
    "title": "",
    "section": "5. Analyzing CNN’s YouTube Channel",
    "text": "5. Analyzing CNN’s YouTube Channel\nThe code extracts CNN’s channel statistics, including:\n\nTotal views\nSubscriber count\nTotal videos\n\n\n## Required Libraries\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(ggplot2)\n\n## CNN Channel ID\ncnn_channel_id &lt;- \"UCupvZG-5ko_eiXAupbDfxWw\"\n\n## get channel stats\ncnn_data &lt;- get_channel_stats(channel_id = \"UCupvZG-5ko_eiXAupbDfxWw\", mine = NULL)\n\nChannel Title: CNN \nNo. of Views: 17776317625 \nNo. of Subscribers: 17900000 \nNo. of Videos: 170959 \n\ncnn_stats = cnn_data$statistics\nhead(cnn_stats)\n\n$viewCount\n[1] \"17776317625\"\n\n$subscriberCount\n[1] \"17900000\"\n\n$hiddenSubscriberCount\n[1] FALSE\n\n$videoCount\n[1] \"170959\""
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#youtube-comments-sentiment-analysis",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#youtube-comments-sentiment-analysis",
    "title": "",
    "section": "6. YouTube Comments Sentiment Analysis",
    "text": "6. YouTube Comments Sentiment Analysis\nThe code collects comments from a specific CNN video and processes the text for analysis.\n\nAnalyze the stats and comments:\n\nvideo_id =  \"Yzb5LGwt6LA\"\n\n## Get Video Statistics\nvideo_stats &lt;- get_stats(video_id)\ncat(\"Video Stats:\\n\")\n\nVideo Stats:\n\nhead(video_stats)\n\n$id\n[1] \"Yzb5LGwt6LA\"\n\n$viewCount\n[1] \"170244\"\n\n$likeCount\n[1] \"1791\"\n\n$favoriteCount\n[1] \"0\"\n\n$commentCount\n[1] \"694\"\n\n\n\nvideo_id =  \"Yzb5LGwt6LA\"\nvideocomments &lt;- get_all_comments(video_id)\nhead(videocomments$textOriginal)\n\n[1] \"Hollywood CNN! Welcome fake news!\"                                                                                                                                                                                   \n[2] \"6:28 damn this dude was completely  wrong\"                                                                                                                                                                           \n[3] \"Omg\"                                                                                                                                                                                                                 \n[4] \"Abandoned equipments - tanks,  APCs, airplanes ,etc - showed that Assad ‘s army is in complete disarray.\"                                                                                                            \n[5] \"May the innocent people be saved by God.\"                                                                                                                                                                            \n[6] \"The winner of the conflict is not the takfiris but Toyota, the favorite 'warhorse' of the Islamists which runs faster than the Russian tanks of Assad which enabled the Islamists to reach Aleppo in no time at all.\"\n\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n\nCan you use quanteda to analyze the text data from YouTube comments?\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"&gt;\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n      feature frequency rank docfreq group\n1      rebels        76    1      70   all\n2         not        75    2      66   all\n3       syria        66    3      54   all\n4          us        54    4      45   all\n5      people        53    5      32   all\n6          no        41    6      31   all\n7         cnn        40    7      31   all\n8         war        40    7      36   all\n9      russia        40    7      34   all\n10       like        36   10      29   all\n11 terrorists        35   11      30   all\n12        has        34   12      19   all\n13        now        33   13      31   all\n14         we        33   13      27   all\n15      there        32   15      27   all\n16       when        32   15      27   all\n17        who        31   17      29   all\n18        why        28   18      26   all\n19    because        27   19      22   all\n20       isis        27   19      25   all\n21       them        25   21      22   all\n22      putin        25   21      19   all\n23         he        25   21      17   all\n24       just        25   21      23   all\n25     spirit        25   21       2   all\n26      world        25   21      21   all\n27     syrian        24   27      18   all\n\n\n\n# Visualize the Most Frequent Words\ntstat_freq %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\", title = \"Top 15 Words in YouTube Comments\") +\n  theme_minimal()\n\n\n\n\nFigure 6: Most Frequent Words in YouTube Comments – A ranking of the 15 most common words used in YouTube comments on election-related videos."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "title": "",
    "section": "7. Generating a Word Cloud of YouTube Comments",
    "text": "7. Generating a Word Cloud of YouTube Comments\n\n# Create a Word Cloud\nset.seed(132)\ntextplot_wordcloud(dfm_nonzero, max_words = 100)\n\n\n\n\nFigure 7: YouTube Comment Word Cloud – Visual representation of frequently used words in YouTube comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#discussion",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#discussion",
    "title": "",
    "section": "Discussion:",
    "text": "Discussion:\n\nAssignment Reflection:\nThis assignment required extensive effort to successfully retrieve and analyze YouTube comments from a CNN video. To achieve this, I leveraged the tuber package for data collection and quanteda for text analysis, with additional coding assistance from ChatGPT and reference to online documentation.\nThe comment processing involved tokenizing words, removing emojis, filtering out usernames, and eliminating common stopwords to ensure meaningful analysis. By visualizing the most frequently used words, I was able to identify dominant themes in the discussion.\nThe analysis revealed a predominantly negative sentiment, with frequent mentions of terms related to terrorism, conflict in the Middle East, and Syria. This suggests that the video’s content likely revolves around geopolitical tensions, aligning with CNN’s coverage focus. The findings highlight how YouTube comments can reflect public sentiment and engagement with political topics, making social media a powerful tool for understanding audience reactions to news coverage."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#course-overview",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#assignments",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#assignments",
    "title": "",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!"
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#course-project",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nHere is my proposal for the final project in this course. I will be exploring the topic of “Forecasting User Sentiment in Mobile Apps: A Knowledge Mining Approach”. This project will leverage sentiment analysis, NLP, and predictive modeling to forecast user sentiment in mobile apps, helping developers improve user experience and app ratings.\nMore to come after the completion of the final project"
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#course-overview",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#assignments",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#assignments",
    "title": "",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!"
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#course-project",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nThis project focuses on designing a relational database and interactive dashboard for the Texas Public Safety Association (TPSA) to evaluate the effectiveness of scoring rubrics in competitive events. By integrating student scores, rubric details, event types, and conference data, the system will enable data-driven insights into rubric fairness and effectiveness over time. Using SQL, PostgreSQL, and a Shiny-based web dashboard, this project will provide TPSA staff with an intuitive tool to refine scoring criteria, ensuring fairer and more accurate assessments across events.\nMore to come after the completion of the final project"
  },
  {
    "objectID": "pages/EPPS_6354/Assignment01/epps.6354.assignment1.html",
    "href": "pages/EPPS_6354/Assignment01/epps.6354.assignment1.html",
    "title": "",
    "section": "",
    "text": "# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\nWhat problems do you encounter when working with the data set?\nThere is a few missing values that result in a NA from the data set.\n\n\nHow to deal with missing values?\nFollowing the assignment and example online, to resolve this issue"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html",
    "title": "",
    "section": "",
    "text": "This assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#analyzing-political-discourse-using-text-data",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#analyzing-political-discourse-using-text-data",
    "title": "",
    "section": "",
    "text": "This assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#biden-xi-summit-twitter-analysis",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#biden-xi-summit-twitter-analysis",
    "title": "",
    "section": "1. Biden-Xi Summit Twitter Analysis",
    "text": "1. Biden-Xi Summit Twitter Analysis\n\nLoading Twitter Data\nThe dataset consists of tweets discussing the Biden-Xi summit (November 2021). We load the dataset using readr and extract the tweet text.\n\n\nPreprocessing the Text Data\nWe tokenize the tweet text, remove punctuation, and create a document-feature matrix (DFM), which converts text into a structured numerical format. then to analyze discussion topics, we extract hashtags from tweets and identify the most frequently used ones. Finaly, to visualize the relationships between hashtags, we create a feature co-occurrence matrix (FCM) and plot a network graph.\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\nFigure 1: Hashtag Network: The network visualization highlights key discussion topics related to the summit. Central hashtags like #biden and #china dominate the conversation, while #humanrights and #uyghurs indicate concerns over human rights issues.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#u.s.-presidential-inaugural-speeches",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#u.s.-presidential-inaugural-speeches",
    "title": "",
    "section": "2. U.S. Presidential Inaugural Speeches",
    "text": "2. U.S. Presidential Inaugural Speeches\nThis section examines U.S. presidential inaugural speeches over time, analyzing their linguistic trends and thematic focus.\n\nAnalyzing Early Inaugural Speeches & Keyword Trends Over Time\nWe create a document-feature matrix (DFM) for speeches from 1789 to 1826, removing common stopwords. We analyze post-1949 speeches and generate x-ray plots for key terms like liberty.\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\nFigure 2: Keyword Frequency of ‘Liberty’: This visualization highlights the usage pattern of ‘liberty’ in presidential speeches, showing how its prominence fluctuates over time.\n\n\n\n\n\n\nComparing Key Terms in Presidential Speeches\nWe generate x-ray plots for three key words: foreign, we, and god.\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\nFigure 3: Word Frequency Comparison: ‘Foreign’ was more common during Cold War-era speeches. ‘We’ is frequently used by presidents emphasizing unity (e.g., Obama, Biden). ‘God’ appears consistently toward the end of speeches, reflecting a tradition of invoking divine guidance.\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#wordfish-scaling-model",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#wordfish-scaling-model",
    "title": "",
    "section": "3. Wordfish Scaling Model",
    "text": "3. Wordfish Scaling Model\nThe Wordfish model is an unsupervised text scaling method that estimates document positions based on word frequencies.\n\nApplying Wordfish to the 2010 Irish Budget Speeches\nWe use Wordfish to analyze Irish parliamentary speeches and estimate ideological positions.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\nFigure 4: Wordfish model with document positions: This plot visualizes word scaling based on frequency and distribution in Irish Budget 2010 speeches. Highlighted terms like government, economy, and deficit shape document positioning, while central clustering suggests shared vocabulary. Words at the extremes indicate stronger differentiation in political discourse.\n\n\n\n\n\n\nVisualizing Word Positions\nThis plot highlights the relative importance of words in different political contexts.\n\n# Load necessary libraries\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\nFigure 5: Estimated Document Positions by Political Party: This plot shows estimated ideological positions of politicians based on word usage. Fianna Fáil (FF) leans right, Sinn Féin (SF) and Labour (LAB) lean left, while Fine Gael (FG) and the Greens vary. Black dots represent individual positions, with confidence intervals highlighting linguistic and ideological differences.\n\n\n\n\n\n\nScaling Political Documents by Party\nWe visualize document positions grouped by political party.\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\nFigure 6: Estimated Document Positions by Political Party: This plot visualizes document positions using Correspondence Analysis (CA), grouping speeches by political party. It highlights linguistic differences across parties, mapping ideological tendencies based on word usage in Irish Budget 2010 speeches.\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772.\n\n\n\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html",
    "title": "",
    "section": "",
    "text": "This assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "title": "",
    "section": "",
    "text": "This assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website.\n\n\n\n\n\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $639,593         2025-03-21 30,165 \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-u.s.-dollar-table",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-u.s.-dollar-table",
    "title": "",
    "section": "2. Scraping U.S. Dollar Table",
    "text": "2. Scraping U.S. Dollar Table\nThis section extracts a table from the Wikipedia page on U.S. currency, removing unnecessary image columns.\n\n\n\nFigure: Wikipedia Page on the U.S. Dollar – The Wikipedia entry for the United States dollar (USD), detailing its history, value, and international use. The right panel displays various denominations of U.S. banknotes.\n\n\n\nReading the U.S. Dollar Wikipedia Page\nThe script extracts the third table using XPath and removes unnecessary columns. This cleaned dataset helps track changes in U.S. banknotes over time.\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(rvest)\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/United_States_dollar'\n\n# Read the webpage\nusd_page &lt;- read_html(url)\n\n# Extract the third table on the page using XPath (skipping image columns 2 and 3)\nusd_table &lt;- usd_page %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[3]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nusd_data &lt;- usd_table[[1]]\n\n# Remove columns 2 and 3 (image columns)\nusd_data &lt;- usd_data %&gt;%\n  select(-`Front`, -`Reverse`)\n\n# Rename columns for clarity\ncolnames(usd_data) &lt;- c(\"Denomination\", \"Portrait\", \"Reverse_Motif\", \n                        \"First_Series\", \"Latest_Series\", \"Circulation\")\n\n# Clean up the data (if necessary)\nusd_data &lt;- usd_data %&gt;%\n  filter(!is.na(Denomination) & Denomination != \"\")  # Remove empty rows\n\n# View the cleaned and structured data\nhead(usd_data)\n\n# A tibble: 6 × 6\n  Denomination   Portrait   Reverse_Motif First_Series Latest_Series Circulation\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n1 One dollar     George Wa… Great Seal o… Series 1963… Series 2021[… Wide       \n2 Two dollars    Thomas Je… Declaration … Series 1976  Series 2017A  Limited[48]\n3 Five dollars   Abraham L… Lincoln Memo… Series 2006  Series 2021[… Wide       \n4 Ten dollars    Alexander… Treasury Bui… Series 2004A Series 2017A  Wide       \n5 Twenty dollars Andrew Ja… White House   Series 2004  Series 2017A  Wide       \n6 Fifty dollars  Ulysses S… United State… Series 2004  Series 2017A  Wide"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#downloading-government-documents",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#downloading-government-documents",
    "title": "",
    "section": "3. Downloading Government Documents",
    "text": "3. Downloading Government Documents\nThis section automates the bulk download of government bills related to water policy using “https://www.govinfo.gov/app/search/”.\n\n\n\nFigure: GovInfo Search Portal – The homepage of GovInfo, a U.S. government website for accessing official documents. Users can search for records using the search bar or browse by category, date, committee, or author.\n\n\n\nReading the Government Search Results\nThe script downloads 10 Congressional bills using a loop with error handling.\n\nlibrary(purrr)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\nsetwd(\"/Users/olivermyers/MyWebsite/govtdata.assignent04\")\n\n# csv downloaed from https://www.govinfo.gov/app/search/ and searching \"water\", filtering congressonal bills from 2024\n\n# Read the CSV file without skipping rows\ngovfiles &lt;- read.csv(file = \"/Users/olivermyers/MyWebsite/govinfo-search-results.csv\", skip = 2, header = FALSE)\n\ncolnames(govfiles) &lt;- govfiles[1, ]\ngovfiles &lt;- govfiles[-1, ]\nrownames(govfiles) &lt;- NULL\ncolnames(govfiles) &lt;- make.names(colnames(govfiles), unique = TRUE)\nhead(govfiles$packageId)\n\n[1] \"BILLS-118hr5770rh\"  \"BILLS-118hr5770rfs\" \"BILLS-118hr5770eh\" \n[4] \"BILLS-118hr8096ih\"  \"BILLS-118s4188is\"   \"BILLS-118hr7065ih\" \n\n# Preparing for bulk download of government documents\ngovfiles$id &lt;- govfiles$packageId\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# saving files into govdata.assignent04 folder\nsave_dir &lt;- \"/Users/olivermyers/MyWebsite/govtdata.assignent04\"\n\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    # Ensure the file path includes a proper separator\n    destfile &lt;- file.path(save_dir, paste0(\"govfiles_\", id, \".pdf\"))\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to avoid server throttling\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n\n## Download the first 10 from the csv file\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;%  # Change to limit to the first 10 files\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 24.14704 secs\n\n# List and print all files in the directory\nall_files &lt;- list.files(path = save_dir, full.names = FALSE)  \nprint(\"Files in the govtdata.assignent04 directory:\")\n\n[1] \"Files in the govtdata.assignent04 directory:\"\n\nhead(all_files)\n\n[1] \"govfiles_BILLS-118hr10150ih.pdf\" \"govfiles_BILLS-118hr3675rh.pdf\" \n[3] \"govfiles_BILLS-118hr5770eh.pdf\"  \"govfiles_BILLS-118hr5770rfs.pdf\"\n[5] \"govfiles_BILLS-118hr5770rh.pdf\"  \"govfiles_BILLS-118hr7021ih.pdf\""
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#discussion",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#discussion",
    "title": "",
    "section": "Discussion:",
    "text": "Discussion:\n\nSimple report on difficulties encountered in the scraping process:\nScraping data using the first method, rvest, was initially a bit challenging for me. The need to inspect elements on the webpage and copy the XPath IDs to make the code work was a new concept. Additionally, some parts of the code were not as straightforward compared to the second method. That said, I found rvest to be significantly more useful in the long run because it allows for automated web scraping of large amounts of data from various webpage elements. Once I became familiar with the process, I appreciated the potential for efficiently formatting and organizing scraped data, even if it was tricky to set up at first.\nThe second method, on the other hand, was easier to use but felt less practical. This approach requires manually finding and downloading the necessary list yourself, which limits its automation capabilities. Initially, I encountered issues with downloading the files into the correct folder, but after consulting ChatGPT, I resolved the problem and successfully downloaded the files to the appropriate directory.\nIn conclusion, both methods have their advantages and can produce highly usable data. However, in my personal opinion, the rvest method stands out for its versatility and ability to scrape and format large-scale data efficiently. Although it requires more time and effort to understand and implement correctly, its potential for automating repetitive scraping tasks makes it the more valuable option overall. This could then be improved with more automation and cleaning steps build into the flow when using rvest."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html",
    "title": "",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#old-model",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#old-model",
    "title": "",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "title": "",
    "section": "New Model with improved predictive ability:",
    "text": "New Model with improved predictive ability:\n\n# Install and load required packages\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"doParallel\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\nif (length(new_packages)) install.packages(new_packages)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(ranger)\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Data ingestion & preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\nset.seed(123)\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Define Preprocessing Recipe with enhancements\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  # Normalize the text (using NFC form)\n  step_text_normalization(text, normalization_form = \"nfkc\") %&gt;%\n  # Tokenize text into words\n  step_tokenize(text, token = \"words\") %&gt;%\n  # Remove stopwords (default language is English)\n  step_stopwords(text) %&gt;%\n  # Create n-grams (unigrams and bigrams)\n  step_ngram(text, num_tokens = 2, min_num_tokens = 1) %&gt;%\n  # Filter tokens to limit the number of features\n  step_tokenfilter(text, max_tokens = 1000, min_times = 2) %&gt;%\n  # Create TF-IDF features\n  step_tfidf(text) %&gt;%\n  # Normalize predictors (if needed)\n  step_normalize(all_predictors())\n\n# Model Specification: Random Forest tuned on mtry and min_n (trees fixed at 1000)\nrf_spec &lt;- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n# Create workflow\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Set up 5-fold cross-validation with stratification\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# Define a grid for tuning mtry and min_n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Register parallel backend to speed up tuning\ndoParallel::registerDoParallel()\n\n# Tune model with cross-validation and evaluate using accuracy and kappa\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\nWarning: ! tune detected a parallel backend registered with foreach but no backend\n  registered with future.\nℹ Support for parallel processing with foreach was soft-deprecated in tune\n  1.2.1.\nℹ See ?parallelism (`?tune::parallelism()`) to learn more.\n\n# Review best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     2 Preprocessor1_Model01\n\n# Finalize workflow with the best hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% fit(data = train_data)\n\nWarning: max_tokens was set to 1000, but only 236 was available and selected.\n\n# Evaluate final model on the test set\nfinal_preds &lt;- final_fit %&gt;% \n  predict(new_data = test_data) %&gt;% \n  bind_cols(test_data)\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.883\n2 kap      multiclass     0.870\n\n# Display the confusion matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             0           0       0      0\n  Entertainment       0         0             6           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       6      0\n  Health              4         0             0           0       0      6\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          6      0\n  Travel               0      0          0      6\n\n# Predict on new samples and display the results\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples)\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Sports     \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Finance    \n\n\n\nHow I improved the predication ability:\nThe new model is better because it achieves an accuracy of about 88.3% and a kappa of around 0.87, which clearly shows it is making more reliable predictions and generalizes well to new data. We changed the preprocessing by normalizing the text to maintain consistency, incorporating n-grams to capture context beyond single words, and tuning the token filtering settings to retain a more representative vocabulary while cutting out noise. In addition, we refined the hyperparameter tuning process for the random forest by focusing on key parameters such as mtry and min_n. Together, these adjustments have resulted in a model that fits the data much more effectively and performs significantly better than the previous version.\n\nThe model before preformed with:\naccuracy multiclass 0.733\nkap multiclass 0.704\n\n\nWhere as now it preforms with:\naccuracy multiclass 0.883\nkap multiclass 0.870\nDisclaimer: I Used chat GPT 03-mini-high to help with optimization and to make better predictions*"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf",
    "title": "",
    "section": "Project Proposal (Pdf)",
    "text": "Project Proposal (Pdf)"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-foreign-reserve-data",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-foreign-reserve-data",
    "title": "",
    "section": "1. Scraping Foreign Reserve Data",
    "text": "1. Scraping Foreign Reserve Data\n\nReading the Wikipedia Page using the rvest package\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $639,593         2025-03-21 30,165 \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#final-project-presentation-slides",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#final-project-presentation-slides",
    "title": "",
    "section": "Final Project Presentation (Slides)",
    "text": "Final Project Presentation (Slides)\n\n\nHere is the code for the data collection, cleaning and analysis:\n\n\n Data Collection Code (R) \n\n\n Analysis Code (Stata)"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#final-project-paper-pdf",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#final-project-paper-pdf",
    "title": "",
    "section": "Final Project Paper (PDF)",
    "text": "Final Project Paper (PDF)\n\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-1",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-1",
    "title": "",
    "section": "Project Proposal (PDf)",
    "text": "Project Proposal (PDf)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#project-progress-report-slides",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#project-progress-report-slides",
    "title": "",
    "section": "Project Progress Report (Slides)",
    "text": "Project Progress Report (Slides)"
  },
  {
    "objectID": "pages/About/aboutme.html",
    "href": "pages/About/aboutme.html",
    "title": "",
    "section": "",
    "text": "Hi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nUX Research and Design Portfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html",
    "href": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html",
    "title": "",
    "section": "",
    "text": "Go to live Shiny app website »"
  },
  {
    "objectID": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html#live-preview-of-my-app",
    "href": "pages/EPPS_6354/Assignment07/epps.6354.assignment07.html#live-preview-of-my-app",
    "title": "",
    "section": "Live Preview of My App",
    "text": "Live Preview of My App\n\n\n\n\n\n Launch Live Shiny App » \n\nAbout this dashboard\nA dynamic instructor dashboard that connects to your local PostgreSQL server and lets you:\n- Filter by department or instructor\n- Sort salaries highest → lowest\n- Visualize trends over time with interactive bar charts\n\n\nPreview Outline\n\nHeader – Course selector + connection status\n\nData Table – Searchable list of instructors & salaries\n\nSalary Chart – Clickable bars drill down to details\n\nAlternate View – Swap in another variable (e.g. research funding)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-presentation-slides",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-presentation-slides",
    "title": "",
    "section": "Final Presentation (Slides)",
    "text": "Final Presentation (Slides)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-app",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-app",
    "title": "",
    "section": "Final Project (APP)",
    "text": "Final Project (APP)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-2",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-2",
    "title": "",
    "section": "Project Proposal (PDf)",
    "text": "Project Proposal (PDf)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-progress-report-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-progress-report-pdf",
    "title": "",
    "section": "Project Progress Report (Pdf)",
    "text": "Project Progress Report (Pdf)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-presentation-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-presentation-pdf",
    "title": "",
    "section": "Final Project Presentation (Pdf)",
    "text": "Final Project Presentation (Pdf)"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-app",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-app",
    "title": "",
    "section": "Final Project (APP)",
    "text": "Final Project (APP)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "library(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\n\n\nx &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9969523\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#create-object-using-the-assignment-operator--",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#using-function",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#using---operators",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#matrix-operations",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9969523\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#simple-descriptive-statistics-base",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "pages/EPPS_6323/Assignment01/Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpG49zLC/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#indexing-data-using",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#loading-data-from-github-remote",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#load-data-from-islr-website",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#linear-regression",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpG49zLC/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#multiple-linear-regression",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#qualitative-predictors",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment01/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "pages/EPPS_6323/Assignment01/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "library(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\n\n\nx &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9957205\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#create-object-using-the-assignment-operator--",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#using-function",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#using---operators",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"TEDS_2016\" \"x\"         \"y\"        \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"TEDS_2016\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#matrix-operations",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9957205\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#simple-descriptive-statistics-base",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "pages/EPPS_6323/Assignment02/Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpWyoN2S/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#indexing-data-using",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#loading-data-from-github-remote",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#load-data-from-islr-website",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#linear-regression",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//RtmpWyoN2S/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#multiple-linear-regression",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#qualitative-predictors",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "pages/EPPS_6323/Assignment02/Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html",
    "href": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html",
    "title": "",
    "section": "",
    "text": "(https://datageneration.io/dataprogramming/r-programming.html#illustration)\n\n# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\n\n# Check the variable\nattach(TEDS_2016)\nhead(PartyID)\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\ntail(PartyID)\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\n\n\n# Run a frequency table of the Party ID variable using the descr package\n## install.packages(\"descr\")\nlibrary(descr)\nfreq(TEDS_2016$PartyID)\n\n\n\n\n\n\n\n\nTEDS_2016$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n##install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nTEDS_2016 %&gt;% \n  count(PartyID) %&gt;% \n  mutate(perc = n / nrow(TEDS_2016)) -&gt; T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#running-exploratory-analysis",
    "href": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#running-exploratory-analysis",
    "title": "",
    "section": "",
    "text": "(https://datageneration.io/dataprogramming/r-programming.html#illustration)\n\n# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\n\n# Check the variable\nattach(TEDS_2016)\nhead(PartyID)\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\ntail(PartyID)\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\n\n\n# Run a frequency table of the Party ID variable using the descr package\n## install.packages(\"descr\")\nlibrary(descr)\nfreq(TEDS_2016$PartyID)\n\n\n\n\n\n\n\n\nTEDS_2016$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(PartyID)) + \n  geom_bar(aes(y = (..count..)/sum(..count..),fill=PartyID)) + \n  scale_y_continuous(labels=scales::percent) +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n##install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nTEDS_2016 %&gt;% \n  count(PartyID) %&gt;% \n  mutate(perc = n / nrow(TEDS_2016)) -&gt; T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#assignment-02-questions",
    "href": "pages/EPPS_6323/Assignment02/EPPS_6323_Assignment02.html#assignment-02-questions",
    "title": "",
    "section": "Assignment 02 Questions:",
    "text": "Assignment 02 Questions:\n\nWhat problems do you encounter when working with the dataset?\n\n# Display the first 5 rows and first 9 columns\nhead(TEDS_2016[, 1:9], 5)\n\n# A tibble: 5 × 9\n  District      Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n  &lt;dbl+lbl&gt;     &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n1 201 [Yi Lan … 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n2 201 [Yi Lan … 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n3 201 [Yi Lan … 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n4 201 [Yi Lan … 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n5 201 [Yi Lan … 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n\n\nOne of the main challenges with the TEDS_2016 dataset is that many variables are coded numerically without accompanying labels, making interpretation difficult. For instance, the “sex” variable contains values like 1 and 2, but there is no immediate indication of which value corresponds to which gender. Similarly, variables such as “education” and “age” appear as numerical codes that are unclear without a codebook or variable dictionary. This lack of descriptive metadata makes it hard to conduct meaningful analysis without first decoding the variables. In addition there are missing values in various cells in our database giving us “N/A”.\n\n\nHow to deal with missing values?\nOne approach to dealing with missing values is to remove rows that contain them, which helps maintain the integrity of the analysis by ensuring that only complete cases are used. \n\n\n(Next step) Explore the relationship between Tondu and other variables including female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\nTo explore these relationships, I used grouped bar charts and boxplots to examine how political and demographic factors relate to Tondu preferences (views on unification vs. independence). For categorical variables like gender, DPP support, and Taiwanese identity, I used grouped bar charts to show the distribution of Tondu preferences. For continuous variables like age and income, I used boxplots to illustrate how median values and spreads differ across Tondu groups. This approach allows for both a categorical comparison and a clear view of variation within numeric data.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# remove rows with n/a data\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  filter(!is.na(Tondu))\n\n# Prepare Education levels only (leave Tondu as-is)\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  mutate(\n    edu_level = factor(edu, levels = 1:5,\n                       labels = c(\"Primary\", \"Middle\", \"High\", \"College\", \"Postgrad\"))\n  )\n\n# 1. Gender\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(female))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#FF9999\", \"#6699CC\"), labels = c(\"Male\", \"Female\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Gender\") +\n  ggtitle(\"Tondu preference by Gender\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 2. DPP Support\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(DPP))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#FFD700\", \"#228B22\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"DPP Support\") +\n  ggtitle(\"Tondu preference by DPP Support\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 3. Taiwanese Identity\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Taiwanese))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#D95F02\", \"#1B9E77\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Taiwanese Identity\") +\n  ggtitle(\"Tondu preference by Taiwanese Identity\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 4. Economic Perception\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Econ_worse))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#A6CEE3\", \"#FB9A99\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Economy Worse?\") +\n  ggtitle(\"Tondu preference by Economic Perception\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 5. Education\nggplot(TEDS_2016, aes(x = Tondu, fill = edu_level)) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Education Level\") +\n  ggtitle(\"Tondu preference by Education\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 1. Tondu vs Age (Boxplot)\nggplot(TEDS_2016, aes(x = Tondu, y = age, fill = Tondu)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(x = \"Tondu Preference\", y = \"Age\") +\n  ggtitle(\"Age Distribution by Tondu Preference\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1),\n        legend.position = \"none\")\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n# 2. Tondu vs Income (Boxplot)\nggplot(TEDS_2016, aes(x = Tondu, y = income, fill = Tondu)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(x = \"Tondu Preference\", y = \"Income\") +\n  ggtitle(\"Income Distribution by Tondu Preference\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1),\n        legend.position = \"none\")\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\nThe following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n# 3. Tondu vs Taiwanese Identity (Stacked bar)\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Taiwanese))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#E41A1C\", \"#377EB8\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Taiwanese Identity\") +\n  ggtitle(\"Tondu Preference by Taiwanese Identity\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n# 4. Tondu vs Economic Perception\nggplot(TEDS_2016, aes(x = Tondu, fill = factor(Econ_worse))) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#A6CEE3\", \"#FB9A99\"), labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Economy Worse?\") +\n  ggtitle(\"Tondu Preference by Economic Perception\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n(Next step) How about the votetsai variable (vote for DPP candidate Tsai Ing-wen)?\nTo explore the relationship between support for Tsai Ing-wen and national identity preferences, I created a grouped bar chart comparing Tondu responses by voting behavior. This visualization shows how views on unification versus independence differ between those who voted for Tsai and those who did not in the 2016 election.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filter and prep\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  filter(!is.na(Tondu), !is.na(votetsai_all)) %&gt;%\n  mutate(\n    votetsai_label = factor(votetsai_all,\n                            levels = c(0, 1),\n                            labels = c(\"Did Not Vote Tsai\", \"Voted Tsai\"))\n  )\n\n# Plot: Grouped bar chart\nggplot(TEDS_2016, aes(x = Tondu, fill = votetsai_label)) +\n  geom_bar(position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"#E41A1C\", \"#377EB8\")) +\n  labs(x = \"Tondu Preference\", y = \"Frequency\", fill = \"Vote for Tsai?\") +\n  ggtitle(\"Tondu Preference by Vote for Tsai Ing-wen\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n(Next step) Generate frequency table and barchart of the Tondu variable. Assign labels to the variable using the following:\n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”, “Status quo, unif. in future”, “Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now”, “No respons\"))\n\n\nlibrary(descr)\n\nfreq(TEDS_2016$Tondu)\n\n\n\n\n\n\n\n\nTEDS_2016$Tondu \n      Frequency Percent\n1            23   1.595\n2           156  10.818\n3           459  31.831\n4           283  19.626\n5           327  22.677\n6            97   6.727\n9            97   6.727\nTotal      1442 100.000\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, labels = c(\n  \"Unification now\", \n  \"Status quo, unif. in future\", \n  \"Status quo, decide later\", \n  \"Status quo forever\", \n  \"Status quo, indep. in future\", \n  \"Independence now\", \n  \"No response\"\n))\n\nlibrary(ggplot2)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nggplot(TEDS_2016, aes(x = Tondu)) +\n  geom_bar(aes(y = (..count..) / sum(..count..), fill = Tondu)) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Views on Taiwan's Political Status (Tondu)\",\n    y = \"Percentage of Respondents\"\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\n    \"steelblue\", \"forestgreen\", \"khaki\", \"gray60\", \n    \"gold\", \"red\", \"darkgray\"\n  ))"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html",
    "href": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html",
    "title": "",
    "section": "",
    "text": "library(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\n\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n\nThe Biden–Xi summit data reveals distinct clusters of conversation around human rights and geopolitical tension. Hashtags like #uyghurs, #fentanyl, and #taiwan are tightly linked to #china, suggesting that topics like human rights abuses, drug trade, and territorial sovereignty were central to online discourse surrounding the summit."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#biden-xi-summit-data",
    "href": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#biden-xi-summit-data",
    "title": "",
    "section": "",
    "text": "library(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\n\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n\nThe Biden–Xi summit data reveals distinct clusters of conversation around human rights and geopolitical tension. Hashtags like #uyghurs, #fentanyl, and #taiwan are tightly linked to #china, suggesting that topics like human rights abuses, drug trade, and territorial sovereignty were central to online discourse surrounding the summit."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#us-presidential-inaugural-speeches",
    "href": "pages/EPPS_6323/Assignment04/EPPS_6323_Assignment04.html#us-presidential-inaugural-speeches",
    "title": "",
    "section": "US presidential inaugural speeches",
    "text": "US presidential inaugural speeches\n\n# Locate keywords-in-context\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"trade\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"american\"),\n  kwic(tokens_inaugural, pattern = \"people\"),\n  kwic(tokens_inaugural, pattern = \"trade\")\n)\n\n\n\n\n\n\n\n\n\nAny similarities and differences over time and among presidents?\nLooks like most presidents consistently used the word “people,” showing it’s a go-to way to connect with the public. But words like “trade” barely show up and only pop in during a few speeches, mostly from Reagan, Clinton, and Trumpm so it’s clearly not a top priority for everyone.\n\n\nWhat is Wordfish?\nWordfish is a text scaling algorithm used in political science and computational social science to estimate latent positions of actors based on the word frequencies in their texts. It assumes that word usage varies systematically with political position and applies a statistical model to uncover a one-dimensional scale from the textual data without needing predefined categories.\n\n# Create a dfm from the inaugural subset\ndfm_inaug &lt;- tokens(data_corpus_inaugural_subset, remove_punct = TRUE) %&gt;%\n  tokens_tolower() %&gt;%\n  tokens_remove(stopwords(\"en\")) %&gt;%\n  dfm()\n\n# Optional: trim sparse words to reduce noise\ndfm_inaug_trim &lt;- dfm_trim(dfm_inaug, min_termfreq = 5)\n\n# Run Wordfish scaling\nwordfish_model &lt;- textmodel_wordfish(dfm_inaug_trim, dir = c(1, nrow(dfm_inaug_trim)))\n\n# View estimated positions (theta scores)\nhead(wordfish_model$theta)\n\n[1] -2.439182610 -2.338764710 -0.902685412 -0.156921580 -0.002233262\n[6]  0.263172586\n\n# Plot the positions with ggplot2\nwf_df &lt;- data.frame(\n  President = docnames(dfm_inaug_trim),\n  Position = wordfish_model$theta\n)\n\nggplot(wf_df, aes(x = reorder(President, Position), y = Position)) +\n  geom_col(fill = \"#4F81BD\") +\n  coord_flip() +\n  xlab(\"President\") +\n  ylab(\"Estimated Position (Wordfish)\") +\n  ggtitle(\"Presidential Speech Positions via Wordfish\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHow to compare positions?\nTo compare positions, you can apply scaling methods like Wordfish or Wordshoal in quanteda, which place documents or speakers on an inferred ideological or policy spectrum. These methods work by analyzing differences in word usage across documents and estimating relative positions based on how discriminative certain words are, allowing you to visualize shifts in tone, ideology, or topic emphasis between individuals, parties, or time periods.\n\n\nCreate a corpus using government documents selected from the govinfo.gov website (usesample program govtdata01.R)\n\n# --- Part 0: Load Libraries & Setup ---\n# Make sure these are installed: install.packages(c(\"quanteda\", \"quanteda.textmodels\", \"quanteda.textstats\", \"quanteda.textplots\", \"readtext\", \"purrr\", \"magrittr\", \"jsonlite\", \"dplyr\", \"data.table\"))\n\n# For text analysis\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(readtext) # For reading PDFs\n\n\nAttaching package: 'readtext'\n\n\nThe following object is masked from 'package:quanteda':\n\n    texts\n\n# For data handling and downloading (from your script)\nlibrary(purrr)\nlibrary(magrittr) \n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n# --- Part 1: Configuration - USER NEEDS TO SET THESE ---\n\n# 1.1 Set your working directory (where your R script and downloaded_pdfs folder are)\n#     Replace \"your/path/to/EPPS6323_Assign4\" with your actual path\n#     Example for Windows: setwd(\"C:/Users/YourName/Documents/EPPS6323_Assign4\")\n#     Example for Mac/Linux: setwd(\"/Users/YourName/Documents/EPPS6323_Assign4\")\n# setwd(\"your/path/to/EPPS6323_Assign4\") # IMPORTANT: Uncomment and set this!\nif (getwd() == \"/\") { # A simple check if you're in the root, likely not intended\n  stop(\"Please set your working directory using setwd() before proceeding.\")\n}\nprint(paste(\"Current working directory:\", getwd()))\n\n[1] \"Current working directory: /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04\"\n\n# 1.2 Path to the JSON metadata file you downloaded from GovInfo\n#     OR the direct URL to the JSON if GovInfo provided one.\n#     If you downloaded the file, put it in your working directory.\n#     Example: json_metadata_file_path &lt;- \"govinfo-search-results-foreign-affairs.json\"\n#     Example: json_metadata_url &lt;- \"https_some_direct_url_from_govinfo.json\" # Less common for search results\njson_metadata_file_path &lt;- \"YOUR_JSON_METADATA_FILE.json\" # &lt;--- !!! REPLACE THIS !!! \n# For demonstration, I'll use the one from your script, but you should get your own for the specific search\n# For this example to run out-of-the-box for others, let's use the GitHub URL from your sample.\n# IN YOUR ACTUAL ASSIGNMENT, USE THE JSON FROM YOUR SPECIFIC GovInfo SEARCH.\njson_metadata_url &lt;- \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\nprint(paste(\"Using JSON metadata from:\", json_metadata_url))\n\n[1] \"Using JSON metadata from: https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n\n# 1.3 Directory to save the downloaded PDFs\n#     This should be a subfolder within your working directory.\nsave_dir &lt;- file.path(getwd(), \"downloaded_pdfs\")\nif (!dir.exists(save_dir)) {\n  dir.create(save_dir, recursive = TRUE)\n  print(paste(\"Created directory:\", save_dir))\n} else {\n  print(paste(\"Save directory already exists:\", save_dir))\n}\n\n[1] \"Save directory already exists: /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs\"\n\n# 1.4 Number of documents to download (keep it small for testing, e.g., 3-5)\nnum_docs_to_download &lt;- 3\n\n# --- Part 2: Load Metadata (Adapted from govtdata01.R) ---\nmessage(\"Loading JSON metadata...\")\n\nLoading JSON metadata...\n\n# Try reading from URL first, then from local file if path was given for local\nif (exists(\"json_metadata_url\") && startsWith(json_metadata_url, \"http\")) {\n  gf_list1 &lt;- jsonlite::read_json(json_metadata_url)\n} else if (exists(\"json_metadata_file_path\") && file.exists(json_metadata_file_path)) {\n  gf_list1 &lt;- jsonlite::read_json(json_metadata_file_path)\n} else {\n  stop(\"Please provide a valid json_metadata_url or json_metadata_file_path.\")\n}\n\n# Extract the list of documents\ngovfiles_df &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\nif (nrow(govfiles_df) == 0) {\n  stop(\"No documents found in the JSON metadata. Check your JSON file/URL or search query on GovInfo.\")\n}\nprint(paste(\"Loaded metadata for\", nrow(govfiles_df), \"documents.\"))\n\n[1] \"Loaded metadata for 1000 documents.\"\n\n# head(govfiles_df) #  (e.g., title, packageId, pdfLink, lastModified, dateIssued)\n\n# --- Part 3: Prepare for Bulk Download (Adapted from govtdata01.R) ---\n# Ensure the necessary columns exist\nif (!\"pdfLink\" %in% names(govfiles_df) || !\"packageId\" %in% names(govfiles_df)) {\n  stop(\"The JSON metadata does not contain 'pdfLink' or 'packageId'. Check the structure. Available columns: \", paste(names(govfiles_df), collapse=\", \"))\n}\n\npdf_govfiles_url &lt;- govfiles_df$pdfLink\n# Use packageId for a unique, meaningful ID. If not present, use row index.\nif (\"packageId\" %in% names(govfiles_df)) {\n  pdf_govfiles_id &lt;- govfiles_df$packageId \n} else {\n  # Fallback if packageId is missing, though it's usually there.\n  # Using row number as ID might lead to less descriptive filenames.\n  pdf_govfiles_id &lt;- 1:nrow(govfiles_df)\n  warning(\"packageId not found in metadata, using row index as ID.\")\n}\n\n\n# Trim to the number of documents we want to download\nif (nrow(govfiles_df) &lt; num_docs_to_download) {\n  warning(paste(\"Requested to download\", num_docs_to_download, \n                \"but only\", nrow(govfiles_df), \"are available. Downloading all available.\"))\n  num_docs_to_download &lt;- nrow(govfiles_df)\n}\n\npdf_govfiles_url_subset &lt;- head(pdf_govfiles_url, num_docs_to_download)\npdf_govfiles_id_subset &lt;- head(pdf_govfiles_id, num_docs_to_download)\ngovfiles_df_subset &lt;- head(govfiles_df, num_docs_to_download)\n\n\n# --- Part 4: Download PDFs (Adapted from govtdata01.R) ---\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id, save_directory) {\n  tryCatch({\n    # Sanitize id to make it a valid filename component\n    # Replace non-alphanumeric characters (except hyphen and underscore) with underscore\n    sanitized_id &lt;- gsub(\"[^a-zA-Z0-9_-]\", \"_\", id)\n    destfile &lt;- file.path(save_directory, paste0(\"govfile_\", sanitized_id, \".pdf\"))\n    \n    # Check if file already exists to avoid re-downloading (optional)\n    if (file.exists(destfile)) {\n      return(paste(\"File already exists (skipped):\", destfile))\n    }\n    \n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep\n    return(paste(\"Successfully downloaded:\", destfile))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download from:\", url, \"Error:\", e$message))\n  })\n}\n\nmessage(paste(\"Starting downloads of\", num_docs_to_download, \"PDFs to\", save_dir, \"...\"))\n\nStarting downloads of 3 PDFs to /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs ...\n\nstart.time &lt;- Sys.time()\n\n# Use mapply for iterating over two lists (url and id) simultaneously\n# purrr::map2_chr is also a good alternative\ndownload_results &lt;- mapply(download_govfiles_pdf, \n                           pdf_govfiles_url_subset, \n                           pdf_govfiles_id_subset,\n                           MoreArgs = list(save_directory = save_dir),\n                           SIMPLIFY = TRUE,\n                           USE.NAMES = FALSE)\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\nmessage(\"Finished downloads.\")\n\nFinished downloads.\n\nprint(time.taken)\n\nTime difference of 0.00540185 secs\n\nprint(download_results)\n\n[1] \"File already exists (skipped): /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs/govfile_BILLS-118sres890is.pdf\" \n[2] \"File already exists (skipped): /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs/govfile_BILLS-118sjres114is.pdf\"\n[3] \"File already exists (skipped): /Users/olivermyers/MyWebsite/pages/EPPS_6323/Assignment04/downloaded_pdfs/govfile_BILLS-118sres805ats.pdf\"\n\n# --- Part 5: Read Downloaded PDFs into R using readtext ---\nmessage(\"Reading downloaded PDFs into R...\")\n\nReading downloaded PDFs into R...\n\n# List all PDF files in the save_dir\npdf_files_to_read &lt;- list.files(path = save_dir, pattern = \"\\\\.pdf$\", full.names = TRUE)\n\nif (length(pdf_files_to_read) == 0) {\n  stop(\"No PDF files found in the download directory. Check download step.\")\n}\n\n# Read the text from PDF files\n# This can take some time, and PDF text extraction quality varies.\n# readtext uses pdftools::pdf_text internally for PDFs.\n# We will use the packageId as the doc_id for better linking with metadata\n# We need to extract the ID from the filename to match with our metadata\ndoc_ids_from_filenames &lt;- gsub(\"govfile_ Tuttavia_ Tuttavia_ Tuttavia_ Tuttavia_|.pdf\", \"\", basename(pdf_files_to_read))\n# doc_ids_from_filenames should now match pdf_govfiles_id_subset (after sanitization in download function)\n\n# To ensure correct doc_id assignment, let's build it more robustly\n# We know which IDs we attempted to download: pdf_govfiles_id_subset\n# We can construct the expected filenames\nexpected_filenames_base &lt;- paste0(\"govfile_\", gsub(\"[^a-zA-Z0-9_-]\", \"_\", pdf_govfiles_id_subset), \".pdf\")\nexpected_fullpaths &lt;- file.path(save_dir, expected_filenames_base)\n\n# Filter for only those files that were actually downloaded and exist\nactual_files_to_read &lt;- expected_fullpaths[file.exists(expected_fullpaths)]\nactual_ids_for_corpus &lt;- pdf_govfiles_id_subset[file.exists(expected_fullpaths)]\ngovfiles_df_for_corpus &lt;- govfiles_df_subset[file.exists(expected_fullpaths), ]\n\n\nif (length(actual_files_to_read) &gt; 0) {\n  gov_texts &lt;- readtext(actual_files_to_read, \n                        docvarsfrom = \"filenames\", # we'll add proper docvars later\n                        docnames = actual_ids_for_corpus) # Use our original IDs\n  print(paste(\"Successfully read\", nrow(gov_texts), \"PDF files.\"))\n  # print(head(gov_texts)) # Display first few characters of each document\n} else {\n  stop(\"No PDF files were successfully downloaded or found to read.\")\n}\n\n[1] \"Successfully read 3 PDF files.\"\n\n# --- Part 6: Create a quanteda Corpus ---\nmessage(\"Creating quanteda corpus...\")\n\nCreating quanteda corpus...\n\n# The 'text' column from readtext output contains the text\ngov_corpus &lt;- corpus(gov_texts) # text_field = \"text\" is default\n\n# Add document variables (docvars) from our metadata\n# Make sure the order of govfiles_df_for_corpus matches the order of documents in gov_corpus\n# Since we used actual_ids_for_corpus for docnames, we can match on that.\n# docnames(gov_corpus) should be the same as actual_ids_for_corpus\n\n# Select relevant columns from govfiles_df_for_corpus to be docvars\n# Common useful docvars: title, dateIssued, packageId (which is already docname)\n# Ensure packageId is a column in govfiles_df_for_corpus if you want to use it for merging\nif (!\"packageId\" %in% names(govfiles_df_for_corpus)) {\n  govfiles_df_for_corpus$packageId &lt;- actual_ids_for_corpus # Add it if missing\n}\n\n# Ensure govfiles_df_for_corpus docvars are aligned with corpus docnames\ndocvars_to_add &lt;- govfiles_df_for_corpus[match(docnames(gov_corpus), govfiles_df_for_corpus$packageId), ]\n\n# Add all columns from docvars_to_add as document variables\n# Exclude 'text' if it accidentally got in there, and packageId if we want to keep docnames unique\n# Also exclude pdfLink, etc. that might not be directly useful as docvars.\ncols_for_docvars &lt;- c(\"title\", \"granuleClass\", \"lastModified\", \"dateIssued\", \"branch\", \"suDocClassNumber\", \"governmentAuthor1\", \"pages\")\n# Keep only columns that actually exist in docvars_to_add\ncols_for_docvars &lt;- intersect(names(docvars_to_add), cols_for_docvars)\nif (length(cols_for_docvars) &gt; 0) {\n    docvars(gov_corpus) &lt;- cbind(docvars(gov_corpus), docvars_to_add[, cols_for_docvars, drop = FALSE])\n}\n\n\n# Display corpus summary\nprint(summary(gov_corpus, n = num_docs_to_download)) # Show summary for all downloaded docs\n\nCorpus consisting of 3 documents, showing 3 documents:\n\n                            Text Types Tokens Sentences docvar1\n govfile_BILLS-118sjres114is.pdf   291    625        12 govfile\n govfile_BILLS-118sres805ats.pdf   559   1631        16 govfile\n  govfile_BILLS-118sres890is.pdf   333    884        14 govfile\n             docvar2 title\n BILLS-118sjres114is  &lt;NA&gt;\n BILLS-118sres805ats  &lt;NA&gt;\n  BILLS-118sres890is  &lt;NA&gt;\n\nprint(paste(\"Corpus created with\", ndoc(gov_corpus), \"documents.\"))\n\n[1] \"Corpus created with 3 documents.\"\n\n# You can now proceed with further analysis using this 'gov_corpus' object\n# For example:\n# gov_dfm &lt;- dfm(tokens(gov_corpus, remove_punct = TRUE, remove_numbers = TRUE) %&gt;%\n#                tokens_remove(stopwords(\"en\")))\n# print(gov_dfm[, 1:10])\n\nmessage(\"Assignment 6 task (corpus creation) complete.\")\n\nAssignment 6 task (corpus creation) complete.\n\nmessage(\"The corpus object is named 'gov_corpus'.\")\n\nThe corpus object is named 'gov_corpus'."
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-code",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#final-project-code",
    "title": "",
    "section": "Final Project Code",
    "text": "Final Project Code"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-report-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#final-project-report-pdf",
    "title": "",
    "section": "Final Project Report (Pdf)",
    "text": "Final Project Report (Pdf)"
  }
]