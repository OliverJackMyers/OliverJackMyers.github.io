[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oliver Myers",
    "section": "",
    "text": "Oliver Myers Data Science in UTD EEPS.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nplot(iris, col='forestgreen')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "Personal",
    "section": "",
    "text": "I am a dog person. üêÄ"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Method 1: Analyze Google Trends search term data for ‚ÄúTrump‚Äù, ‚ÄúKamala Harris‚Äù and ‚ÄúElection‚Äù\n\ngoogle_trends_data &lt;- read.csv(\"~/Desktop/Trump.Harris.Google.TrendsData.Method1.csv\")\n\nMethod 2: Using gtrendsR Package to collect data\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\ninstall.packages(\"gtrendsR\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//Rtmp9PvEW4/downloaded_packages\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n#plot(HarrisTrumpElection_interest$hits, type = \"l\", main = \"Google Trends Data for Trump, Harris, and Election\",\n#     xlab = \"Time\", ylab = \"Search Interest\")\n\n\n## Install package\n#install.packages(\"gtrendsR\")\n\n#library(gtrendsR)\n\n#res &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"))\n#plot(res)\n\nDifferences between the two methods: In the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the¬†gtrendsR¬†package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Analyzing Biden-Xi summit data\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of ‚ÄúGod‚Äù toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses ‚Äúwe‚Äù to emphasize collective action and national unity, whereas Trump‚Äôs less frequent use of ‚Äúwe‚Äù suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on ‚Äúforeign‚Äù issues during the Cold War, while more recent speeches, including Biden‚Äôs, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn‚Äôt require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word‚Äôs power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. ‚ÄúA Scaling Model for Estimating Time-Series Party Positions from Texts.‚ÄùAmerican Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "CopyOfassignment2.html",
    "href": "CopyOfassignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Method 1: Analyze Google Trends search term data for ‚ÄúTrump‚Äù, ‚ÄúKamala Harris‚Äù and ‚ÄúElection‚Äù\n\ngoogle_trends_data &lt;- read.csv(\"~/Desktop/Trump.Harris.Google.TrendsData.Method1.csv\")\n\nMethod 2: Using gtrendsR Package to collect data\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\ninstall.packages(\"gtrendsR\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//Rtmpa9c09Q/downloaded_packages\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n#plot(HarrisTrumpElection_interest$hits, type = \"l\", main = \"Google Trends Data for Trump, Harris, and Election\",\n#     xlab = \"Time\", ylab = \"Search Interest\")\n\n\n## Install package\n#install.packages(\"gtrendsR\")\n\n#library(gtrendsR)\n\n#res &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"))\n#plot(res)\n\nDifferences between the two methods: In the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the¬†gtrendsR¬†package to retrieve and plot the data all in one place."
  }
]