[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Welcome Friend!\nHello and welcome to my website! My name is Oliver Myers, and I am excited to share my journey in data science and UX research with you.\n\nLocation: Dallas, Texas\nRole: Mixed-Methods User Experience Researcher\nSchool: University of Texas at Dallas\nDegree: Master of Science in Applied Cognition and Neuroscience, focusing on Human-Computer Interaction\n\n\n\n\nUX Research Portfolio:\nIf you are looking for my UX Research Portfolio here is the link Portfolio: OliverJackMyers.com\n\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\nOliver Myers is a User Experience Researcher and Designer - Current Applied Cognition and Neuroscience Masters Student at UTD."
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "About Me",
    "section": "",
    "text": "About Me\nHi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nPortfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 2"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 3"
  },
  {
    "objectID": "CopyOfassignment2.html",
    "href": "CopyOfassignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Method 1: Analyze Google Trends search term data for “Trump”, “Kamala Harris” and “Election”\n\ngoogle_trends_data &lt;- read.csv(\"~/Desktop/Trump.Harris.Google.TrendsData.Method1.csv\")\n\nMethod 2: Using gtrendsR Package to collect data\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\ninstall.packages(\"gtrendsR\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jr/lsx8jskd7hz338bmsv_5j43w0000gn/T//Rtmpa9c09Q/downloaded_packages\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n#plot(HarrisTrumpElection_interest$hits, type = \"l\", main = \"Google Trends Data for Trump, Harris, and Election\",\n#     xlab = \"Time\", ylab = \"Search Interest\")\n\n\n## Install package\n#install.packages(\"gtrendsR\")\n\n#library(gtrendsR)\n\n#res &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"))\n#plot(res)\n\nDifferences between the two methods: In the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 5"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "EPPS 6302 &gt; Assignment 4"
  },
  {
    "objectID": "finalProject.html",
    "href": "finalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nUX Research and Design Portfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  },
  {
    "objectID": "index.html#epps-6302-data-analysis-and-visualization",
    "href": "index.html#epps-6302-data-analysis-and-visualization",
    "title": "Oliver Myers",
    "section": "",
    "text": "Here are my assignments and final project for the EPPS 6302 course:\n&lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"col\"&gt;\n      1 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      2 of 3\n    &lt;/div&gt;\n    &lt;div class=\"col\"&gt;\n      3 of 3\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "EPPS6302Final.html",
    "href": "EPPS6302Final.html",
    "title": "Final Project",
    "section": "",
    "text": "Google Trends Data\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event.\nAnalyzing US presidential inaugural speeches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery.\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772."
  },
  {
    "objectID": "EPPS6302FinalProjects.html",
    "href": "EPPS6302FinalProjects.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "r"
  },
  {
    "objectID": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "href": "assignment2.html#method-1-analyze-google-trends-search-term-data-for-trump-harris-and-election",
    "title": "Assignment 2",
    "section": "Method 1: Analyze Google Trends search term data for “Trump”, “Harris” and “Election”",
    "text": "Method 1: Analyze Google Trends search term data for “Trump”, “Harris” and “Election”\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\nKey Dates of Interest:\n\nJuly 14, 2024:\n\nTrump’s Peak: This date marks the first attempted assassination at a Trump rally, leading to a significant spike in search interest.\n\nJuly 21, 2024:\n\nHarris Begins to Trend: Following President Biden’s decision to drop out of the race, interest in Kamala Harris starts to increase.\n\nJuly 22, 2024:\n\nHarris Surpasses Trump: Harris peaks above Trump as she announces her candidacy for president.\n\nAugust 6, 2024:\n\nHarris’s Peak Over Trump: Harris reaches another peak after announcing Tim Walz as her running mate.\n\nAugust 23, 2024:\n\nAcceptance Speech: Harris experiences another spike in search interest during her acceptance speech at the DNC, where she becomes the Democratic front-runner for the 2024 presidential election.\n\nSeptember 11, 2024:\n\nSimultaneous Peaks: Both Trump and Harris see significant spikes as they attend the 9/11 Memorial event in New York City.\n\nSeptember 15, 2024:\n\nTrump’s Peak: A second attempted assassination at Trump’s international golf course results in another surge in interest for Trump.\n\nElection Momentum:\n\nAs the dates approach Election Day, search interest for all three terms—Trump, Harris, and Election—steadily increases."
  },
  {
    "objectID": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "href": "assignment2.html#method-2-using-gtrendsr-package-to-collect-data",
    "title": "Assignment 2",
    "section": "Method 2: Using gtrendsR Package to collect data",
    "text": "Method 2: Using gtrendsR Package to collect data\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\n\n\n\n\n\nDifferences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "Below are the resources for the EPPS 6302 Final Project"
  },
  {
    "objectID": "final_project.html#project-summary",
    "href": "final_project.html#project-summary",
    "title": "EPPS 6302 Final Project",
    "section": "Project Summary",
    "text": "Project Summary\nOur study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\nData Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram\n\n\n\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#data-collection-flow",
    "href": "final_project.html#data-collection-flow",
    "title": "EPPS 6302 Final Project",
    "section": "Data Collection Flow",
    "text": "Data Collection Flow\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\nData Collection Flow Diagram\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "final_project.html#citations",
    "href": "final_project.html#citations",
    "title": "EPPS 6302 Final Project",
    "section": "Citations",
    "text": "Citations\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "final_project.html#contact-me",
    "href": "final_project.html#contact-me",
    "title": "EPPS 6302 Final Project",
    "section": "Contact Me",
    "text": "Contact Me\nIf you have any questions about this project, please feel free to contact me at:\nEmail: oliver.myers@utdallas.edu"
  },
  {
    "objectID": "final_project.html#links-to-external-resources",
    "href": "final_project.html#links-to-external-resources",
    "title": "EPPS 6302 Final Project",
    "section": "Links to External Resources",
    "text": "Links to External Resources\nBelow are the resources for the EPPS 6302 Final Project:\n\n\nFinal Project Paper \nFinal Presentation \nMovie Collection Code (R) \nAnalysis Code (Stata)"
  },
  {
    "objectID": "final_project.html#section",
    "href": "final_project.html#section",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "References\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "epps.6302.home.html",
    "href": "epps.6302.home.html",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research.\n\n\n\n\n\n\n\n\n\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing political discourse through text mining: Twitter trends, presidential speeches, and Wordfish modeling\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Foreign Reserve Data, U.S. Dollar Table, and Downloading Government Documents\n\n\n\n\n\n\n\n\n\n\n\n\nExploring YouTube Search Trends, Video Metadata, and Comment Analysis using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps.6302.home.html#overview",
    "href": "epps.6302.home.html#overview",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6302.home.html#assignments",
    "href": "epps.6302.home.html#assignments",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\n\n\n\n\nAssignment 2\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPPS 6302 Final Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignment 2:Data Preprocessing\n\n\nAssignment 3:Text Mining\n\n\nAssignment 4:Sentiment Analysis\n\n\nAssignment 5:Knowledge Graphs"
  },
  {
    "objectID": "epps.6302.home.html#course-overview",
    "href": "epps.6302.home.html#course-overview",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research.\n\n\n\n\n\n\n\n\n\n\n\nA data-driven exploration of search interest trends for key political figures and events using Google Trends and R.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing political discourse through text mining: Twitter trends, presidential speeches, and Wordfish modeling\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Foreign Reserve Data, U.S. Dollar Table, and Downloading Government Documents\n\n\n\n\n\n\n\n\n\n\n\n\nExploring YouTube Search Trends, Video Metadata, and Comment Analysis using R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps.6302.assignment1.html",
    "href": "epps.6302.assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\nWhat problems do you encounter when working with the dataset?\nThere is a few missing values that result in a NA from the dataset.\nHow to deal with missing values?\nFollowing the assignmnet and example online, to resolve this issue"
  },
  {
    "objectID": "epps.6302.home.html#grading-requirements",
    "href": "epps.6302.home.html#grading-requirements",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Participation: 10%\n\nAssignments (posted on website): 10%\n\nProject Proposal: 20% (Due February 25, 2025)\n\nProgress Reports: 30% (1st due March 25, 2025; 2nd due April 22, 2025)\n\nFinal Project & Presentation: 30% (Final due May 6, 2025)"
  },
  {
    "objectID": "epps.6302.home.html#course-topics",
    "href": "epps.6302.home.html#course-topics",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "This course covers an interdisciplinary approach to knowledge mining, integrating data science, AI, and machine learning with a focus on LLMs, generative AI, and text mining.\nKey topics include: - Data Science Foundations - Machine Learning for Knowledge Mining - Text Mining & NLP - Large Language Models (LLMs) & AI for Research - Causal & Predictive Modeling - Ethics in AI & Knowledge Mining\nFor more details, refer to the syllabus provided by Dr. Ho."
  },
  {
    "objectID": "epps.6302.home.html#assignments-1",
    "href": "epps.6302.home.html#assignments-1",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1: Data Preprocessing\n\n\nAssignment 2: Text Mining\n\n\nAssignment 3: Sentiment Analysis\n\n\nAssignment 4: Knowledge Graphs\n\n\nAssignment 5: Document Clustering\n\n\nFinal Project"
  },
  {
    "objectID": "epps.6302.home.html#assignments-2",
    "href": "epps.6302.home.html#assignments-2",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:\nData Preprocessing\n\n\nAssignment 2:\nText Mining\n\n\nAssignment 3:\nSentiment Analysis\n\n\nAssignment 4:\nKnowledge Graphs\n\n\nAssignment 5:\nDocument Clustering\n\n\nFinal Project:\nCapstone Analysis"
  },
  {
    "objectID": "epps.6302.home.html#assignments-3",
    "href": "epps.6302.home.html#assignments-3",
    "title": "Knowledge Mining Class EPPS 6323",
    "section": "",
    "text": "Here’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs\n\n\nAssignment 5:Document Clustering\n\n\nFinal Project:Capstone Analysis"
  },
  {
    "objectID": "particle-js/Particle.Test.html",
    "href": "particle-js/Particle.Test.html",
    "title": "Some particles",
    "section": "",
    "text": "This doc showcases how to use particle.js to get a nice header in your quarto document.\nlet’s dive in.\n\nWhat is particle.js\nIt’s a javascript library that draws stunning particles in a HTML document.\nYou can check it on github, and play with this little tool to find the configuration that is right for you.\n\n\nHow to use it in Quarto?\nIt is possible thanks to a “template partials”. It’s a quarto option that allows to replace the code of a section of the document.\nThe title-block partial can be used to customize the header, and inject some particles in it!\nFor more explanation, check the gallery of Quarto tips and tricks!"
  },
  {
    "objectID": "Particle.Test.html",
    "href": "Particle.Test.html",
    "title": "Some particles",
    "section": "",
    "text": "This doc showcases how to use particle.js to get a nice header in your quarto document.\nlet’s dive in.\n\nWhat is particle.js\nIt’s a javascript library that draws stunning particles in a HTML document.\nYou can check it on github, and play with this little tool to find the configuration that is right for you.\n\n\nHow to use it in Quarto?\nIt is possible thanks to a “template partials”. It’s a quarto option that allows to replace the code of a section of the document.\nThe title-block partial can be used to customize the header, and inject some particles in it!\nFor more explanation, check the gallery of Quarto tips and tricks!"
  },
  {
    "objectID": "particles.js-master/LICENSE.html",
    "href": "particles.js-master/LICENSE.html",
    "title": "Oliver Myers",
    "section": "",
    "text": "The MIT License (MIT)\nCopyright (c) 2015, Vincent Garreau\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "epps.6302.home.html#course-project",
    "href": "epps.6302.home.html#course-project",
    "title": "Methods of Data Collection and Production (EPPS 6302)",
    "section": "Course Project",
    "text": "Course Project\nThis project explores the relationship between Google Trends search interest and movie ratings on IMDb and Rotten Tomatoes. By analyzing the drop rate of search interest over the first 21 days post-release, we assess whether early online engagement correlates with audience reception. Using web scraping, API integration, and regression analysis in R, this study applies data collection, cleaning, and predictive modeling to uncover insights into digital engagement and consumer behavior.\n\n\nFinal Project Page:Page\n\n\nFinal Project Presentation:PDF\n\n\nProject Final Paper:PDF"
  },
  {
    "objectID": "epps.6354.home.html",
    "href": "epps.6354.home.html",
    "title": "Information Management (6354)",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "epps.6354.home.html#course-overview",
    "href": "epps.6354.home.html#course-overview",
    "title": "Information Management (6354)",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "epps.6354.home.html#assignments",
    "href": "epps.6354.home.html#assignments",
    "title": "Information Management (6354)",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:—-\n\n\nAssignment 2:—-\n\n\nAssignment 3:—-\n\n\nAssignment 4:—–"
  },
  {
    "objectID": "epps.6354.home.html#course-project",
    "href": "epps.6354.home.html#course-project",
    "title": "Information Management (6354)",
    "section": "Course Project",
    "text": "Course Project\nThis project focuses on designing a relational database and interactive dashboard for the Texas Public Safety Association (TPSA) to evaluate the effectiveness of scoring rubrics in competitive events. By integrating student scores, rubric details, event types, and conference data, the system will enable data-driven insights into rubric fairness and effectiveness over time. Using SQL, PostgreSQL, and a Shiny-based web dashboard, this project will provide TPSA staff with an intuitive tool to refine scoring criteria, ensuring fairer and more accurate assessments across events.\n\n\nProject Proposal Paper:PDF\n\n\nProject Proposal Slides:PDF\n\n\nMore to come after the completion of the final project"
  },
  {
    "objectID": "epps.6323.home.html",
    "href": "epps.6323.home.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6323.home.html#course-overview",
    "href": "epps.6323.home.html#course-overview",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "epps.6323.home.html#assignments",
    "href": "epps.6323.home.html#assignments",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!\n\n\nAssignment 1:Data Preprocessing\n\n\nAssignment 2:Text Mining\n\n\nAssignment 3:Sentiment Analysis\n\n\nAssignment 4:Knowledge Graphs"
  },
  {
    "objectID": "epps.6323.home.html#course-project",
    "href": "epps.6323.home.html#course-project",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "Course Project",
    "text": "Course Project\nHere is my proposal for the final project in this course. I will be exploring the topic of “Forecasting User Sentiment in Mobile Apps: A Knowledge Mining Approach”. This project will leverage sentiment analysis, NLP, and predictive modeling to forecast user sentiment in mobile apps, helping developers improve user experience and app ratings.\n\n\nProject Proposal Paper:PDF\n\n\nProject Proposal Slides:PDF\n\n\nMore to come after the completion of the final project"
  },
  {
    "objectID": "assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "href": "assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "title": "Assignment 2",
    "section": "Google Trends Analysis of Political Search Interest (July–November 2024)",
    "text": "Google Trends Analysis of Political Search Interest (July–November 2024)\nThis assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "assignment2.html#method-1-google-trends-csv-data-analysis",
    "href": "assignment2.html#method-1-google-trends-csv-data-analysis",
    "title": "Assignment 2",
    "section": "Method 1: Google Trends CSV Data & Analysis",
    "text": "Method 1: Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#method-2-google-trends-using-gtrendsr-packageusing",
    "href": "assignment2.html#method-2-google-trends-using-gtrendsr-packageusing",
    "title": "Assignment 2",
    "section": "Method 2: Google Trends Using “gtrendsR” PackageUsing",
    "text": "Method 2: Google Trends Using “gtrendsR” PackageUsing\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion"
  },
  {
    "objectID": "assignment2.html#discussion",
    "href": "assignment2.html#discussion",
    "title": "Assignment 2",
    "section": "Discussion",
    "text": "Discussion\n\nDifferences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment2.html#method-1",
    "href": "assignment2.html#method-1",
    "title": "Assignment 2",
    "section": "Method 1:",
    "text": "Method 1:\n\nGoogle Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\n\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#method-2",
    "href": "assignment2.html#method-2",
    "title": "Assignment 2",
    "section": "Method 2:",
    "text": "Method 2:\n\nGoogle Trends Using “gtrendsR” PackageUsing\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n# Set CRAN mirror\n#options(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Install package\n\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\n#par(family=\"Georgia\")\n\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion"
  },
  {
    "objectID": "assignment2.html#google-trends-csv-data-analysis",
    "href": "assignment2.html#google-trends-csv-data-analysis",
    "title": "Assignment 2",
    "section": "1. Google Trends CSV Data & Analysis",
    "text": "1. Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n\nLoading the Google Trends CSV Data\nThe dataset is read into R, and unnecessary rows are removed to clean the data.\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\n\n\nPlotting Google Search Trends Over Time\nThis plot visualizes the search interest over time for Trump, Harris, and Election, with key event dates highlighted.\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\nFigure 1: Google Trends Search Interest Over Time: Search interest for ‘Trump,’ ‘Harris,’ and ‘Election’ from July to November 2024, highlighting key political events influencing search spikes.\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "assignment2.html#google-trends-using-gtrendsr-packageusing",
    "href": "assignment2.html#google-trends-using-gtrendsr-packageusing",
    "title": "Assignment 2",
    "section": "2. Google Trends Using “gtrendsR” PackageUsing",
    "text": "2. Google Trends Using “gtrendsR” PackageUsing\nn this method, the gtrendsR package in R was used to directly query Google Trends for real-time search interest data on “Trump,” “Harris,” and “Election.” Instead of manually downloading a CSV, this approach allows for automated data retrieval over a specified time range.\n\nFetching Data from Google Trends API\nThis code retrieves real-time Google search interest data directly from Google’s API.\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\nFigure 2: Google Trends Data Retrieved via API: Real-time search interest trends retrieved using the gtrendsR package, offering an automated alternative to manual CSV downloads.\n\n\n\n\nKey Advantages\n\nAutomated Data Collection → Eliminates the need for manual downloads.\nReal-Time Updates → Ensures the latest data can be pulled dynamically.\nReproducibility → Allows future analysis with updated data."
  },
  {
    "objectID": "assignment2.html#discussion-differences-between-the-two-methods",
    "href": "assignment2.html#discussion-differences-between-the-two-methods",
    "title": "Assignment 2",
    "section": "Discussion: Differences between the two methods:",
    "text": "Discussion: Differences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "assignment3.html#analyzing-political-discourse-using-text-data",
    "href": "assignment3.html#analyzing-political-discourse-using-text-data",
    "title": "Assignment 3",
    "section": "Analyzing Political Discourse Using Text Data",
    "text": "Analyzing Political Discourse Using Text Data\nThis assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "assignment3.html#biden-xi-summit-twitter-analysis",
    "href": "assignment3.html#biden-xi-summit-twitter-analysis",
    "title": "Assignment 3",
    "section": "1. Biden-Xi Summit Twitter Analysis",
    "text": "1. Biden-Xi Summit Twitter Analysis\n\nLoading Twitter Data\nThe dataset consists of tweets discussing the Biden-Xi summit (November 2021). We load the dataset using readr and extract the tweet text.\n\n\nPreprocessing the Text Data\nWe tokenize the tweet text, remove punctuation, and create a document-feature matrix (DFM), which converts text into a structured numerical format. then to analyze discussion topics, we extract hashtags from tweets and identify the most frequently used ones. Finaly, to visualize the relationships between hashtags, we create a feature co-occurrence matrix (FCM) and plot a network graph.\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\nFigure 1: Hashtag Network: The network visualization highlights key discussion topics related to the summit. Central hashtags like #biden and #china dominate the conversation, while #humanrights and #uyghurs indicate concerns over human rights issues.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event."
  },
  {
    "objectID": "assignment3.html#u.s.-presidential-inaugural-speeches",
    "href": "assignment3.html#u.s.-presidential-inaugural-speeches",
    "title": "Assignment 3",
    "section": "2. U.S. Presidential Inaugural Speeches",
    "text": "2. U.S. Presidential Inaugural Speeches\nThis section examines U.S. presidential inaugural speeches over time, analyzing their linguistic trends and thematic focus.\n\nAnalyzing Early Inaugural Speeches & Keyword Trends Over Time\nWe create a document-feature matrix (DFM) for speeches from 1789 to 1826, removing common stopwords. We analyze post-1949 speeches and generate x-ray plots for key terms like liberty.\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\nFigure 2: Keyword Frequency of ‘Liberty’: This visualization highlights the usage pattern of ‘liberty’ in presidential speeches, showing how its prominence fluctuates over time.\n\n\n\n\n\n\nComparing Key Terms in Presidential Speeches\nWe generate x-ray plots for three key words: foreign, we, and god.\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\nFigure 3: Word Frequency Comparison: ‘Foreign’ was more common during Cold War-era speeches. ‘We’ is frequently used by presidents emphasizing unity (e.g., Obama, Biden). ‘God’ appears consistently toward the end of speeches, reflecting a tradition of invoking divine guidance.\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery."
  },
  {
    "objectID": "assignment3.html#wordfish-scaling-model",
    "href": "assignment3.html#wordfish-scaling-model",
    "title": "Assignment 3",
    "section": "3. Wordfish Scaling Model",
    "text": "3. Wordfish Scaling Model\nThe Wordfish model is an unsupervised text scaling method that estimates document positions based on word frequencies.\n\nApplying Wordfish to the 2010 Irish Budget Speeches\nWe use Wordfish to analyze Irish parliamentary speeches and estimate ideological positions.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\nFigure 4: Wordfish model with document positions: This plot visualizes word scaling based on frequency and distribution in Irish Budget 2010 speeches. Highlighted terms like government, economy, and deficit shape document positioning, while central clustering suggests shared vocabulary. Words at the extremes indicate stronger differentiation in political discourse.\n\n\n\n\n\n\nVisualizing Word Positions\nThis plot highlights the relative importance of words in different political contexts.\n\n# Load necessary libraries\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\nFigure 5: Estimated Document Positions by Political Party: This plot shows estimated ideological positions of politicians based on word usage. Fianna Fáil (FF) leans right, Sinn Féin (SF) and Labour (LAB) lean left, while Fine Gael (FG) and the Greens vary. Black dots represent individual positions, with confidence intervals highlighting linguistic and ideological differences.\n\n\n\n\n\n\nScaling Political Documents by Party\nWe visualize document positions grouped by political party.\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\nFigure 6: Estimated Document Positions by Political Party: This plot visualizes document positions using Correspondence Analysis (CA), grouping speeches by political party. It highlights linguistic differences across parties, mapping ideological tendencies based on word usage in Irish Budget 2010 speeches.\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772.\n\n\n\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties."
  },
  {
    "objectID": "assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "href": "assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "title": "Assignment 4",
    "section": "Automating Web Scraping for Economic and Government Data",
    "text": "Automating Web Scraping for Economic and Government Data\nThis assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website.\n\n\n1. Scraping Foreign Reserve Data\n\nReading the Wikipedia Page using the rvest package\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $638,261         2025-02-07 7,654  \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "assignment4.html#scraping-u.s.-dollar-table",
    "href": "assignment4.html#scraping-u.s.-dollar-table",
    "title": "Assignment 4",
    "section": "2. Scraping U.S. Dollar Table",
    "text": "2. Scraping U.S. Dollar Table\nThis section extracts a table from the Wikipedia page on U.S. currency, removing unnecessary image columns.\n\n\n\nFigure: Wikipedia Page on the U.S. Dollar – The Wikipedia entry for the United States dollar (USD), detailing its history, value, and international use. The right panel displays various denominations of U.S. banknotes.\n\n\n\nReading the U.S. Dollar Wikipedia Page\nThe script extracts the third table using XPath and removes unnecessary columns. This cleaned dataset helps track changes in U.S. banknotes over time.\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(rvest)\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/United_States_dollar'\n\n# Read the webpage\nusd_page &lt;- read_html(url)\n\n# Extract the third table on the page using XPath (skipping image columns 2 and 3)\nusd_table &lt;- usd_page %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[3]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nusd_data &lt;- usd_table[[1]]\n\n# Remove columns 2 and 3 (image columns)\nusd_data &lt;- usd_data %&gt;%\n  select(-`Front`, -`Reverse`)\n\n# Rename columns for clarity\ncolnames(usd_data) &lt;- c(\"Denomination\", \"Portrait\", \"Reverse_Motif\", \n                        \"First_Series\", \"Latest_Series\", \"Circulation\")\n\n# Clean up the data (if necessary)\nusd_data &lt;- usd_data %&gt;%\n  filter(!is.na(Denomination) & Denomination != \"\")  # Remove empty rows\n\n# View the cleaned and structured data\nhead(usd_data)\n\n# A tibble: 6 × 6\n  Denomination   Portrait   Reverse_Motif First_Series Latest_Series Circulation\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n1 One dollar     George Wa… Great Seal o… Series 1963… Series 2021[… Wide       \n2 Two dollars    Thomas Je… Declaration … Series 1976  Series 2017A  Limited[48]\n3 Five dollars   Abraham L… Lincoln Memo… Series 2006  Series 2021[… Wide       \n4 Ten dollars    Alexander… Treasury Bui… Series 2004A Series 2017A  Wide       \n5 Twenty dollars Andrew Ja… White House   Series 2004  Series 2017A  Wide       \n6 Fifty dollars  Ulysses S… United State… Series 2004  Series 2017A  Wide"
  },
  {
    "objectID": "assignment4.html#downloading-government-documents",
    "href": "assignment4.html#downloading-government-documents",
    "title": "Assignment 4",
    "section": "3. Downloading Government Documents",
    "text": "3. Downloading Government Documents\nThis section automates the bulk download of government bills related to water policy using “https://www.govinfo.gov/app/search/”.\n\n\n\nFigure: GovInfo Search Portal – The homepage of GovInfo, a U.S. government website for accessing official documents. Users can search for records using the search bar or browse by category, date, committee, or author.\n\n\n\nReading the Government Search Results\nThe script downloads 10 Congressional bills using a loop with error handling.\n\nlibrary(purrr)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\nsetwd(\"/Users/olivermyers/MyWebsite/govtdata.assignent04\")\n\n# csv downloaed from https://www.govinfo.gov/app/search/ and searching \"water\", filtering congressonal bills from 2024\n\n# Read the CSV file without skipping rows\ngovfiles &lt;- read.csv(file = \"/Users/olivermyers/MyWebsite/govinfo-search-results.csv\", skip = 2, header = FALSE)\n\ncolnames(govfiles) &lt;- govfiles[1, ]\ngovfiles &lt;- govfiles[-1, ]\nrownames(govfiles) &lt;- NULL\ncolnames(govfiles) &lt;- make.names(colnames(govfiles), unique = TRUE)\nhead(govfiles$packageId)\n\n[1] \"BILLS-118hr5770rh\"  \"BILLS-118hr5770rfs\" \"BILLS-118hr5770eh\" \n[4] \"BILLS-118hr8096ih\"  \"BILLS-118s4188is\"   \"BILLS-118hr7065ih\" \n\n# Preparing for bulk download of government documents\ngovfiles$id &lt;- govfiles$packageId\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# saving files into govdata.assignent04 folder\nsave_dir &lt;- \"/Users/olivermyers/MyWebsite/govtdata.assignent04\"\n\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    # Ensure the file path includes a proper separator\n    destfile &lt;- file.path(save_dir, paste0(\"govfiles_\", id, \".pdf\"))\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to avoid server throttling\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n\n## Download the first 10 from the csv file\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;%  # Change to limit to the first 10 files\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 25.64369 secs\n\n# List and print all files in the directory\nall_files &lt;- list.files(path = save_dir, full.names = FALSE)  \nprint(\"Files in the govtdata.assignent04 directory:\")\n\n[1] \"Files in the govtdata.assignent04 directory:\"\n\nhead(all_files)\n\n[1] \"govfiles_BILLS-118hr10150ih.pdf\" \"govfiles_BILLS-118hr3675rh.pdf\" \n[3] \"govfiles_BILLS-118hr5770eh.pdf\"  \"govfiles_BILLS-118hr5770rfs.pdf\"\n[5] \"govfiles_BILLS-118hr5770rh.pdf\"  \"govfiles_BILLS-118hr7021ih.pdf\""
  },
  {
    "objectID": "assignment4.html#discussion",
    "href": "assignment4.html#discussion",
    "title": "Assignment 4",
    "section": "Discussion:",
    "text": "Discussion:\n\nSimple report on difficulties encountered in the scraping process:\nScraping data using the first method, rvest, was initially a bit challenging for me. The need to inspect elements on the webpage and copy the XPath IDs to make the code work was a new concept. Additionally, some parts of the code were not as straightforward compared to the second method. That said, I found rvest to be significantly more useful in the long run because it allows for automated web scraping of large amounts of data from various webpage elements. Once I became familiar with the process, I appreciated the potential for efficiently formatting and organizing scraped data, even if it was tricky to set up at first.\nThe second method, on the other hand, was easier to use but felt less practical. This approach requires manually finding and downloading the necessary list yourself, which limits its automation capabilities. Initially, I encountered issues with downloading the files into the correct folder, but after consulting ChatGPT, I resolved the problem and successfully downloaded the files to the appropriate directory.\nIn conclusion, both methods have their advantages and can produce highly usable data. However, in my personal opinion, the rvest method stands out for its versatility and ability to scrape and format large-scale data efficiently. Although it requires more time and effort to understand and implement correctly, its potential for automating repetitive scraping tasks makes it the more valuable option overall. This could then be improved with more automation and cleaning steps build into the flow when using rvest."
  },
  {
    "objectID": "assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "href": "assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "title": "Assignment 5",
    "section": "Analyzing YouTube News Coverage on the 2024 U.S. Election",
    "text": "Analyzing YouTube News Coverage on the 2024 U.S. Election\nThis assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "assignment5.html#collecting-youtube-video-data",
    "href": "assignment5.html#collecting-youtube-video-data",
    "title": "Assignment 5",
    "section": "1. Collecting YouTube Video Data",
    "text": "1. Collecting YouTube Video Data\n\nRun YouTubenews01.R (prerequisites: YouTube developer API)\nThe code first authenticates the YouTube API using yt_oauth(). Then, it searches for videos related to “US election 2024” and extracts:\n\nlibrary(tuber)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(httr)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:httr':\n\n    content\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n# This one works today #\nyt_oauth(\"449459649595-3nqbg06e1ij0i2184ulc19q33lc08tvo.apps.googleusercontent.com\", \"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\", token = \"\")\n\n### Step 3: Download YouTube Data\n#### Here’s an example of collecting data on the “US election 2024.”\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\nAuto-refreshing stale OAuth token.\n\n#### Display the first few rows\nvideo_data &lt;- head(yt_uselection2024)\n\nsubset_video_data &lt;- video_data %&gt;%\n  select(video_id, channelId, title)\n\nprint(subset_video_data)\n\n     video_id                channelId\n1 uT9s4BXcv6w UCXIJgqnII2ZOINSWNOGFThA\n2 X4FtUJdKFCE UCeY0bbntWzzVIaj2z3QigXg\n3 9olb6OvXjKg UCupvZG-5ko_eiXAupbDfxWw\n4 BJkf7e_cZ68 UCH1oRy1dINbMVp3UFWrKP0w\n5 ORJI0-VSykQ UCeY0bbntWzzVIaj2z3QigXg\n6 0EPtWxTJMYc UCeY0bbntWzzVIaj2z3QigXg\n                                                                    title\n1        Donald Trump speaks after winning the 2024 Presidential Election\n2 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW\n3                     Trump wins 2024 presidential election, CNN projects\n4                     What polls say about the 2024 presidential election\n5               Possible paths to a win in the 2024 presidential election\n6 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW"
  },
  {
    "objectID": "assignment5.html#word-cloud-of-video-titles",
    "href": "assignment5.html#word-cloud-of-video-titles",
    "title": "Assignment 5",
    "section": "2. Word Cloud of Video Titles",
    "text": "2. Word Cloud of Video Titles\nThe most frequently used words in video titles are extracted and visualized.\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n\nFigure 2: Word Cloud of Video Titles – Most frequent words in YouTube video titles about the 2024 U.S. election."
  },
  {
    "objectID": "assignment5.html#analyzing-video-publication-dates",
    "href": "assignment5.html#analyzing-video-publication-dates",
    "title": "Assignment 5",
    "section": "3. Analyzing Video Publication Dates",
    "text": "3. Analyzing Video Publication Dates\nThe following code extracts publish dates and plots the frequency of videos published over time.\n\n### 4.2. Plot Video Publish Dates\n# Format publish dates and aggregate data\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n\nFigure 3: Video Publication Timeline – Distribution of videos published over time."
  },
  {
    "objectID": "assignment5.html#identifying-top-youtube-channels",
    "href": "assignment5.html#identifying-top-youtube-channels",
    "title": "Assignment 5",
    "section": "4. Identifying Top YouTube Channels",
    "text": "4. Identifying Top YouTube Channels\nThe top 10 channels with the highest number of videos related to the election are visualized.\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\nSelecting by n\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")"
  },
  {
    "objectID": "assignment5.html#analyzing-cnns-youtube-channel",
    "href": "assignment5.html#analyzing-cnns-youtube-channel",
    "title": "Assignment 5",
    "section": "5. Analyzing CNN’s YouTube Channel",
    "text": "5. Analyzing CNN’s YouTube Channel\nThe code extracts CNN’s channel statistics, including:\n\nTotal views\nSubscriber count\nTotal videos\n\n\n## Required Libraries\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(ggplot2)\n\n## CNN Channel ID\ncnn_channel_id &lt;- \"UCupvZG-5ko_eiXAupbDfxWw\"\n\n## get channel stats\ncnn_data &lt;- get_channel_stats(channel_id = \"UCupvZG-5ko_eiXAupbDfxWw\", mine = NULL)\n\nChannel Title: CNN \nNo. of Views: 17428751276 \nNo. of Subscribers: 17700000 \nNo. of Videos: 170170 \n\ncnn_stats = cnn_data$statistics\nhead(cnn_stats)\n\n$viewCount\n[1] \"17428751276\"\n\n$subscriberCount\n[1] \"17700000\"\n\n$hiddenSubscriberCount\n[1] FALSE\n\n$videoCount\n[1] \"170170\""
  },
  {
    "objectID": "assignment5.html#youtube-comments-sentiment-analysis",
    "href": "assignment5.html#youtube-comments-sentiment-analysis",
    "title": "Assignment 5",
    "section": "6. YouTube Comments Sentiment Analysis",
    "text": "6. YouTube Comments Sentiment Analysis\nThe code collects comments from a specific CNN video and processes the text for analysis.\n\nAnalyze the stats and comments:\n\nvideo_id =  \"Yzb5LGwt6LA\"\n\n## Get Video Statistics\nvideo_stats &lt;- get_stats(video_id)\ncat(\"Video Stats:\\n\")\n\nVideo Stats:\n\nhead(video_stats)\n\n$id\n[1] \"Yzb5LGwt6LA\"\n\n$viewCount\n[1] \"170169\"\n\n$likeCount\n[1] \"1797\"\n\n$favoriteCount\n[1] \"0\"\n\n$commentCount\n[1] \"695\"\n\n\n\nvideo_id =  \"Yzb5LGwt6LA\"\nvideocomments &lt;- get_all_comments(video_id)\nhead(videocomments$textOriginal)\n\n[1] \"Hollywood CNN! Welcome fake news!\"                                                                                                                                                                                   \n[2] \"6:28 damn this dude was completely  wrong\"                                                                                                                                                                           \n[3] \"Omg\"                                                                                                                                                                                                                 \n[4] \"Abandoned equipments - tanks,  APCs, airplanes ,etc - showed that Assad ‘s army is in complete disarray.\"                                                                                                            \n[5] \"May the innocent people be saved by God.\"                                                                                                                                                                            \n[6] \"The winner of the conflict is not the takfiris but Toyota, the favorite 'warhorse' of the Islamists which runs faster than the Russian tanks of Assad which enabled the Islamists to reach Aleppo in no time at all.\"\n\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n\nCan you use quanteda to analyze the text data from YouTube comments?\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"&gt;\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n      feature frequency rank docfreq group\n1         not        76    1      67   all\n2      rebels        76    1      70   all\n3       syria        67    3      55   all\n4          us        55    4      46   all\n5      people        53    5      32   all\n6          no        43    6      32   all\n7      russia        41    7      35   all\n8         cnn        40    8      31   all\n9         war        40    8      36   all\n10       like        36   10      29   all\n11        has        35   11      20   all\n12 terrorists        35   11      30   all\n13         we        34   13      28   all\n14        now        33   14      31   all\n15      there        32   15      27   all\n16       when        32   15      27   all\n17        who        31   17      29   all\n18        why        29   18      27   all\n19    because        27   19      22   all\n20       isis        27   19      25   all\n21       them        25   21      22   all\n22      putin        25   21      19   all\n23         he        25   21      17   all\n24       just        25   21      23   all\n25     spirit        25   21       2   all\n26      world        25   21      21   all\n27     syrian        24   27      18   all\n\n\n\n# Visualize the Most Frequent Words\ntstat_freq %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\", title = \"Top 15 Words in YouTube Comments\") +\n  theme_minimal()\n\n\n\n\nFigure 6: Most Frequent Words in YouTube Comments – A ranking of the 15 most common words used in YouTube comments on election-related videos."
  },
  {
    "objectID": "assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "href": "assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "title": "Assignment 5",
    "section": "7. Generating a Word Cloud of YouTube Comments",
    "text": "7. Generating a Word Cloud of YouTube Comments\n\n# Create a Word Cloud\nset.seed(132)\ntextplot_wordcloud(dfm_nonzero, max_words = 100)\n\n\n\n\nFigure 7: YouTube Comment Word Cloud – Visual representation of frequently used words in YouTube comments."
  },
  {
    "objectID": "assignment5.html#discussion",
    "href": "assignment5.html#discussion",
    "title": "Assignment 5",
    "section": "Discussion:",
    "text": "Discussion:\n\nAssignment Reflection:\nThis assignment required extensive effort to successfully retrieve and analyze YouTube comments from a CNN video. To achieve this, I leveraged the tuber package for data collection and quanteda for text analysis, with additional coding assistance from ChatGPT and reference to online documentation.\nThe comment processing involved tokenizing words, removing emojis, filtering out usernames, and eliminating common stopwords to ensure meaningful analysis. By visualizing the most frequently used words, I was able to identify dominant themes in the discussion.\nThe analysis revealed a predominantly negative sentiment, with frequent mentions of terms related to terrorism, conflict in the Middle East, and Syria. This suggests that the video’s content likely revolves around geopolitical tensions, aligning with CNN’s coverage focus. The findings highlight how YouTube comments can reflect public sentiment and engagement with political topics, making social media a powerful tool for understanding audience reactions to news coverage."
  },
  {
    "objectID": "nlp_assignment05.html",
    "href": "nlp_assignment05.html",
    "title": "nlp_assignment05",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "nlp_assignment05.html#old-model",
    "href": "nlp_assignment05.html#old-model",
    "title": "nlp_assignment05",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "href": "nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "title": "nlp_assignment05",
    "section": "New Model with improved predictive ability:",
    "text": "New Model with improved predictive ability:\n\n# Install and load required packages\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"doParallel\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\nif (length(new_packages)) install.packages(new_packages)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(ranger)\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Data ingestion & preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\nset.seed(123)\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Define Preprocessing Recipe with enhancements\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  # Normalize the text (using NFC form)\n  step_text_normalization(text, normalization_form = \"nfkc\") %&gt;%\n  # Tokenize text into words\n  step_tokenize(text, token = \"words\") %&gt;%\n  # Remove stopwords (default language is English)\n  step_stopwords(text) %&gt;%\n  # Create n-grams (unigrams and bigrams)\n  step_ngram(text, num_tokens = 2, min_num_tokens = 1) %&gt;%\n  # Filter tokens to limit the number of features\n  step_tokenfilter(text, max_tokens = 1000, min_times = 2) %&gt;%\n  # Create TF-IDF features\n  step_tfidf(text) %&gt;%\n  # Normalize predictors (if needed)\n  step_normalize(all_predictors())\n\n# Model Specification: Random Forest tuned on mtry and min_n (trees fixed at 1000)\nrf_spec &lt;- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n# Create workflow\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Set up 5-fold cross-validation with stratification\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# Define a grid for tuning mtry and min_n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Register parallel backend to speed up tuning\ndoParallel::registerDoParallel()\n\n# Tune model with cross-validation and evaluate using accuracy and kappa\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\nWarning: ! tune detected a parallel backend registered with foreach but no backend\n  registered with future.\nℹ Support for parallel processing with foreach was soft-deprecated in tune\n  1.2.1.\nℹ See ?parallelism (`?tune::parallelism()`) to learn more.\n\n# Review best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     2 Preprocessor1_Model01\n\n# Finalize workflow with the best hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% fit(data = train_data)\n\nWarning: max_tokens was set to 1000, but only 236 was available and selected.\n\n# Evaluate final model on the test set\nfinal_preds &lt;- final_fit %&gt;% \n  predict(new_data = test_data) %&gt;% \n  bind_cols(test_data)\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.883\n2 kap      multiclass     0.870\n\n# Display the confusion matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             0           0       0      0\n  Entertainment       0         0             6           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       6      0\n  Health              4         0             0           0       0      6\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          6      0\n  Travel               0      0          0      6\n\n# Predict on new samples and display the results\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples)\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Sports     \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Finance    \n\n\n\nHow I improved the predication ability:\nThe new model is better because it achieves an accuracy of about 88.3% and a kappa of around 0.87, which clearly shows it is making more reliable predictions and generalizes well to new data. We changed the preprocessing by normalizing the text to maintain consistency, incorporating n-grams to capture context beyond single words, and tuning the token filtering settings to retain a more representative vocabulary while cutting out noise. In addition, we refined the hyperparameter tuning process for the random forest by focusing on key parameters such as mtry and min_n. Together, these adjustments have resulted in a model that fits the data much more effectively and performs significantly better than the previous version.\n\nThe model before preformed with:\naccuracy multiclass 0.733\nkap multiclass 0.704\n\n\nWhere as now it preforms with:\naccuracy multiclass 0.883\nkap multiclass 0.870\nDisclaimer: I Used chat GPT 03-mini-high to help with optimization and to make better predictions*"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-analysis-of-political-search-interest-julynovember-2024",
    "title": "",
    "section": "",
    "text": "This assignment explores search interest trends for “Trump,” “Harris,” and “Election” using Google Trends data. Two methods were applied:\n\nCSV Data Analysis – Manually downloaded Google Trends CSV data was analyzed for significant date intervals.\nAPI-Based Analysis – The gtrendsR package in R retrieved and visualized real-time trend data.\n\nThe study identifies key political events that influenced search volume, visualizing their impact over time. Findings highlight notable spikes in search interest, particularly following candidate announcements, debates, and significant political events leading up to the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-csv-data-analysis",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-csv-data-analysis",
    "title": "",
    "section": "1. Google Trends CSV Data & Analysis",
    "text": "1. Google Trends CSV Data & Analysis\nI analyzed Google Trends data for three key terms: “Trump,” “Harris,” and “Election,” covering the date range from July 1, 2024, to November 1, 2024. The data was downloaded as a CSV file to examine the trends and significant date intervals.\n\nLoading the Google Trends CSV Data\nThe dataset is read into R, and unnecessary rows are removed to clean the data.\n\n# Load the CSV file from google trends with the 3 search terms\ngoogle_trends_data &lt;- read.csv(\"/Users/olivermyers/MyWebsite/Trump_Harris_Election_GoogleTrends.csv\")\ngoogle_trends_data &lt;- google_trends_data[-c(1, 2), ]\ncolnames(google_trends_data) &lt;- c(\"Day\", \"Trump\", \"Harris\", \"Election\")\ngoogle_trends_data$Day &lt;- as.Date(google_trends_data$Day)\n\n# print the cleaned dataset\nhead(google_trends_data)\n\n         Day Trump Harris Election\n3 2024-07-01     4      1        1\n4 2024-07-02     4      1        1\n5 2024-07-03     3      1        1\n6 2024-07-04     3      1        1\n7 2024-07-05     3      1        2\n8 2024-07-06     3      1        1\n\n\n\n\nPlotting Google Search Trends Over Time\nThis plot visualizes the search interest over time for Trump, Harris, and Election, with key event dates highlighted.\n\nsignificant_dates &lt;- as.Date(c(\"2024-07-14\", \"2024-07-21\", \"2024-07-22\", \n                               \"2024-08-06\", \"2024-08-23\", \"2024-09-11\", \"2024-09-15\"))\ndate_labels &lt;- 1:length(significant_dates)\nplot(google_trends_data$Day, google_trends_data$Harris, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Date\", ylab = \"Search Interest\", main = \"Google Trends: Harris, Trump, and Election\",\n     ylim = c(0, 100))  # Set y-axis limits from 0 to 100\n# Add lines for Trump and Election\nlines(google_trends_data$Day, google_trends_data$Trump, col = \"red\", lwd = 2)\nlines(google_trends_data$Day, google_trends_data$Election, col = \"orange\", lwd = 2)\n# Add vertical lines and numbers for significant dates, will be listed below\nfor (i in seq_along(significant_dates)) {\n  abline(v = significant_dates[i], col = \"lightgray\", lty = 2, lwd = 2)  # Light gray vertical lines\n  text(significant_dates[i], 100, labels = date_labels[i], col = \"black\", cex = 0.8, pos = 3)  # Add numbers above the lines\n}\n# Adding a legend in the top right cornner\nlegend(\"topright\", legend = c(\"Harris\", \"Trump\", \"Election\", \"Significant Day\"),\n       col = c(\"blue\", \"red\", \"orange\", \"lightgray\"), lty = c(1, 1, 1, 2), lwd = c(2, 2, 2, 2))\n\n\n\n\nFigure 1: Google Trends Search Interest Over Time: Search interest for ‘Trump,’ ‘Harris,’ and ‘Election’ from July to November 2024, highlighting key political events influencing search spikes.\n\n\n\n\n\nKey Political Events and Google Trends Impact\n\n\n\n\n\n\n\n\nDate\nEvent Significance\nTrend Impact\n\n\n\n\nJuly 14, 2024\nAn assassination attempt at a Trump rally draws national attention.\nTrump’s Peak: Significant spike in search interest for Trump.\n\n\nJuly 21, 2024\nBiden exits the race, shifting focus to Kamala Harris.\nSurge in searches for Harris.\n\n\nJuly 22, 2024\nHarris officially announces her candidacy for president.\nHarris Surpasses Trump: Harris overtakes Trump in search volume.\n\n\nAugust 6, 2024\nHarris selects Tim Walz as her running mate.\nHarris’s Peak Over Trump: Increased search interest for Harris and Walz.\n\n\nAugust 23, 2024\nHarris delivers her Democratic National Convention acceptance speech.\nSpike in searches as Harris becomes the Democratic front-runner.\n\n\nSeptember 11, 2024\nTrump and Harris attend the 9/11 Memorial, drawing significant media coverage.\nSimultaneous Peaks: Interest rises for both candidates.\n\n\nSeptember 15, 2024\nA second assassination attempt on Trump occurs at his golf course.\nTrump’s Peak: Major increase in Trump’s search interest.\n\n\nElection Momentum\nAs Election Day nears, public interest in candidates and key topics escalates.\nSteady growth in searches for Trump, Harris, and Election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-using-gtrendsr-packageusing",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#google-trends-using-gtrendsr-packageusing",
    "title": "",
    "section": "2. Google Trends Using “gtrendsR” PackageUsing",
    "text": "2. Google Trends Using “gtrendsR” PackageUsing\nn this method, the gtrendsR package in R was used to directly query Google Trends for real-time search interest data on “Trump,” “Harris,” and “Election.” Instead of manually downloading a CSV, this approach allows for automated data retrieval over a specified time range.\n\nFetching Data from Google Trends API\nThis code retrieves real-time Google search interest data directly from Google’s API.\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection &lt;- gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n\nFigure 2: Google Trends Data Retrieved via API: Real-time search interest trends retrieved using the gtrendsR package, offering an automated alternative to manual CSV downloads.\n\n\n\n\nKey Advantages\n\nAutomated Data Collection → Eliminates the need for manual downloads.\nReal-Time Updates → Ensures the latest data can be pulled dynamically.\nReproducibility → Allows future analysis with updated data."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment01/assignment2.html#discussion-differences-between-the-two-methods",
    "href": "pages/EPPS_6302/Assignment01/assignment2.html#discussion-differences-between-the-two-methods",
    "title": "",
    "section": "Discussion: Differences between the two methods:",
    "text": "Discussion: Differences between the two methods:\nIn the first method, data was downloaded directly from the Google Trends website after selecting the key terms and generating the trends. Afterward, the CSV file was downloaded and analyzed separately. In contrast, the second method used R and the gtrendsR package to retrieve and plot the data all in one place."
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html",
    "href": "pages/EPPS_6302/FinalProject/final_project.html",
    "title": "",
    "section": "",
    "text": "Our study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\n\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\n\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#project-summary",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#project-summary",
    "title": "",
    "section": "",
    "text": "Our study investigates whether there is a correlation between the decline in Google Trends search interest for movies during the first 21 days post-release and their ratings on IMDb and Rotten Tomatoes. The project combines data from Google Trends, IMDb, and Box Office Mojo to evaluate digital engagement as a predictive measure for movie reception.\n\n\nBelow is an overview of the data collection process:\n\nWeb Scraping:\n\nUsed the rvest package to scrape Box Office Mojo for movie metadata.\nEnriched the dataset with additional information from the OMDb API.\n\nGoogle Trends Data:\n\nRetrieved search interest data (Day 1 and Day 21) using the gtrendsR package.\n\nData Integration:\n\nCombined Google Trends, IMDb ratings, and daily earnings data into a unified dataset.\n\nAnalysis:\n\nConducted regression analysis in Stata with key variables including search interest drop rate, runtime, box office revenue, and number of votes.\n\n\n\n\n\n\n\n\nData Collection Flow Diagram"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#contact-me",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#contact-me",
    "title": "",
    "section": "Contact Me",
    "text": "Contact Me\nIf you have any questions about this project, please feel free to contact me at:\nEmail: oliver.myers@utdallas.edu"
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research."
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html#course-overview",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Methods of Data Collection and Production homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nProfessor: Dr. Karl Ho\n\nThis course covered data collection and production in the big data era, focusing on surveys, interviews, experiments, web data, and social media data. I gained hands-on experience in R and Python, learning best practices for data generation, structuring, and management in social science research."
  },
  {
    "objectID": "pages/EPPS_6302/Home/epps.6302.home.html#course-project",
    "href": "pages/EPPS_6302/Home/epps.6302.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nThis project explores the relationship between Google Trends search interest and movie ratings on IMDb and Rotten Tomatoes. By analyzing the drop rate of search interest over the first 21 days post-release, we assess whether early online engagement correlates with audience reception. Using web scraping, API integration, and regression analysis in R, this study applies data collection, cleaning, and predictive modeling to uncover insights into digital engagement and consumer behavior."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html",
    "title": "",
    "section": "",
    "text": "This assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-youtube-news-coverage-on-the-2024-u.s.-election",
    "title": "",
    "section": "",
    "text": "This assignment focuses on analyzing YouTube coverage of the 2024 U.S. Election using the tuber R package. The goal is to:\n\nSearch for videos related to “US election 2024.”\nExtract video metadata, including titles and channels.\nPerform text analysis on video titles and comments.\nVisualize data trends such as word clouds, publication frequency, and top channels.\nAnalyze CNN’s YouTube channel, retrieving statistics, video data, and viewer comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#collecting-youtube-video-data",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#collecting-youtube-video-data",
    "title": "",
    "section": "1. Collecting YouTube Video Data",
    "text": "1. Collecting YouTube Video Data\n\nRun YouTubenews01.R (prerequisites: YouTube developer API)\nThe code first authenticates the YouTube API using yt_oauth(). Then, it searches for videos related to “US election 2024” and extracts:\n\nlibrary(tuber)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(httr)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:httr':\n\n    content\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n# This one works today #\nyt_oauth(\"449459649595-3nqbg06e1ij0i2184ulc19q33lc08tvo.apps.googleusercontent.com\", \"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\", token = \"\")\n\n### Step 3: Download YouTube Data\n#### Here’s an example of collecting data on the “US election 2024.”\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\n#### Display the first few rows\nvideo_data &lt;- head(yt_uselection2024)\n\nsubset_video_data &lt;- video_data %&gt;%\n  select(video_id, channelId, title)\n\nprint(subset_video_data)\n\n     video_id                channelId\n1 A3AXszRgX7I UCO0akufu9MOzyz3nvGIXAAw\n2 X4FtUJdKFCE UCeY0bbntWzzVIaj2z3QigXg\n3 BJkf7e_cZ68 UCH1oRy1dINbMVp3UFWrKP0w\n4 uT9s4BXcv6w UCXIJgqnII2ZOINSWNOGFThA\n5 ORJI0-VSykQ UCeY0bbntWzzVIaj2z3QigXg\n6 9olb6OvXjKg UCupvZG-5ko_eiXAupbDfxWw\n                                                                    title\n1                                  Donald Trump wins the 2024 US election\n2 WATCH LIVE: Donald Trump wins 2024 presidential election | NBC News NOW\n3                     What polls say about the 2024 presidential election\n4        Donald Trump speaks after winning the 2024 Presidential Election\n5               Possible paths to a win in the 2024 presidential election\n6                     Trump wins 2024 presidential election, CNN projects"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#word-cloud-of-video-titles",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#word-cloud-of-video-titles",
    "title": "",
    "section": "2. Word Cloud of Video Titles",
    "text": "2. Word Cloud of Video Titles\nThe most frequently used words in video titles are extracted and visualized.\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n\nFigure 2: Word Cloud of Video Titles – Most frequent words in YouTube video titles about the 2024 U.S. election."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-video-publication-dates",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-video-publication-dates",
    "title": "",
    "section": "3. Analyzing Video Publication Dates",
    "text": "3. Analyzing Video Publication Dates\nThe following code extracts publish dates and plots the frequency of videos published over time.\n\n### 4.2. Plot Video Publish Dates\n# Format publish dates and aggregate data\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n\nFigure 3: Video Publication Timeline – Distribution of videos published over time."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#identifying-top-youtube-channels",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#identifying-top-youtube-channels",
    "title": "",
    "section": "4. Identifying Top YouTube Channels",
    "text": "4. Identifying Top YouTube Channels\nThe top 10 channels with the highest number of videos related to the election are visualized.\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\nSelecting by n\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-cnns-youtube-channel",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#analyzing-cnns-youtube-channel",
    "title": "",
    "section": "5. Analyzing CNN’s YouTube Channel",
    "text": "5. Analyzing CNN’s YouTube Channel\nThe code extracts CNN’s channel statistics, including:\n\nTotal views\nSubscriber count\nTotal videos\n\n\n## Required Libraries\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(ggplot2)\n\n## CNN Channel ID\ncnn_channel_id &lt;- \"UCupvZG-5ko_eiXAupbDfxWw\"\n\n## get channel stats\ncnn_data &lt;- get_channel_stats(channel_id = \"UCupvZG-5ko_eiXAupbDfxWw\", mine = NULL)\n\nChannel Title: CNN \nNo. of Views: 17776317625 \nNo. of Subscribers: 17900000 \nNo. of Videos: 170959 \n\ncnn_stats = cnn_data$statistics\nhead(cnn_stats)\n\n$viewCount\n[1] \"17776317625\"\n\n$subscriberCount\n[1] \"17900000\"\n\n$hiddenSubscriberCount\n[1] FALSE\n\n$videoCount\n[1] \"170959\""
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#youtube-comments-sentiment-analysis",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#youtube-comments-sentiment-analysis",
    "title": "",
    "section": "6. YouTube Comments Sentiment Analysis",
    "text": "6. YouTube Comments Sentiment Analysis\nThe code collects comments from a specific CNN video and processes the text for analysis.\n\nAnalyze the stats and comments:\n\nvideo_id =  \"Yzb5LGwt6LA\"\n\n## Get Video Statistics\nvideo_stats &lt;- get_stats(video_id)\ncat(\"Video Stats:\\n\")\n\nVideo Stats:\n\nhead(video_stats)\n\n$id\n[1] \"Yzb5LGwt6LA\"\n\n$viewCount\n[1] \"170244\"\n\n$likeCount\n[1] \"1791\"\n\n$favoriteCount\n[1] \"0\"\n\n$commentCount\n[1] \"694\"\n\n\n\nvideo_id =  \"Yzb5LGwt6LA\"\nvideocomments &lt;- get_all_comments(video_id)\nhead(videocomments$textOriginal)\n\n[1] \"Hollywood CNN! Welcome fake news!\"                                                                                                                                                                                   \n[2] \"6:28 damn this dude was completely  wrong\"                                                                                                                                                                           \n[3] \"Omg\"                                                                                                                                                                                                                 \n[4] \"Abandoned equipments - tanks,  APCs, airplanes ,etc - showed that Assad ‘s army is in complete disarray.\"                                                                                                            \n[5] \"May the innocent people be saved by God.\"                                                                                                                                                                            \n[6] \"The winner of the conflict is not the takfiris but Toyota, the favorite 'warhorse' of the Islamists which runs faster than the Russian tanks of Assad which enabled the Islamists to reach Aleppo in no time at all.\"\n\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n\nCan you use quanteda to analyze the text data from YouTube comments?\n\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\n# Define a function to remove emojis using a regex pattern\nremove_emojis &lt;- function(text) {\n  stri_replace_all_regex(\n    text,\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002700-\\U000027BF]\",\n    \"\",\n    vectorize_all = FALSE\n  )\n}\n\n# Clean comments by removing mentions, \"@@\", emojis, and extra whitespace\nvideocomments_cleaned &lt;- videocomments %&gt;%\n  mutate(\n    textOriginal = textOriginal %&gt;%\n      stri_replace_all_regex(\"\\\\S*@\\\\S*\", \"\") %&gt;% # Remove mentions\n      stri_replace_all_fixed(\"@@\", \"\") %&gt;%       # Remove \"@@\"\n      remove_emojis() %&gt;%                        # Remove emojis\n      str_squish()                               # Remove extra whitespace\n  )\n\n# Create a corpus from cleaned comments\ncorp_comments &lt;- corpus(videocomments_cleaned$textOriginal)\n\n# Tokenize the comments\ntoks_comments &lt;- tokens(corp_comments, remove_punct = TRUE)\ndfmat_comments &lt;- dfm(toks_comments)\n\n# Define custom stopwords\ncustom_stopwords &lt;- c(\"the\", \"and\", \"to\", \"is\", \"are\", \"a\", \"in\", \"of\", \"you\", \"this\", \"they\", \"&gt;\", \"that\", \"it\", \"from\", \"by\", \"i\", \"with\", \"for\", \"these\", \"as\", \"on\", \"be\", \"all\", \"their\", \"so\", \"have\" , \"al\", \"was\", \"at\", \"your\", \"it's\", \"do\", \"assad\", \"but\", \"will\", \"what\", \"if\", \"one\", \"how\", \"or\", \"about\", \"know\")\n\ndfmat_comments &lt;- dfm_remove(dfmat_comments, pattern = custom_stopwords)\ndfm_nonzero &lt;- dfmat_comments[ntoken(dfmat_comments) &gt; 0, ]\ntstat_freq &lt;- textstat_frequency(dfm_nonzero, n = 27)\n\n# View the top 20 frequencies\nprint(head(tstat_freq, 27))\n\n      feature frequency rank docfreq group\n1      rebels        76    1      70   all\n2         not        75    2      66   all\n3       syria        66    3      54   all\n4          us        54    4      45   all\n5      people        53    5      32   all\n6          no        41    6      31   all\n7         cnn        40    7      31   all\n8         war        40    7      36   all\n9      russia        40    7      34   all\n10       like        36   10      29   all\n11 terrorists        35   11      30   all\n12        has        34   12      19   all\n13        now        33   13      31   all\n14         we        33   13      27   all\n15      there        32   15      27   all\n16       when        32   15      27   all\n17        who        31   17      29   all\n18        why        28   18      26   all\n19    because        27   19      22   all\n20       isis        27   19      25   all\n21       them        25   21      22   all\n22      putin        25   21      19   all\n23         he        25   21      17   all\n24       just        25   21      23   all\n25     spirit        25   21       2   all\n26      world        25   21      21   all\n27     syrian        24   27      18   all\n\n\n\n# Visualize the Most Frequent Words\ntstat_freq %&gt;% \n  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +\n  geom_point() +\n  coord_flip() +\n  labs(x = NULL, y = \"Frequency\", title = \"Top 15 Words in YouTube Comments\") +\n  theme_minimal()\n\n\n\n\nFigure 6: Most Frequent Words in YouTube Comments – A ranking of the 15 most common words used in YouTube comments on election-related videos."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#generating-a-word-cloud-of-youtube-comments",
    "title": "",
    "section": "7. Generating a Word Cloud of YouTube Comments",
    "text": "7. Generating a Word Cloud of YouTube Comments\n\n# Create a Word Cloud\nset.seed(132)\ntextplot_wordcloud(dfm_nonzero, max_words = 100)\n\n\n\n\nFigure 7: YouTube Comment Word Cloud – Visual representation of frequently used words in YouTube comments."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment04/assignment5.html#discussion",
    "href": "pages/EPPS_6302/Assignment04/assignment5.html#discussion",
    "title": "",
    "section": "Discussion:",
    "text": "Discussion:\n\nAssignment Reflection:\nThis assignment required extensive effort to successfully retrieve and analyze YouTube comments from a CNN video. To achieve this, I leveraged the tuber package for data collection and quanteda for text analysis, with additional coding assistance from ChatGPT and reference to online documentation.\nThe comment processing involved tokenizing words, removing emojis, filtering out usernames, and eliminating common stopwords to ensure meaningful analysis. By visualizing the most frequently used words, I was able to identify dominant themes in the discussion.\nThe analysis revealed a predominantly negative sentiment, with frequent mentions of terms related to terrorism, conflict in the Middle East, and Syria. This suggests that the video’s content likely revolves around geopolitical tensions, aligning with CNN’s coverage focus. The findings highlight how YouTube comments can reflect public sentiment and engagement with political topics, making social media a powerful tool for understanding audience reactions to news coverage."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#course-overview",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Knowledge Mining Class homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Knowledge Mining\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course taught me how to harness knowledge mining, combining data science, AI, and machine learning to extract insights from complex data. I explored LLMs, generative AI, text mining, NLP, and predictive modeling, gaining hands-on experience in AI-driven research and its ethical implications."
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#assignments",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#assignments",
    "title": "",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!"
  },
  {
    "objectID": "pages/EPPS_6323/Home/epps.6323.home.html#course-project",
    "href": "pages/EPPS_6323/Home/epps.6323.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nHere is my proposal for the final project in this course. I will be exploring the topic of “Forecasting User Sentiment in Mobile Apps: A Knowledge Mining Approach”. This project will leverage sentiment analysis, NLP, and predictive modeling to forecast user sentiment in mobile apps, helping developers improve user experience and app ratings.\nMore to come after the completion of the final project"
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html",
    "title": "",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#course-overview",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#course-overview",
    "title": "",
    "section": "",
    "text": "Welcome to the Information Management homepage! This page summarizes my experience in this course, including key assignments and the final project.\n\nCourse Name: Information Management\n\nSemester: Spring 2025\n\nProfessor: Dr. Karl Ho\n\nThis course explored database design, management, and programming using SQL, Python, HTML, and Dash, alongside modern technologies like NoSQL. I applied knowledge mining, AI, and machine learning to extract insights, integrating text mining, NLP, and predictive modeling into database-driven applications."
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#assignments",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#assignments",
    "title": "",
    "section": "Assignments",
    "text": "Assignments\nHere’s a collection of assignments I’ve completed for this class. Click on each card to explore the details of each assignment or project!"
  },
  {
    "objectID": "pages/EPPS_6354/Home/epps.6354.home.html#course-project",
    "href": "pages/EPPS_6354/Home/epps.6354.home.html#course-project",
    "title": "",
    "section": "Course Project",
    "text": "Course Project\nThis project focuses on designing a relational database and interactive dashboard for the Texas Public Safety Association (TPSA) to evaluate the effectiveness of scoring rubrics in competitive events. By integrating student scores, rubric details, event types, and conference data, the system will enable data-driven insights into rubric fairness and effectiveness over time. Using SQL, PostgreSQL, and a Shiny-based web dashboard, this project will provide TPSA staff with an intuitive tool to refine scoring criteria, ensuring fairer and more accurate assessments across events.\nMore to come after the completion of the final project"
  },
  {
    "objectID": "pages/EPPS_6354/Assignment01/epps.6354.assignment1.html",
    "href": "pages/EPPS_6354/Assignment01/epps.6354.assignment1.html",
    "title": "",
    "section": "",
    "text": "# Import the TEDS 2016 data in Stata format using the haven package\n## install.packages(\"haven\")\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\nWhat problems do you encounter when working with the data set?\nThere is a few missing values that result in a NA from the data set.\n\n\nHow to deal with missing values?\nFollowing the assignment and example online, to resolve this issue"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html",
    "title": "",
    "section": "",
    "text": "This assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#analyzing-political-discourse-using-text-data",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#analyzing-political-discourse-using-text-data",
    "title": "",
    "section": "",
    "text": "This assignment explores the use of computational text analysis techniques to analyze political discourse. Through these methods, we gain insights into public discourse, political rhetoric, and ideological shifts over time. The study is divided into three main sections:\n\nBiden-Xi Summit Twitter Analysis - Extracting and analyzing Twitter data related to the Biden-Xi summit in November 2021, visualizing hashtag networks to identify key topics.\nU.S. Presidential Inaugural Speeches - Examining linguistic trends in U.S. presidential inaugural addresses over time, with a focus on key terms like “liberty,” “foreign,” and “we.”\nWordfish Scaling Model - Applying the Wordfish model to scale political documents and estimate ideological positioning using word frequencies."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#biden-xi-summit-twitter-analysis",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#biden-xi-summit-twitter-analysis",
    "title": "",
    "section": "1. Biden-Xi Summit Twitter Analysis",
    "text": "1. Biden-Xi Summit Twitter Analysis\n\nLoading Twitter Data\nThe dataset consists of tweets discussing the Biden-Xi summit (November 2021). We load the dataset using readr and extract the tweet text.\n\n\nPreprocessing the Text Data\nWe tokenize the tweet text, remove punctuation, and create a document-feature matrix (DFM), which converts text into a structured numerical format. then to analyze discussion topics, we extract hashtags from tweets and identify the most frequently used ones. Finaly, to visualize the relationships between hashtags, we create a feature co-occurrence matrix (FCM) and plot a network graph.\n\n# Set CRAN mirror for package downloads\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Load necessary libraries\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load Twitter data about the Biden-Xi summit (November 2021)\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\", show_col_types = FALSE)\n\n# Extract tweet text and create a document-feature matrix (DFM)\ntweet_text &lt;- summit$text\ntokens_tweet &lt;- tokens(tweet_text)\ntweet_dfm &lt;- dfm(tokens_tweet)\n\n# Clean the data by removing punctuation\ncleaned_dfm &lt;- tokens(tweet_text, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Identify and select the top hashtags from the dataset\nhashtag_dfm &lt;- dfm_select(cleaned_dfm, pattern = \"#*\")\ntop_hashtags &lt;- names(topfeatures(hashtag_dfm, 50))\n\n# Create a feature co-occurrence matrix (FCM) for hashtags\nhashtag_fcm &lt;- fcm(hashtag_dfm)\n\n# Visualize the hashtag network\ntop_hashtag_fcm &lt;- fcm_select(hashtag_fcm, pattern = top_hashtags)\ntextplot_network(top_hashtag_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\nFigure 1: Hashtag Network: The network visualization highlights key discussion topics related to the summit. Central hashtags like #biden and #china dominate the conversation, while #humanrights and #uyghurs indicate concerns over human rights issues.\n\n\n\n\n\n\n\n\n\nDiscussion:\nAnalyzing Twitter data from the November 2021 Biden-Xi summit, the semantic network visualization shows key connections between trending hash-tags. Central topics like #biden and #china highlight the focus on U.S.-China relations, while hash-tags such as #coronavirus and #fentanyl suggest that domestic issues like the pandemic and opioid crisis were closely linked to the summit discussions. A distinct cluster around #uyghurs and #humanrights underscores the prominence of human rights concerns, particularly regarding Xi Jinping. More peripheral topics like #taiwan reflect specific geopolitical tensions raised during the event."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#u.s.-presidential-inaugural-speeches",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#u.s.-presidential-inaugural-speeches",
    "title": "",
    "section": "2. U.S. Presidential Inaugural Speeches",
    "text": "2. U.S. Presidential Inaugural Speeches\nThis section examines U.S. presidential inaugural speeches over time, analyzing their linguistic trends and thematic focus.\n\nAnalyzing Early Inaugural Speeches & Keyword Trends Over Time\nWe create a document-feature matrix (DFM) for speeches from 1789 to 1826, removing common stopwords. We analyze post-1949 speeches and generate x-ray plots for key terms like liberty.\n\n# Quanteda Text Modeling and Analysis Example\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\"))\n# Website: https://quanteda.io/\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Create a Document-Feature Matrix (DFM) for speeches from 1789 to 1826\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\n# Analyze U.S. Presidential Inaugural Speeches Over Time\n# Focus on keywords from speeches after 1949\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\n# Generate an x-ray plot for the word \"liberty\"\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"liberty\") %&gt;%\n  textplot_xray()\n\n\n\n\nFigure 2: Keyword Frequency of ‘Liberty’: This visualization highlights the usage pattern of ‘liberty’ in presidential speeches, showing how its prominence fluctuates over time.\n\n\n\n\n\n\nComparing Key Terms in Presidential Speeches\nWe generate x-ray plots for three key words: foreign, we, and god.\n\n# Tokenize the subset of speeches for further analysis\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\n\n# Generate an x-ray plot for the words \"foreign,\" \"we,\" and \"god\"\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"foreign\"),\n  kwic(tokens_inaugural, pattern = \"we\"),\n  kwic(tokens_inaugural, pattern = \"god\")\n)\n\n\n\n\nFigure 3: Word Frequency Comparison: ‘Foreign’ was more common during Cold War-era speeches. ‘We’ is frequently used by presidents emphasizing unity (e.g., Obama, Biden). ‘God’ appears consistently toward the end of speeches, reflecting a tradition of invoking divine guidance.\n\n\n\n\n\n\nDiscussion:\nOver time, one consistent similarity among presidents is the frequent use of “God” toward the end of their speeches, reflecting a tradition of invoking moral authority and divine guidance. However, differences emerge in how presidents emphasize unity and foreign policy. Biden, like Obama, frequently uses “we” to emphasize collective action and national unity, whereas Trump’s less frequent use of “we” suggests a more individualistic tone. Earlier presidents like Johnson and Nixon focused heavily on “foreign” issues during the Cold War, while more recent speeches, including Biden’s, show a shift away from foreign policy toward domestic concerns, such as unity and economic recovery."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment02/assignment3.html#wordfish-scaling-model",
    "href": "pages/EPPS_6302/Assignment02/assignment3.html#wordfish-scaling-model",
    "title": "",
    "section": "3. Wordfish Scaling Model",
    "text": "3. Wordfish Scaling Model\nThe Wordfish model is an unsupervised text scaling method that estimates document positions based on word frequencies.\n\nApplying Wordfish to the 2010 Irish Budget Speeches\nWe use Wordfish to analyze Irish parliamentary speeches and estimate ideological positions.\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Load the Irish Budget 2010 corpus\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\n\n# Transform the corpus into a document-feature matrix (DFM)\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores for Wordfish model\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Estimate Wordfish model with document positions\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot estimated word positions, highlighting selected terms\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\nFigure 4: Wordfish model with document positions: This plot visualizes word scaling based on frequency and distribution in Irish Budget 2010 speeches. Highlighted terms like government, economy, and deficit shape document positioning, while central clustering suggests shared vocabulary. Words at the extremes indicate stronger differentiation in political discourse.\n\n\n\n\n\n\nVisualizing Word Positions\nThis plot highlights the relative importance of words in different political contexts.\n\n# Load necessary libraries\n# Plot estimated document positions grouped by party\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\nFigure 5: Estimated Document Positions by Political Party: This plot shows estimated ideological positions of politicians based on word usage. Fianna Fáil (FF) leans right, Sinn Féin (SF) and Labour (LAB) lean left, while Fine Gael (FG) and the Greens vary. Black dots represent individual positions, with confidence intervals highlighting linguistic and ideological differences.\n\n\n\n\n\n\nScaling Political Documents by Party\nWe visualize document positions grouped by political party.\n\n# Run correspondence analysis (CA) on the DFM\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot CA results, grouping documents by party\ntextplot_scale1d(ca, margin = \"documents\", \n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\nFigure 6: Estimated Document Positions by Political Party: This plot visualizes document positions using Correspondence Analysis (CA), grouping speeches by political party. It highlights linguistic differences across parties, mapping ideological tendencies based on word usage in Irish Budget 2010 speeches.\n\n\n\n\n\nSlapin, Jonathan and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.”American Journal of Political Science52(3): 705-772.\n\n\n\nWhat is wordfish?\nWordfish is an unsupervised Poisson scaling model that estimates document positions based on observed word frequencies amoung the documents. Unlike Wordscores, Wordfish doesn’t require reference texts, making it more flexible to use. Wordfish estimates word positions by calculating the estimates of: Psi (The overall frequency of each word across all documents), and Beta (The word’s power to differentiate between documents).\nThe following is an example of using wordfish to plot estimated word potions and to show and highlight certain features and where they are positioned relative to the other words. This data comes from 2010 Irish budget speeches and is used to analysis words position and to then also group words use within various parties."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html",
    "title": "",
    "section": "",
    "text": "This assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website."
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#automating-web-scraping-for-economic-and-government-data",
    "title": "",
    "section": "",
    "text": "This assignment explores web scraping techniques using rvest in R to extract structured data from Wikipedia and government databases. The study is divided into three main sections:\n\nScraping Foreign Reserve Data - Extracting global foreign exchange reserves from Wikipedia, cleaning the data, and formatting it for analysis.\nScraping U.S. Dollar Table - Extracting U.S. dollar banknote details from Wikipedia, removing unnecessary columns, and restructuring the data.\nDownloading Government Documents - Automating the retrieval of Congressional bills related to “water” from the govinfo.gov website.\n\n\n\n\n\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $639,593         2025-03-21 30,165 \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-u.s.-dollar-table",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-u.s.-dollar-table",
    "title": "",
    "section": "2. Scraping U.S. Dollar Table",
    "text": "2. Scraping U.S. Dollar Table\nThis section extracts a table from the Wikipedia page on U.S. currency, removing unnecessary image columns.\n\n\n\nFigure: Wikipedia Page on the U.S. Dollar – The Wikipedia entry for the United States dollar (USD), detailing its history, value, and international use. The right panel displays various denominations of U.S. banknotes.\n\n\n\nReading the U.S. Dollar Wikipedia Page\nThe script extracts the third table using XPath and removes unnecessary columns. This cleaned dataset helps track changes in U.S. banknotes over time.\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(rvest)\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/United_States_dollar'\n\n# Read the webpage\nusd_page &lt;- read_html(url)\n\n# Extract the third table on the page using XPath (skipping image columns 2 and 3)\nusd_table &lt;- usd_page %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[3]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nusd_data &lt;- usd_table[[1]]\n\n# Remove columns 2 and 3 (image columns)\nusd_data &lt;- usd_data %&gt;%\n  select(-`Front`, -`Reverse`)\n\n# Rename columns for clarity\ncolnames(usd_data) &lt;- c(\"Denomination\", \"Portrait\", \"Reverse_Motif\", \n                        \"First_Series\", \"Latest_Series\", \"Circulation\")\n\n# Clean up the data (if necessary)\nusd_data &lt;- usd_data %&gt;%\n  filter(!is.na(Denomination) & Denomination != \"\")  # Remove empty rows\n\n# View the cleaned and structured data\nhead(usd_data)\n\n# A tibble: 6 × 6\n  Denomination   Portrait   Reverse_Motif First_Series Latest_Series Circulation\n  &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n1 One dollar     George Wa… Great Seal o… Series 1963… Series 2021[… Wide       \n2 Two dollars    Thomas Je… Declaration … Series 1976  Series 2017A  Limited[48]\n3 Five dollars   Abraham L… Lincoln Memo… Series 2006  Series 2021[… Wide       \n4 Ten dollars    Alexander… Treasury Bui… Series 2004A Series 2017A  Wide       \n5 Twenty dollars Andrew Ja… White House   Series 2004  Series 2017A  Wide       \n6 Fifty dollars  Ulysses S… United State… Series 2004  Series 2017A  Wide"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#downloading-government-documents",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#downloading-government-documents",
    "title": "",
    "section": "3. Downloading Government Documents",
    "text": "3. Downloading Government Documents\nThis section automates the bulk download of government bills related to water policy using “https://www.govinfo.gov/app/search/”.\n\n\n\nFigure: GovInfo Search Portal – The homepage of GovInfo, a U.S. government website for accessing official documents. Users can search for records using the search bar or browse by category, date, committee, or author.\n\n\n\nReading the Government Search Results\nThe script downloads 10 Congressional bills using a loop with error handling.\n\nlibrary(purrr)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\nsetwd(\"/Users/olivermyers/MyWebsite/govtdata.assignent04\")\n\n# csv downloaed from https://www.govinfo.gov/app/search/ and searching \"water\", filtering congressonal bills from 2024\n\n# Read the CSV file without skipping rows\ngovfiles &lt;- read.csv(file = \"/Users/olivermyers/MyWebsite/govinfo-search-results.csv\", skip = 2, header = FALSE)\n\ncolnames(govfiles) &lt;- govfiles[1, ]\ngovfiles &lt;- govfiles[-1, ]\nrownames(govfiles) &lt;- NULL\ncolnames(govfiles) &lt;- make.names(colnames(govfiles), unique = TRUE)\nhead(govfiles$packageId)\n\n[1] \"BILLS-118hr5770rh\"  \"BILLS-118hr5770rfs\" \"BILLS-118hr5770eh\" \n[4] \"BILLS-118hr8096ih\"  \"BILLS-118s4188is\"   \"BILLS-118hr7065ih\" \n\n# Preparing for bulk download of government documents\ngovfiles$id &lt;- govfiles$packageId\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$id\n\n# saving files into govdata.assignent04 folder\nsave_dir &lt;- \"/Users/olivermyers/MyWebsite/govtdata.assignent04\"\n\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    # Ensure the file path includes a proper separator\n    destfile &lt;- file.path(save_dir, paste0(\"govfiles_\", id, \".pdf\"))\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to avoid server throttling\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n\n## Download the first 10 from the csv file\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults &lt;- 1:10 %&gt;%  # Change to limit to the first 10 files\n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\nTime difference of 24.14704 secs\n\n# List and print all files in the directory\nall_files &lt;- list.files(path = save_dir, full.names = FALSE)  \nprint(\"Files in the govtdata.assignent04 directory:\")\n\n[1] \"Files in the govtdata.assignent04 directory:\"\n\nhead(all_files)\n\n[1] \"govfiles_BILLS-118hr10150ih.pdf\" \"govfiles_BILLS-118hr3675rh.pdf\" \n[3] \"govfiles_BILLS-118hr5770eh.pdf\"  \"govfiles_BILLS-118hr5770rfs.pdf\"\n[5] \"govfiles_BILLS-118hr5770rh.pdf\"  \"govfiles_BILLS-118hr7021ih.pdf\""
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#discussion",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#discussion",
    "title": "",
    "section": "Discussion:",
    "text": "Discussion:\n\nSimple report on difficulties encountered in the scraping process:\nScraping data using the first method, rvest, was initially a bit challenging for me. The need to inspect elements on the webpage and copy the XPath IDs to make the code work was a new concept. Additionally, some parts of the code were not as straightforward compared to the second method. That said, I found rvest to be significantly more useful in the long run because it allows for automated web scraping of large amounts of data from various webpage elements. Once I became familiar with the process, I appreciated the potential for efficiently formatting and organizing scraped data, even if it was tricky to set up at first.\nThe second method, on the other hand, was easier to use but felt less practical. This approach requires manually finding and downloading the necessary list yourself, which limits its automation capabilities. Initially, I encountered issues with downloading the files into the correct folder, but after consulting ChatGPT, I resolved the problem and successfully downloaded the files to the appropriate directory.\nIn conclusion, both methods have their advantages and can produce highly usable data. However, in my personal opinion, the rvest method stands out for its versatility and ability to scrape and format large-scale data efficiently. Although it requires more time and effort to understand and implement correctly, its potential for automating repetitive scraping tasks makes it the more valuable option overall. This could then be improved with more automation and cleaning steps build into the flow when using rvest."
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html",
    "title": "",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#old-model",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#old-model",
    "title": "",
    "section": "",
    "text": "## NLP 2: text prediction\n## Purpose: \n# Install required packages if not already installed\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"workflows\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\n\n# 1. Data Ingestion and Preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\n\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 2. Define a Preprocessing Recipe\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;%   # Keep top 100 tokens\n  step_tfidf(text)                             # Convert tokens to TF-IDF features\n\n\n\n# 3. Specify a Random Forest Model with Tunable Hyperparameters\n# We'll tune mtry (number of predictors sampled for splitting)\n# and min_n (minimum number of observations in a node).\nrf_spec &lt;- rand_forest(\n  trees = 100,      # We'll keep trees fixed for this tuning example\n  mtry = tune(),    # Number of predictors to sample at each split\n  min_n = tune()    # Minimum number of data points in a node\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# 4. Create a Workflow Combining the Recipe and the Model Specification\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# 5. Set Up Cross-Validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# 6. Define a Grid for Hyperparameter Tuning\n# Here, we specify a grid for mtry and min_n.\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5  # 5 levels for each hyperparameter\n)\n\n# 7. Tune the Model Using Cross-Validation\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n# Collect the best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     4 Preprocessor1_Model06\n\n# 8. Finalize the Workflow with the Best Hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% workflows::fit(data = train_data)\n\n# 9. Evaluate the Final Model on the Test Set\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Performance Metrics\nfinal_preds &lt;- final_preds %&gt;% mutate(label = as.factor(label))\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\n\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.733\n2 kap      multiclass     0.704\n\n# Confusion Matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             2           0       3      1\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       3      0\n  Health              4         0             0           0       0      5\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          3      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          3      0\n  Travel               0      0          0      6\n\n# 10. Predict on New Samples (Optional)\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples) # Note the misclassified cases\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Education  \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Education"
  },
  {
    "objectID": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "href": "pages/EPPS_6323/Assignment05/nlp_assignment05.html#new-model-with-improved-predictive-ability",
    "title": "",
    "section": "New Model with improved predictive ability:",
    "text": "New Model with improved predictive ability:\n\n# Install and load required packages\nrequired_packages &lt;- c(\"tidyverse\", \"tidymodels\", \"textrecipes\", \"ranger\", \"doParallel\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\nif (length(new_packages)) install.packages(new_packages)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(ranger)\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Data ingestion & preparation\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\nRows: 200 Columns: 2\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata200 &lt;- data200 %&gt;% mutate(label = factor(label))\nset.seed(123)\nsplit &lt;- initial_split(data200, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Define Preprocessing Recipe with enhancements\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  # Normalize the text (using NFC form)\n  step_text_normalization(text, normalization_form = \"nfkc\") %&gt;%\n  # Tokenize text into words\n  step_tokenize(text, token = \"words\") %&gt;%\n  # Remove stopwords (default language is English)\n  step_stopwords(text) %&gt;%\n  # Create n-grams (unigrams and bigrams)\n  step_ngram(text, num_tokens = 2, min_num_tokens = 1) %&gt;%\n  # Filter tokens to limit the number of features\n  step_tokenfilter(text, max_tokens = 1000, min_times = 2) %&gt;%\n  # Create TF-IDF features\n  step_tfidf(text) %&gt;%\n  # Normalize predictors (if needed)\n  step_normalize(all_predictors())\n\n# Model Specification: Random Forest tuned on mtry and min_n (trees fixed at 1000)\nrf_spec &lt;- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n# Create workflow\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Set up 5-fold cross-validation with stratification\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)\n\n# Define a grid for tuning mtry and min_n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 20)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Register parallel backend to speed up tuning\ndoParallel::registerDoParallel()\n\n# Tune model with cross-validation and evaluate using accuracy and kappa\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\nWarning: ! tune detected a parallel backend registered with foreach but no backend\n  registered with future.\nℹ Support for parallel processing with foreach was soft-deprecated in tune\n  1.2.1.\nℹ See ?parallelism (`?tune::parallelism()`) to learn more.\n\n# Review best parameters based on accuracy\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")\nprint(best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     5     2 Preprocessor1_Model01\n\n# Finalize workflow with the best hyperparameters\nfinal_wf &lt;- finalize_workflow(wf, best_params)\n\n# Fit the final model on the full training data\nfinal_fit &lt;- final_wf %&gt;% fit(data = train_data)\n\nWarning: max_tokens was set to 1000, but only 236 was available and selected.\n\n# Evaluate final model on the test set\nfinal_preds &lt;- final_fit %&gt;% \n  predict(new_data = test_data) %&gt;% \n  bind_cols(test_data)\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\nprint(final_metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.883\n2 kap      multiclass     0.870\n\n# Display the confusion matrix\nfinal_conf_mat &lt;- conf_mat(final_preds, truth = label, estimate = .pred_class)\nprint(final_conf_mat)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             2         0             0           0       0      0\n  Education           0         6             0           0       0      0\n  Entertainment       0         0             6           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       6      0\n  Health              4         0             0           0       0      6\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          6      0\n  Travel               0      0          0      6\n\n# Predict on new samples and display the results\nnew_samples &lt;- tibble(\n  text = c(\"The international film festival showcased diverse movies.\",\n           \"Renewable energy projects are being launched globally.\",\n           \"Financial markets are showing unusual volatility today.\")\n)\nnew_preds &lt;- predict(final_fit, new_data = new_samples)\nnew_samples &lt;- new_samples %&gt;% bind_cols(new_preds)\nprint(new_samples)\n\n# A tibble: 3 × 2\n  text                                                      .pred_class\n  &lt;chr&gt;                                                     &lt;fct&gt;      \n1 The international film festival showcased diverse movies. Sports     \n2 Renewable energy projects are being launched globally.    Environment\n3 Financial markets are showing unusual volatility today.   Finance    \n\n\n\nHow I improved the predication ability:\nThe new model is better because it achieves an accuracy of about 88.3% and a kappa of around 0.87, which clearly shows it is making more reliable predictions and generalizes well to new data. We changed the preprocessing by normalizing the text to maintain consistency, incorporating n-grams to capture context beyond single words, and tuning the token filtering settings to retain a more representative vocabulary while cutting out noise. In addition, we refined the hyperparameter tuning process for the random forest by focusing on key parameters such as mtry and min_n. Together, these adjustments have resulted in a model that fits the data much more effectively and performs significantly better than the previous version.\n\nThe model before preformed with:\naccuracy multiclass 0.733\nkap multiclass 0.704\n\n\nWhere as now it preforms with:\naccuracy multiclass 0.883\nkap multiclass 0.870\nDisclaimer: I Used chat GPT 03-mini-high to help with optimization and to make better predictions*"
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf",
    "title": "",
    "section": "Project Proposal (PDf)",
    "text": "Project Proposal (PDf)"
  },
  {
    "objectID": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-foreign-reserve-data",
    "href": "pages/EPPS_6302/Assignment03/assignment4.html#scraping-foreign-reserve-data",
    "title": "",
    "section": "1. Scraping Foreign Reserve Data",
    "text": "1. Scraping Foreign Reserve Data\n\nReading the Wikipedia Page using the rvest package\nThe script first loads the required libraries and defines the Wikipedia URL for foreign exchange reserves. Using XPath selectors, the script extracts the first table from the Wikipedia page. The dataset is cleaned by renaming columns, filtering missing values, and converting foreign reserves into currency format.\n\n# Load required libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(scales)  # For currency formatting\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Define the URL\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table on the page using XPath\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table(fill = TRUE)\n\n# Extract the data frame\nfores &lt;- foreignreserve[[1]]\n\n# Rename columns for consistency\ncolnames(fores) &lt;- c(\"Country\", \"Continent\", \"Subregion\", \n                     \"Forexreswithgold\", \"Date1\", \"Change1\", \n                     \"Forexreswithoutgold\", \"Date2\", \"Change2\", \"Sources\")\n\n# Clean up variables:\n# Remove any rows where \"Country\" is missing\nfores &lt;- fores %&gt;% filter(!is.na(Country) & Country != \"\")\n\n# Clean up \"Forexreswithgold\" and \"Forexreswithoutgold\" columns\nfores$Forexreswithgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithgold))\n\nWarning: NAs introduced by coercion\n\nfores$Forexreswithoutgold &lt;- as.numeric(gsub(\",\", \"\", fores$Forexreswithoutgold))\n\nWarning: NAs introduced by coercion\n\n# Convert \"Date1\" and \"Date2\" to Date format\nfores$Date1 &lt;- as.Date(fores$Date1, format = \"%d %b %Y\")\nfores$Date2 &lt;- as.Date(fores$Date2, format = \"%d %b %Y\")\n\n# Format as currency\nfores$Forexreswithgold &lt;- dollar(fores$Forexreswithgold)\nfores$Forexreswithoutgold &lt;- dollar(fores$Forexreswithoutgold)\n\n# View the cleaned and formatted data\nhead(fores)\n\n# A tibble: 6 × 10\n  Country                Continent Subregion Forexreswithgold Date1      Change1\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;  \n1 Country and region(as… Continent Sub-regi… &lt;NA&gt;             NA         Change \n2 China                  Asia      East Asia $3,571,803       2024-10-31 21,957 \n3 Japan                  Asia      East Asia $1,238,950       2024-11-01 15,948 \n4 Switzerland            Europe    Western … $952,687         2024-09-30 1,127  \n5 India                  Asia      South As… $639,593         2025-03-21 30,165 \n6 Russia                 Europe    Eastern … $620,800         2024-11-08 11,900 \n# ℹ 4 more variables: Forexreswithoutgold &lt;chr&gt;, Date2 &lt;date&gt;, Change2 &lt;chr&gt;,\n#   Sources &lt;chr&gt;"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#final-project-presentation-slides",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#final-project-presentation-slides",
    "title": "",
    "section": "Final Project Presentation (Slides)",
    "text": "Final Project Presentation (Slides)\n\n\nHere is the code for the data collection, cleaning and analysis:\n\n\n Data Collection Code (R) \n\n\n Analysis Code (Stata)"
  },
  {
    "objectID": "pages/EPPS_6302/FinalProject/final_project.html#final-project-paper-pdf",
    "href": "pages/EPPS_6302/FinalProject/final_project.html#final-project-paper-pdf",
    "title": "",
    "section": "Final Project Paper (PDF)",
    "text": "Final Project Paper (PDF)\n\n\nReferences\n\n\nBox Office Mojo. (n.d.). Yearly box office results. https://www.boxofficemojo.com (November 16, 2024).\nCebrián, Eduardo, and Josep Domenech. 2023. “Is Google Trends a Quality Data Source?” Applied Economics Letters 30(6): 811–15. doi:10.1080/13504851.2021.2023088 (November 8, 2024).\nDemir, Deniz, Olga Kapralova, and Hongze Lai. 2012. “Predicting IMDB Movie Ratings Using Google Trends.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fb53e9605997374f178359d3e1e86008dac6c28a(November 1, 2024).\nFritzsch, Benjamin, Kai Wenger, Philipp Sibbertsen, and Georg Ullmann. 2020. “Can Google Trends Improve Sales Forecasts on a Product Level?” Applied Economics Letters 27(17): 1409–14. doi:10.1080/13504851.2019.1686110(November 8, 2024).\nGoogle. 2024. Google News Initiative: Google Trends.https://newsinitiative.withgoogle.com/resources/trainings/advanced-google-trends/ (November 2, 2024).\nHand, Chris, and Guy Judge. 2012. “Searching for the Picture: Forecasting UK Cinema Admissions Using Google Trends Data.” Applied Economics Letters 19(11): 1051–55. doi:10.1080/13504851.2011.613744 (November 4, 2024).\nJun, Seung-Pyo, Hyoung Sun Yoo, and San Choi. 2018. “Ten Years of Research Change Using Google Trends: From the Perspective of Big Data Utilizations and Applications.” Technological Forecasting and Social Change130: 69–87. doi:10.1016/j.techfore.2017.11.009 (November 4, 2024).\nMassicotte, Pierre, and Dirk Eddelbuettel. 2022. gtrendsR: Perform and Display Google Trends Queries. R package version 1.5.1. https://CRAN.R-project.org/package=gtrendsR (December 3, 2024).\nOMDb API. (n.d.). The Open Movie Database. https://www.omdbapi.com (November 17, 2024).\nOpenAI. 2024. ChatGPT. https://openai.com/chatgpt (November 30, 2024).\nShukla, Anuja, Aditya Yadav, and Shiv Kumar Sharma. 2022. “Predicting Movie Ticket Sales Using Google Trends: Implication of Big Data Analytics.” IUP Journal of Management Research 21(1). https://openurl.ebsco.com/EPDB:gcd:11:23114539/detailv2 (November 4, 2024).\nSilva, Emmanuel Sirimal, and Dag Øivind Madsen. 2022. “Google Trends.” In Encyclopedia of Tourism Management and Marketing, ed. Dimitrios Buhalis, 446–47. Edward Elgar Publishing. doi:10.4337/9781800377486.google.trends (November 2, 2024).\nWickham, Hadley, and Davis Vaughan. 2024. tidyr: Tidy Messy Data. R package version 1.3.1. https://CRAN.R-project.org/package=tidyr (December 3, 2024).\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. readr: Read Rectangular Text Data. R package version 2.1.5. https://CRAN.R-project.org/package=readr (December 3, 2024).\nWickham, Hadley. 2024. rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.4. https://github.com/tidyverse/rvest, https://rvest.tidyverse.org/ (November 5, 2024).\nWooldridge, Jeffrey M. 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning (November 5, 2024)."
  },
  {
    "objectID": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-1",
    "href": "pages/EPPS_6323/projectproposal/EPPS_6323_projectproposal.html#project-proposal-pdf-1",
    "title": "",
    "section": "Project Proposal (PDf)",
    "text": "Project Proposal (PDf)"
  },
  {
    "objectID": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#project-progress-report-slides",
    "href": "pages/EPPS_6354/projectproposal/EPPS_6354_projectproposal.html#project-progress-report-slides",
    "title": "",
    "section": "Project Progress Report (Slides)",
    "text": "Project Progress Report (Slides)"
  },
  {
    "objectID": "pages/About/aboutme.html",
    "href": "pages/About/aboutme.html",
    "title": "",
    "section": "",
    "text": "Hi there! I’m Oliver Jack Myers, a passionate mixed-methods User Experience (UX) Researcher and Designer currently pursuing my Master of Science in Applied Cognition and Neuroscience with a focus on Human-Computer Interaction at the University of Texas at Dallas.\nI specialize in using both qualitative and quantitative methods to uncover insights that drive impactful design solutions. My experience spans user interviews, usability analysis, survey design, journey mapping, prototyping, and more. With a strong foundation in tools like R Studio, Figma, and Qualtrics, I bring a comprehensive approach to improving user experiences.\nWhen I’m not diving into research or design, you can find me exploring the intersection of cognitive science and technology, continuously seeking innovative ways to bridge the gap between human needs and digital solutions.\nFeel free to explore my portfolio and connect with me to learn more about my work!\n\nUX Research and Design Portfolio: OliverJackMyers.com\nEmail: OliverJackMyers.Design@gmail.com\nLinkedIn: @oliverjackmyers\nGitHub: @OliverJackMyers"
  }
]